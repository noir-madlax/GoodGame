"""
ä½¿ç”¨å¢å¼ºæ–‡æœ¬é‡æ–°ç”Ÿæˆå•†å“å‘é‡

ä½¿ç”¨é¡µé¢: ç‹¬ç«‹è„šæœ¬
åŠŸèƒ½:
  1. ä» gg_taobao_product_apu è·å–å¢å¼ºæ–‡æœ¬ (enhanced_text)
  2. ä½¿ç”¨ OpenAI text-embedding-3-small ç”Ÿæˆå‘é‡
  3. æ›´æ–° gg_taobao_product_embeddings è¡¨

æ¨¡å‹è¯´æ˜:
  - æ¨¡å‹: text-embedding-3-small (ä¸æœç´¢ API ä¸€è‡´)
  - ç»´åº¦: 1536
  - æœç´¢æ—¶ç”¨æˆ·è¾“å…¥ä¹Ÿä½¿ç”¨åŒä¸€æ¨¡å‹å‘é‡åŒ–ï¼Œç¡®ä¿è¯­ä¹‰ç©ºé—´ä¸€è‡´

è¿è¡Œæ–¹å¼:
  python regenerate_embeddings_enhanced.py
  
  # æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†å‰ 10 ä¸ªï¼‰
  python regenerate_embeddings_enhanced.py --limit 10
  
  # å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼ˆè¦†ç›–å·²æœ‰å‘é‡ï¼‰
  python regenerate_embeddings_enhanced.py --force
"""

import os
import time
import argparse
from pathlib import Path
from dotenv import load_dotenv
from supabase import create_client, Client
from openai import OpenAI

# ==================== é…ç½®åŒºåŸŸ ====================
# Embedding æ¨¡å‹ (ä¸æœç´¢ API ä¸€è‡´)
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSIONS = 1536

# æ‰¹å¤„ç†å¤§å°
BATCH_SIZE = 50

# è¯·æ±‚é—´éš” (ç§’)
REQUEST_DELAY = 0.5


def load_clients() -> tuple:
    """åŠ è½½ Supabase å’Œ OpenAI å®¢æˆ·ç«¯"""
    env_path = Path(__file__).parent.parent.parent / ".env"
    load_dotenv(env_path)
    
    # Supabase
    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_KEY")
    
    if not supabase_url or not supabase_key:
        raise ValueError("æœªæ‰¾åˆ° SUPABASE_URL æˆ– SUPABASE_KEY")
    
    supabase = create_client(supabase_url, supabase_key)
    print(f"âœ… æˆåŠŸè¿æ¥ Supabase")
    
    # OpenAI - ä¼˜å…ˆä½¿ç”¨ OPENAI_API_KEYï¼Œå¦åˆ™ä½¿ç”¨ OPENROUTER_API_KEY
    openai_key = os.getenv("OPENAI_API_KEY")
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    
    if openai_key:
        openai_client = OpenAI(api_key=openai_key)
        print(f"âœ… ä½¿ç”¨ OpenAI API")
    elif openrouter_key:
        openai_client = OpenAI(
            api_key=openrouter_key,
            base_url="https://openrouter.ai/api/v1"
        )
        print(f"âœ… ä½¿ç”¨ OpenRouter API (è°ƒç”¨ OpenAI)")
    else:
        raise ValueError("æœªæ‰¾åˆ° OPENAI_API_KEY æˆ– OPENROUTER_API_KEY")
    
    return supabase, openai_client


def get_products_with_enhanced_text(supabase: Client, limit: int = None, force: bool = False) -> list:
    """
    è·å–æœ‰å¢å¼ºæ–‡æœ¬çš„å•†å“
    
    å‚æ•°:
        supabase: Supabase å®¢æˆ·ç«¯
        limit: é™åˆ¶æ•°é‡
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼ˆä¸è·³è¿‡å·²æœ‰å‘é‡çš„ï¼‰
    """
    # è·å–æ‰€æœ‰æœ‰å¢å¼ºæ–‡æœ¬çš„å•†å“
    query = supabase.table("gg_taobao_product_apu").select(
        "product_id, enhanced_text"
    ).not_.is_("enhanced_text", "null")
    
    if limit:
        query = query.limit(limit)
    
    result = query.execute()
    products = result.data
    
    if not force:
        # è·å–å·²æœ‰å‘é‡çš„å•†å“ ID
        embeddings_result = supabase.table("gg_taobao_product_embeddings").select(
            "product_id"
        ).eq("embedding_type", "text_enhanced").execute()
        
        existing_ids = {row["product_id"] for row in embeddings_result.data}
        
        # è¿‡æ»¤å‡ºæœªç”Ÿæˆå‘é‡çš„å•†å“
        products = [p for p in products if p["product_id"] not in existing_ids]
    
    return products


def generate_embeddings_batch(openai_client: OpenAI, texts: list) -> list:
    """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡"""
    response = openai_client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=texts
    )
    
    # æŒ‰ index æ’åºç¡®ä¿é¡ºåºæ­£ç¡®
    embeddings = [None] * len(texts)
    for item in response.data:
        embeddings[item.index] = item.embedding
    
    return embeddings


def save_embeddings(supabase: Client, embeddings_data: list) -> None:
    """ä¿å­˜å‘é‡åˆ°æ•°æ®åº“"""
    if not embeddings_data:
        return
    
    # æ‰¹é‡ upsert
    supabase.table("gg_taobao_product_embeddings").upsert(
        embeddings_data,
        on_conflict="product_id,embedding_type"
    ).execute()


def update_original_embeddings(supabase: Client, embeddings_data: list) -> None:
    """
    åŒæ—¶æ›´æ–°åŸå§‹çš„ text ç±»å‹å‘é‡
    è¿™æ ·æœç´¢æ—¶ä½¿ç”¨åŸæœ‰çš„ text ç±»å‹æŸ¥è¯¢ä¹Ÿèƒ½å—ç›Š
    """
    if not embeddings_data:
        return
    
    # è½¬æ¢ä¸º text ç±»å‹
    text_embeddings = []
    for item in embeddings_data:
        text_item = item.copy()
        text_item["embedding_type"] = "text"
        text_embeddings.append(text_item)
    
    supabase.table("gg_taobao_product_embeddings").upsert(
        text_embeddings,
        on_conflict="product_id,embedding_type"
    ).execute()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä½¿ç”¨å¢å¼ºæ–‡æœ¬é‡æ–°ç”Ÿæˆå•†å“å‘é‡")
    parser.add_argument("--limit", "-l", type=int, help="é™åˆ¶å¤„ç†æ•°é‡")
    parser.add_argument("--force", "-f", action="store_true", help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆ")
    args = parser.parse_args()
    
    print("=" * 70)
    print("ğŸ”¢ ä½¿ç”¨å¢å¼ºæ–‡æœ¬é‡æ–°ç”Ÿæˆå•†å“å‘é‡")
    print(f"   æ¨¡å‹: {EMBEDDING_MODEL}")
    print(f"   ç»´åº¦: {EMBEDDING_DIMENSIONS}")
    print("=" * 70)
    
    # 1. åŠ è½½å®¢æˆ·ç«¯
    supabase, openai_client = load_clients()
    
    # 2. è·å–å¾…å¤„ç†å•†å“
    print(f"\nğŸ“‹ æ­£åœ¨è·å–å¾…å¤„ç†å•†å“...")
    products = get_products_with_enhanced_text(supabase, args.limit, args.force)
    print(f"   å…± {len(products)} ä¸ªå•†å“å¾…å¤„ç†")
    
    if not products:
        print("\nâœ… æ‰€æœ‰å•†å“å·²å®Œæˆå¢å¼ºå‘é‡åŒ–ï¼")
        return
    
    # 3. åˆ†æ‰¹å¤„ç†
    print(f"\nğŸš€ å¼€å§‹ç”Ÿæˆå‘é‡ (æ‰¹å¤§å°: {BATCH_SIZE})...")
    
    total_processed = 0
    
    for i in range(0, len(products), BATCH_SIZE):
        batch = products[i:i + BATCH_SIZE]
        
        # æå–å¢å¼ºæ–‡æœ¬
        texts = [p["enhanced_text"] for p in batch]
        product_ids = [p["product_id"] for p in batch]
        
        try:
            # ç”Ÿæˆå‘é‡
            embeddings = generate_embeddings_batch(openai_client, texts)
            
            # æ„å»ºæ•°æ® (text_enhanced ç±»å‹)
            embeddings_data = []
            for j, product_id in enumerate(product_ids):
                if embeddings[j]:
                    embeddings_data.append({
                        "product_id": product_id,
                        "embedding_type": "text_enhanced",
                        "embedding_model": EMBEDDING_MODEL,
                        "embedding": embeddings[j],
                        "source_text": texts[j]
                    })
            
            # ä¿å­˜ text_enhanced ç±»å‹
            save_embeddings(supabase, embeddings_data)
            
            # åŒæ—¶æ›´æ–° text ç±»å‹ï¼ˆè¦†ç›–åŸå§‹å‘é‡ï¼‰
            update_original_embeddings(supabase, embeddings_data)
            
            total_processed += len(batch)
            print(f"   âœ… å·²å¤„ç† {total_processed}/{len(products)} ä¸ªå•†å“")
            
            # è¯·æ±‚é—´éš”
            time.sleep(REQUEST_DELAY)
            
        except Exception as e:
            print(f"   âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            time.sleep(2)
    
    # 4. ç»Ÿè®¡
    print(f"\nğŸ“Š å¤„ç†å®Œæˆ:")
    print(f"   å¤„ç†å•†å“æ•°: {total_processed}")
    
    # éªŒè¯
    result = supabase.table("gg_taobao_product_embeddings").select(
        "id", count="exact"
    ).eq("embedding_type", "text_enhanced").execute()
    
    print(f"   å¢å¼ºå‘é‡æ•°: {result.count}")
    
    result2 = supabase.table("gg_taobao_product_embeddings").select(
        "id", count="exact"
    ).eq("embedding_type", "text").execute()
    
    print(f"   æ–‡æœ¬å‘é‡æ•° (å·²æ›´æ–°): {result2.count}")
    
    print("\n" + "=" * 70)
    print("âœ… å¢å¼ºå‘é‡åŒ–å®Œæˆï¼")
    print("=" * 70)


if __name__ == "__main__":
    main()

