"""
å…¨é‡å•†å“ APU åˆ†æè„šæœ¬
ä½¿ç”¨ LLM åˆ†ææ‰€æœ‰å•†å“çš„ Attribute-Performance-Use ä¸‰ç»´åº¦

ä½¿ç”¨é¡µé¢: ç‹¬ç«‹åˆ†æè„šæœ¬
åŠŸèƒ½:
  1. ä»æ•°æ®åº“è¯»å–æ‰€æœ‰å•†å“
  2. ä½¿ç”¨ LLM (Gemini) åˆ†ææ¯ä¸ªå•†å“çš„ APU
  3. è¾“å‡º JSON ç»“æœä¾›ç”¨æˆ·ç¡®è®¤
  4. ç¡®è®¤ååŒæ—¶å…¥åº“åˆ°:
     - gg_taobao_product_apu: å•†å“ APU è§£æç»“æœ
     - gg_apu_product_rules: æ–°çš„è§„åˆ™åº“ï¼ˆ5 ç»´åº¦ï¼‰

è¿è¡Œæ–¹å¼:
  # åˆ†æå¹¶è¾“å‡º JSONï¼ˆä¸å…¥åº“ï¼‰
  python analyze_products_apu.py --output output/apu_analysis.json
  
  # åˆ†ææŒ‡å®šæ•°é‡çš„å•†å“ï¼ˆæµ‹è¯•ç”¨ï¼‰
  python analyze_products_apu.py --limit 10 --output output/apu_analysis_test.json
  
  # å°†ç¡®è®¤åçš„ JSON å…¥åº“ï¼ˆåŒæ—¶å¯¼å…¥ product_apu å’Œ product_rulesï¼‰
  python analyze_products_apu.py --import output/apu_analysis.json
"""

import os
import json
import re
import time
import argparse
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
from openai import OpenAI

from apu_prompt_builder import APUPromptBuilder, load_supabase_client


# ==================== é…ç½® ====================
# LLM é…ç½®
LLM_MODEL = "google/gemini-2.5-flash"  # é€šè¿‡ OpenRouter è°ƒç”¨
REQUEST_DELAY = 0.5  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
MAX_RETRIES = 3      # æœ€å¤§é‡è¯•æ¬¡æ•°

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path(__file__).parent / "output" / "apu_analysis"


class ProductAPUAnalyzer:
    """å•†å“ APU åˆ†æå™¨"""
    
    def __init__(self, supabase: Client, llm_client: OpenAI):
        """
        åˆå§‹åŒ–
        
        å‚æ•°:
            supabase: Supabase å®¢æˆ·ç«¯
            llm_client: OpenAI/OpenRouter å®¢æˆ·ç«¯
        """
        self.supabase = supabase
        self.llm = llm_client
        self.prompt_builder = APUPromptBuilder(supabase)
    
    def get_all_products(self, limit: Optional[int] = None, skip_analyzed: bool = False) -> List[Dict]:
        """
        è·å–æ‰€æœ‰å•†å“
        
        å‚æ•°:
            limit: é™åˆ¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            skip_analyzed: æ˜¯å¦è·³è¿‡å·²åˆ†æçš„å•†å“ï¼ˆå¢é‡æ¨¡å¼ï¼‰
        """
        if skip_analyzed:
            # è·å–å·²åˆ†æçš„å•†å“ ID
            analyzed_result = self.supabase.table("gg_taobao_product_apu").select("product_id").execute()
            analyzed_ids = [r["product_id"] for r in analyzed_result.data]
            
            # è·å–æœªåˆ†æçš„å•†å“
            query = self.supabase.table("gg_taobao_products").select(
                "id, item_id, item_name, price_yuan"
            )
            
            if analyzed_ids:
                # ä½¿ç”¨ not.in_ è¿‡æ»¤å·²åˆ†æçš„å•†å“
                query = query.not_.in_("id", analyzed_ids)
            
            query = query.order("id")
            
            if limit:
                query = query.limit(limit)
            
            result = query.execute()
            return result.data
        else:
            query = self.supabase.table("gg_taobao_products").select(
                "id, item_id, item_name, price_yuan"
            ).order("id")
            
            if limit:
                query = query.limit(limit)
            
            result = query.execute()
            return result.data
    
    def analyze_single_product(self, product: Dict) -> Dict:
        """
        åˆ†æå•ä¸ªå•†å“çš„ APU
        
        å‚æ•°:
            product: å•†å“æ•°æ®
            
        è¿”å›:
            APU åˆ†æç»“æœ
        """
        item_name = product["item_name"]
        price = str(product["price_yuan"])
        
        # æ„å»º Prompt
        prompt = self.prompt_builder.build_ingest_prompt(item_name, price)
        
        # è°ƒç”¨ LLM
        for retry in range(MAX_RETRIES):
            try:
                response = self.llm.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,
                    max_tokens=1000,
                )
                
                content = response.choices[0].message.content
                
                # è§£æ JSON
                json_match = re.search(r'\{[\s\S]*\}', content)
                if json_match:
                    result = json.loads(json_match.group())
                    result["_source"] = {
                        "product_id": product["id"],
                        "item_id": product["item_id"],
                        "original_name": item_name,
                        "price": price,
                    }
                    return result
                else:
                    raise ValueError("LLM è¿”å›æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æ JSON")
                    
            except Exception as e:
                print(f"   âš ï¸ é‡è¯• {retry + 1}/{MAX_RETRIES}: {e}")
                if retry < MAX_RETRIES - 1:
                    time.sleep(2)
                else:
                    return {
                        "_error": str(e),
                        "_source": {
                            "product_id": product["id"],
                            "item_id": product["item_id"],
                            "original_name": item_name,
                            "price": price,
                        }
                    }
        
        return None
    
    def analyze_all_products(
        self, 
        limit: Optional[int] = None,
        output_path: Optional[Path] = None,
        skip_analyzed: bool = False
    ) -> List[Dict]:
        """
        åˆ†ææ‰€æœ‰å•†å“
        
        å‚æ•°:
            limit: é™åˆ¶æ•°é‡
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            skip_analyzed: æ˜¯å¦è·³è¿‡å·²åˆ†æçš„å•†å“ï¼ˆå¢é‡æ¨¡å¼ï¼‰
            
        è¿”å›:
            æ‰€æœ‰åˆ†æç»“æœ
        """
        # è·å–å•†å“
        products = self.get_all_products(limit, skip_analyzed)
        print(f"ğŸ“‹ å…± {len(products)} ä¸ªå•†å“å¾…åˆ†æ")
        
        results = []
        success_count = 0
        error_count = 0
        
        start_time = time.time()
        
        for i, product in enumerate(products, 1):
            print(f"\n[{i}/{len(products)}] åˆ†æ: {product['item_name'][:50]}...")
            
            result = self.analyze_single_product(product)
            
            if result and "_error" not in result:
                results.append(result)
                success_count += 1
                print(f"   âœ… å“ç±»: {result.get('category', 'æœªçŸ¥')}")
                print(f"   ğŸ“ æ ¸å¿ƒæè¿°: {result.get('core_description', '')[:40]}...")
            else:
                results.append(result)
                error_count += 1
                print(f"   âŒ åˆ†æå¤±è´¥")
            
            # è¯·æ±‚é—´éš”
            if i < len(products):
                time.sleep(REQUEST_DELAY)
            
            # æ¯ 50 ä¸ªä¿å­˜ä¸€æ¬¡ï¼ˆé˜²æ­¢ä¸­æ–­ä¸¢å¤±ï¼‰
            if output_path and i % 50 == 0:
                self._save_results(results, output_path)
                print(f"   ğŸ’¾ å·²ä¿å­˜ {i} æ¡ç»“æœ")
        
        elapsed = time.time() - start_time
        
        # æœ€ç»ˆä¿å­˜
        if output_path:
            self._save_results(results, output_path)
        
        # ç»Ÿè®¡
        print(f"\n" + "=" * 70)
        print(f"ğŸ“Š åˆ†æå®Œæˆ:")
        print(f"   æ€»æ•°: {len(products)}")
        print(f"   æˆåŠŸ: {success_count}")
        print(f"   å¤±è´¥: {error_count}")
        print(f"   è€—æ—¶: {elapsed:.1f} ç§’")
        if output_path:
            print(f"   è¾“å‡º: {output_path}")
        print("=" * 70)
        
        return results
    
    def _save_results(self, results: List[Dict], output_path: Path):
        """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ ¼å¼åŒ–è¾“å‡ºï¼Œæ–¹ä¾¿æŸ¥çœ‹
        output_data = {
            "generated_at": datetime.now().isoformat(),
            "total_count": len(results),
            "success_count": len([r for r in results if "_error" not in r]),
            "error_count": len([r for r in results if "_error" in r]),
            "results": results
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    def import_results(self, json_path: Path) -> Dict[str, int]:
        """
        å°† JSON ç»“æœå¯¼å…¥æ•°æ®åº“
        åŒæ—¶å¯¼å…¥åˆ°:
          - gg_taobao_product_apu: å•†å“ APU è§£æç»“æœ
          - gg_apu_product_rules: æ–°çš„è§„åˆ™åº“ï¼ˆ5 ç»´åº¦ï¼‰
        
        å‚æ•°:
            json_path: JSON æ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            {"apu": å¯¼å…¥æ•°é‡, "rules": å¯¼å…¥æ•°é‡}
        """
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        results = data.get("results", [])
        print(f"ğŸ“‹ å‡†å¤‡å¯¼å…¥ {len(results)} æ¡ç»“æœ")
        
        apu_imported = 0
        rules_imported = 0
        
        for result in results:
            if "_error" in result:
                continue
            
            source = result.get("_source", {})
            product_id = source.get("product_id")
            core_description = result.get("core_description", "")
            category = result.get("category")
            
            if not product_id or not core_description:
                continue
            
            # ====== 1. å¯¼å…¥ gg_taobao_product_apu ======
            apu_record = {
                "product_id": product_id,
                "core_description": core_description,
                "category": category,
                "attribute_keywords": result.get("attribute", {}).get("keywords", []),
                "attribute_description": result.get("attribute", {}).get("description"),
                "performance_keywords": result.get("performance", {}).get("keywords", []),
                "performance_description": result.get("performance", {}).get("description"),
                "use_keywords": result.get("use", {}).get("keywords", []),
                "use_description": result.get("use", {}).get("description"),
                "causal_reasoning": result.get("causal_reasoning"),
                "enhanced_text": result.get("enhanced_text", ""),
            }
            
            try:
                self.supabase.table("gg_taobao_product_apu").upsert(
                    apu_record,
                    on_conflict="product_id"
                ).execute()
                apu_imported += 1
            except Exception as e:
                print(f"   âš ï¸ APU å¯¼å…¥å¤±è´¥ (product_id={product_id}): {e}")
            
            # ====== 2. å¯¼å…¥ gg_apu_product_rules ======
            # æ–°çš„è§„åˆ™åº“ï¼š5 ç»´åº¦ç»“æ„
            rules_record = {
                "category": category or "æœªåˆ†ç±»",
                "product_description": core_description,  # å•†å“æè¿°ä½œä¸ºä¸»è¦ç´¢å¼•
                "attribute_keywords": result.get("attribute", {}).get("keywords", []),
                "performance_keywords": result.get("performance", {}).get("keywords", []),
                "use_keywords": result.get("use", {}).get("keywords", []),
                "is_featured": False,  # é»˜è®¤éç²¾é€‰ï¼Œåç»­å¯æ‰‹åŠ¨æ ‡è®°
                "source": "llm_analysis",
            }
            
            try:
                self.supabase.table("gg_apu_product_rules").upsert(
                    rules_record,
                    on_conflict="product_description"
                ).execute()
                rules_imported += 1
            except Exception as e:
                print(f"   âš ï¸ Rules å¯¼å…¥å¤±è´¥ (desc={core_description[:30]}): {e}")
        
        print(f"\nâœ… å¯¼å…¥å®Œæˆ:")
        print(f"   gg_taobao_product_apu: {apu_imported} æ¡")
        print(f"   gg_apu_product_rules: {rules_imported} æ¡")
        
        return {"apu": apu_imported, "rules": rules_imported}


def load_llm_client() -> OpenAI:
    """åŠ è½½ LLM å®¢æˆ·ç«¯ï¼ˆé€šè¿‡ OpenRouterï¼‰"""
    env_path = Path(__file__).parent.parent.parent / ".env"
    load_dotenv(env_path)
    
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        raise ValueError("æœªæ‰¾åˆ° OPENROUTER_API_KEY")
    
    return OpenAI(
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1"
    )


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å•†å“ APU åˆ†æå·¥å…·")
    parser.add_argument(
        "--output", "-o",
        type=str,
        help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--limit", "-l",
        type=int,
        default=None,
        help="é™åˆ¶åˆ†ææ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    parser.add_argument(
        "--import", "-i",
        dest="import_path",
        type=str,
        help="å¯¼å…¥å·²ç¡®è®¤çš„ JSON æ–‡ä»¶åˆ°æ•°æ®åº“"
    )
    parser.add_argument(
        "--incremental",
        action="store_true",
        help="å¢é‡æ¨¡å¼ï¼šè·³è¿‡å·²åˆ†æçš„å•†å“"
    )
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ğŸ” å•†å“ APU åˆ†æå·¥å…· (v2 - 5 ç»´åº¦è§„åˆ™åº“)")
    print("=" * 70)
    
    # åŠ è½½å®¢æˆ·ç«¯
    supabase = load_supabase_client()
    print("âœ… æˆåŠŸè¿æ¥ Supabase")
    
    llm = load_llm_client()
    print(f"âœ… æˆåŠŸè¿æ¥ LLM ({LLM_MODEL})")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ProductAPUAnalyzer(supabase, llm)
    
    # æ‰§è¡Œæ“ä½œ
    if args.import_path:
        # å¯¼å…¥æ¨¡å¼
        print(f"\nğŸ“¥ å¯¼å…¥æ¨¡å¼: {args.import_path}")
        print("   å°†åŒæ—¶å¯¼å…¥åˆ°:")
        print("   - gg_taobao_product_apu (å•†å“ APU ç»“æœ)")
        print("   - gg_apu_product_rules (è§„åˆ™åº“)")
        analyzer.import_results(Path(args.import_path))
    else:
        # åˆ†ææ¨¡å¼
        output_path = None
        if args.output:
            output_path = Path(args.output)
        else:
            # é»˜è®¤è¾“å‡ºè·¯å¾„
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = OUTPUT_DIR / f"apu_analysis_{timestamp}.json"
        
        print(f"\nğŸ”¬ åˆ†ææ¨¡å¼")
        if args.limit:
            print(f"   é™åˆ¶: {args.limit} æ¡")
        if args.incremental:
            print(f"   å¢é‡æ¨¡å¼: è·³è¿‡å·²åˆ†æçš„å•†å“")
        print(f"   è¾“å‡º: {output_path}")
        
        analyzer.analyze_all_products(
            limit=args.limit,
            output_path=output_path,
            skip_analyzed=args.incremental
        )
        
        print(f"\nğŸ“„ è¯·æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶ç¡®è®¤ç»“æœ:")
        print(f"   {output_path}")
        print(f"\nç¡®è®¤æ— è¯¯åï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤å¯¼å…¥æ•°æ®åº“:")
        print(f"   python analyze_products_apu.py --import {output_path}")


if __name__ == "__main__":
    main()
