"""
æ·˜å®/å¤©çŒ«å•†å“æœç´¢è„šæœ¬ (å«å›¾ç‰‡ä¸‹è½½)
ä½¿ç”¨ Just One API çš„ /api/taobao/search-item-list/v1 æ¥å£
æœç´¢å…³é”®è¯: zara
ç­›é€‰æ¡ä»¶: ä»…å¤©çŒ«å•†å“ (tmall=true), æŒ‰é”€é‡æ’åº (sort=_sale)

ä½¿ç”¨é¡µé¢: ç‹¬ç«‹æµ‹è¯•è„šæœ¬
åŠŸèƒ½: 
  1. æ‰¹é‡è¯·æ±‚æ·˜å®å¤©çŒ«å•†å“æœç´¢æ¥å£
  2. ä¸‹è½½å•†å“ä¸»å›¾å’Œé™„å›¾
  3. å»ºç«‹å•†å“ä¸å›¾ç‰‡çš„å…³è”ç´¢å¼•

ç›®å½•ç»“æ„:
  output/search-item-list/{timestamp}/
  â”œâ”€â”€ pages/                    # åŸå§‹ JSON æ•°æ®
  â”‚   â”œâ”€â”€ page_1.json
  â”‚   â””â”€â”€ ...
  â”œâ”€â”€ images/                   # å›¾ç‰‡ç›®å½•
  â”‚   â””â”€â”€ {itemId}/            # æŒ‰å•†å“ ID åˆ†ç›®å½•
  â”‚       â”œâ”€â”€ main.jpg         # ä¸»å›¾ (picUrlFull)
  â”‚       â”œâ”€â”€ 1.jpg            # é™„å›¾1 (picUrlList[0])
  â”‚       â””â”€â”€ ...
  â”œâ”€â”€ items_index.json          # å•†å“ç´¢å¼•ï¼ˆitemId -> å•†å“ä¿¡æ¯ + å›¾ç‰‡è·¯å¾„æ˜ å°„ï¼‰
  â””â”€â”€ summary.json              # æ±‡æ€»ä¿¡æ¯
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse

# ==================== é…ç½®åŒºåŸŸ ====================
# API åŸºç¡€ URL
BASE_URL = "http://47.117.133.51:30015"

# æœç´¢æ¥å£è·¯å¾„
SEARCH_ENDPOINT = "/api/taobao/search-item-list/v1"

# æœç´¢å‚æ•°é…ç½®
KEYWORD = "zara"  # æœç´¢å…³é”®è¯
SORT = "_sale"    # æ’åºæ–¹å¼: _sale=æŒ‰é”€é‡æ’åº
TMALL = True      # æ˜¯å¦ä»…æœç´¢å¤©çŒ«å•†å“

# å›¾ç‰‡ URL å‰ç¼€ (ç”¨äºæ‹¼æ¥ picUrlList ä¸­çš„ç›¸å¯¹è·¯å¾„)
IMAGE_URL_PREFIX = "https://g.search2.alicdn.com/img/bao/uploaded/i4/"

# ä¸‹è½½é…ç½®
MAX_DOWNLOAD_WORKERS = 5  # å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°
DOWNLOAD_TIMEOUT = 30     # ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
REQUEST_DELAY = 0.5       # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…é¢‘ç‡é™åˆ¶


def load_api_key() -> str:
    """
    ä» .env æ–‡ä»¶åŠ è½½ API Key
    
    è¿”å›:
        str: JUSTONEAPI_API_KEY çš„å€¼
    
    å¼‚å¸¸:
        ValueError: å¦‚æœæœªæ‰¾åˆ° API Key
    """
    # åŠ è½½ backend/.env æ–‡ä»¶
    env_path = Path(__file__).parent.parent.parent / ".env"
    load_dotenv(env_path)
    
    api_key = os.getenv("JUSTONEAPI_API_KEY")
    if not api_key:
        raise ValueError(
            f"æœªæ‰¾åˆ° JUSTONEAPI_API_KEY ç¯å¢ƒå˜é‡ï¼Œè¯·æ£€æŸ¥ {env_path} æ–‡ä»¶"
        )
    
    print(f"âœ… æˆåŠŸåŠ è½½ API Key: {api_key[:8]}...")
    return api_key


def create_output_dirs(timestamp: str = None) -> dict:
    """
    åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
    
    å‚æ•°:
        timestamp: æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨ç”Ÿæˆ
    
    è¿”å›:
        dict: åŒ…å«å„ç›®å½•è·¯å¾„çš„å­—å…¸
            - root: æ ¹ç›®å½•
            - pages: JSON æ•°æ®ç›®å½•
            - images: å›¾ç‰‡ç›®å½•
    """
    # ç”Ÿæˆæ—¶é—´æˆ³ç›®å½•å
    if timestamp is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åˆ›å»ºç›®å½•è·¯å¾„
    root_dir = Path(__file__).parent / "output" / "search-item-list" / timestamp
    pages_dir = root_dir / "pages"
    images_dir = root_dir / "images"
    
    # åˆ›å»ºç›®å½•
    pages_dir.mkdir(parents=True, exist_ok=True)
    images_dir.mkdir(parents=True, exist_ok=True)
    
    dirs = {
        "root": root_dir,
        "pages": pages_dir,
        "images": images_dir,
        "timestamp": timestamp
    }
    
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {root_dir}")
    return dirs


def search_taobao(
    token: str,
    keyword: str,
    page: int,
    sort: str = "_sale",
    tmall: bool = True
) -> dict:
    """
    è°ƒç”¨æ·˜å®/å¤©çŒ«å•†å“æœç´¢æ¥å£
    
    å‚æ•°:
        token: API Token (JUSTONEAPI_API_KEY)
        keyword: æœç´¢å…³é”®è¯
        page: é¡µç  (ä» 1 å¼€å§‹)
        sort: æ’åºæ–¹å¼ (_sale=é”€é‡, _bid=ä»·æ ¼é™åº, bid=ä»·æ ¼å‡åº, _coefp=ç»¼åˆ)
        tmall: æ˜¯å¦ä»…æœç´¢å¤©çŒ«å•†å“
    
    è¿”å›:
        dict: API å“åº”çš„ JSON æ•°æ®
    """
    # æ„å»ºè¯·æ±‚å‚æ•°
    params = {
        "token": token,
        "keyword": keyword,
        "sort": sort,
        "tmall": str(tmall).lower(),
        "page": page
    }
    
    # æ„å»ºå®Œæ•´ URL
    url = f"{BASE_URL}{SEARCH_ENDPOINT}"
    
    print(f"\nğŸ” æ­£åœ¨è¯·æ±‚ç¬¬ {page} é¡µ...")
    print(f"   å‚æ•°: keyword={keyword}, sort={sort}, tmall={tmall}, page={page}")
    
    # å‘é€ GET è¯·æ±‚
    response = requests.get(url, params=params, timeout=30)
    response.raise_for_status()
    
    # è§£æ JSON å“åº”
    data = response.json()
    
    # æ‰“å°å“åº”çŠ¶æ€
    code = data.get("code", "unknown")
    message = data.get("message", "")
    print(f"   å“åº”çŠ¶æ€: code={code}, message={message}")
    
    return data


def save_page_response(pages_dir: Path, page: int, data: dict) -> None:
    """
    ä¿å­˜ API å“åº”åˆ° JSON æ–‡ä»¶
    
    å‚æ•°:
        pages_dir: pages ç›®å½•è·¯å¾„
        page: é¡µç 
        data: API å“åº”æ•°æ®
    """
    filename = f"page_{page}.json"
    filepath = pages_dir / filename
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ’¾ å·²ä¿å­˜: {filepath.name}")


def extract_items_from_response(data: dict) -> list:
    """
    ä» API å“åº”ä¸­æå–å•†å“åˆ—è¡¨
    
    å‚æ•°:
        data: API å“åº”æ•°æ®
    
    è¿”å›:
        list: å•†å“åˆ—è¡¨
    """
    if data.get("code") != 0:
        return []
    
    api_data = data.get("data", {})
    if isinstance(api_data, dict):
        model = api_data.get("model", {})
        if isinstance(model, dict):
            return model.get("itemList", [])
    
    return []


def get_image_extension(url: str) -> str:
    """
    ä» URL ä¸­è·å–å›¾ç‰‡æ‰©å±•å
    
    å‚æ•°:
        url: å›¾ç‰‡ URL
    
    è¿”å›:
        str: æ‰©å±•å (å¦‚ .jpg, .png)
    """
    parsed = urlparse(url)
    path = parsed.path
    ext = os.path.splitext(path)[1].lower()
    
    # é»˜è®¤ä½¿ç”¨ .jpg
    if ext not in [".jpg", ".jpeg", ".png", ".gif", ".webp"]:
        ext = ".jpg"
    
    return ext


def build_full_image_url(relative_path: str) -> str:
    """
    å°†ç›¸å¯¹è·¯å¾„æ‹¼æ¥ä¸ºå®Œæ•´çš„å›¾ç‰‡ URL
    
    å‚æ•°:
        relative_path: ç›¸å¯¹è·¯å¾„ (å¦‚ "i1/2228361831/xxx.jpg")
    
    è¿”å›:
        str: å®Œæ•´çš„å›¾ç‰‡ URL
    """
    return f"{IMAGE_URL_PREFIX}{relative_path}"


def download_image(url: str, save_path: Path) -> bool:
    """
    ä¸‹è½½å•å¼ å›¾ç‰‡
    
    å‚æ•°:
        url: å›¾ç‰‡ URL
        save_path: ä¿å­˜è·¯å¾„
    
    è¿”å›:
        bool: æ˜¯å¦ä¸‹è½½æˆåŠŸ
    """
    try:
        response = requests.get(url, timeout=DOWNLOAD_TIMEOUT, stream=True)
        response.raise_for_status()
        
        with open(save_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        return True
    except Exception as e:
        print(f"      âš ï¸ ä¸‹è½½å¤±è´¥: {url} - {e}")
        return False


def download_item_images(
    item: dict,
    images_dir: Path
) -> dict:
    """
    ä¸‹è½½å•ä¸ªå•†å“çš„æ‰€æœ‰å›¾ç‰‡
    
    å‚æ•°:
        item: å•†å“æ•°æ®
        images_dir: å›¾ç‰‡æ ¹ç›®å½•
    
    è¿”å›:
        dict: å›¾ç‰‡ä¸‹è½½ç»“æœ
            - item_id: å•†å“ ID
            - main_image: ä¸»å›¾æœ¬åœ°è·¯å¾„ (ç›¸å¯¹äº images_dir)
            - sub_images: é™„å›¾æœ¬åœ°è·¯å¾„åˆ—è¡¨
            - main_url: ä¸»å›¾åŸå§‹ URL
            - sub_urls: é™„å›¾åŸå§‹ URL åˆ—è¡¨
    """
    item_id = item.get("itemId")
    if not item_id:
        return None
    
    # åˆ›å»ºå•†å“å›¾ç‰‡ç›®å½•
    item_dir = images_dir / str(item_id)
    item_dir.mkdir(parents=True, exist_ok=True)
    
    result = {
        "item_id": item_id,
        "main_image": None,
        "sub_images": [],
        "main_url": None,
        "sub_urls": []
    }
    
    # ä¸‹è½½ä¸»å›¾ (picUrlFull)
    main_url = item.get("picUrlFull", "")
    if main_url:
        result["main_url"] = main_url
        ext = get_image_extension(main_url)
        main_path = item_dir / f"main{ext}"
        
        if download_image(main_url, main_path):
            # å­˜å‚¨ç›¸å¯¹è·¯å¾„ (ç›¸å¯¹äº images_dir)
            result["main_image"] = f"{item_id}/main{ext}"
    
    # ä¸‹è½½é™„å›¾ (picUrlList)
    pic_list = item.get("picUrlList", [])
    for idx, relative_url in enumerate(pic_list, start=1):
        full_url = build_full_image_url(relative_url)
        result["sub_urls"].append(full_url)
        
        ext = get_image_extension(relative_url)
        sub_path = item_dir / f"{idx}{ext}"
        
        if download_image(full_url, sub_path):
            result["sub_images"].append(f"{item_id}/{idx}{ext}")
    
    return result


def process_items_and_download_images(
    items: list,
    images_dir: Path,
    page: int
) -> list:
    """
    å¤„ç†å•†å“åˆ—è¡¨å¹¶ä¸‹è½½å›¾ç‰‡
    
    å‚æ•°:
        items: å•†å“åˆ—è¡¨
        images_dir: å›¾ç‰‡ç›®å½•
        page: å½“å‰é¡µç  (ç”¨äºæ—¥å¿—)
    
    è¿”å›:
        list: å•†å“ç´¢å¼•åˆ—è¡¨ (åŒ…å«å•†å“ä¿¡æ¯å’Œå›¾ç‰‡è·¯å¾„)
    """
    items_index = []
    
    print(f"\nğŸ“· æ­£åœ¨ä¸‹è½½ç¬¬ {page} é¡µçš„å•†å“å›¾ç‰‡ (å…± {len(items)} ä¸ªå•†å“)...")
    
    for idx, item in enumerate(items, start=1):
        item_id = item.get("itemId")
        item_name = item.get("itemName", "")[:30]  # æˆªå–å‰30ä¸ªå­—ç¬¦
        print(f"   [{idx}/{len(items)}] å•†å“ {item_id}: {item_name}...")
        
        # ä¸‹è½½å›¾ç‰‡
        image_result = download_item_images(item, images_dir)
        
        if image_result:
            # æ„å»ºå•†å“ç´¢å¼•æ¡ç›®
            index_entry = {
                "item_id": item_id,
                "item_name": item.get("itemName"),
                "shop_id": item.get("shopId"),
                "shop_name": item.get("shopName"),
                "price_yuan": item.get("priceYuanDouble"),
                "discount_price_yuan": item.get("discntPriceYuan"),
                "order_count": item.get("orderPayUV"),
                "item_loc": item.get("itemLoc"),
                "page": page,
                # å›¾ç‰‡ä¿¡æ¯
                "images": {
                    "main": {
                        "url": image_result["main_url"],
                        "local_path": image_result["main_image"]
                    },
                    "sub": [
                        {"url": url, "local_path": path}
                        for url, path in zip(
                            image_result["sub_urls"],
                            image_result["sub_images"]
                        )
                    ]
                }
            }
            items_index.append(index_entry)
            
            # ç»Ÿè®¡ä¸‹è½½æˆåŠŸæ•°
            main_ok = 1 if image_result["main_image"] else 0
            sub_ok = len(image_result["sub_images"])
            sub_total = len(image_result["sub_urls"])
            print(f"      âœ… ä¸»å›¾: {main_ok}/1, é™„å›¾: {sub_ok}/{sub_total}")
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.1)
    
    return items_index


def save_items_index(root_dir: Path, items_index: list) -> None:
    """
    ä¿å­˜å•†å“ç´¢å¼•æ–‡ä»¶
    
    å‚æ•°:
        root_dir: æ ¹ç›®å½•
        items_index: å•†å“ç´¢å¼•åˆ—è¡¨
    """
    index_path = root_dir / "items_index.json"
    
    with open(index_path, "w", encoding="utf-8") as f:
        json.dump(items_index, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“‹ å•†å“ç´¢å¼•å·²ä¿å­˜: {index_path}")
    print(f"   å…± {len(items_index)} ä¸ªå•†å“")


def save_summary(root_dir: Path, pages_results: list, items_index: list) -> None:
    """
    ä¿å­˜æ±‡æ€»ä¿¡æ¯
    
    å‚æ•°:
        root_dir: æ ¹ç›®å½•
        pages_results: é¡µé¢è¯·æ±‚ç»“æœåˆ—è¡¨ [(page, data), ...]
        items_index: å•†å“ç´¢å¼•åˆ—è¡¨
    """
    # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
    total_main_images = sum(
        1 for item in items_index 
        if item.get("images", {}).get("main", {}).get("local_path")
    )
    total_sub_images = sum(
        len(item.get("images", {}).get("sub", []))
        for item in items_index
    )
    
    summary = {
        "search_time": datetime.now().isoformat(),
        "keyword": KEYWORD,
        "sort": SORT,
        "tmall": TMALL,
        "total_pages": len(pages_results),
        "total_items": len(items_index),
        "total_main_images": total_main_images,
        "total_sub_images": total_sub_images,
        "pages": []
    }
    
    for page, data in pages_results:
        items = extract_items_from_response(data)
        page_info = {
            "page": page,
            "code": data.get("code"),
            "message": data.get("message", ""),
            "item_count": len(items)
        }
        summary["pages"].append(page_info)
    
    summary_path = root_dir / "summary.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š æ±‡æ€»ä¿¡æ¯å·²ä¿å­˜: {summary_path}")
    print(f"   æ€»é¡µæ•°: {len(pages_results)}")
    print(f"   æ€»å•†å“æ•°: {len(items_index)}")
    print(f"   ä¸»å›¾æ•°é‡: {total_main_images}")
    print(f"   é™„å›¾æ•°é‡: {total_sub_images}")


def fetch_and_download(
    api_key: str,
    pages: list,
    dirs: dict
) -> tuple:
    """
    è·å–å•†å“æ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡
    
    å‚æ•°:
        api_key: API Key
        pages: é¡µç åˆ—è¡¨
        dirs: ç›®å½•è·¯å¾„å­—å…¸
    
    è¿”å›:
        tuple: (pages_results, items_index)
    """
    pages_results = []
    all_items_index = []
    
    for page in pages:
        try:
            # è¯·æ±‚æ•°æ®
            data = search_taobao(
                token=api_key,
                keyword=KEYWORD,
                page=page,
                sort=SORT,
                tmall=TMALL
            )
            
            # ä¿å­˜ JSON å“åº”
            save_page_response(dirs["pages"], page, data)
            pages_results.append((page, data))
            
            # æå–å•†å“åˆ—è¡¨
            items = extract_items_from_response(data)
            
            # ä¸‹è½½å›¾ç‰‡å¹¶å»ºç«‹ç´¢å¼•
            if items:
                page_index = process_items_and_download_images(
                    items, dirs["images"], page
                )
                all_items_index.extend(page_index)
            
            # è¯·æ±‚é—´éš”
            time.sleep(REQUEST_DELAY)
            
        except requests.RequestException as e:
            print(f"   âŒ è¯·æ±‚å¤±è´¥: {e}")
            error_data = {"error": str(e), "page": page}
            save_page_response(dirs["pages"], page, error_data)
            pages_results.append((page, error_data))
    
    return pages_results, all_items_index


def main():
    """
    ä¸»å‡½æ•°: æ‰§è¡Œæ·˜å®å•†å“æœç´¢å¹¶ä¸‹è½½å›¾ç‰‡
    """
    print("=" * 70)
    print("ğŸ›’ æ·˜å®/å¤©çŒ«å•†å“æœç´¢è„šæœ¬ (å«å›¾ç‰‡ä¸‹è½½)")
    print("=" * 70)
    
    # é…ç½®è¦è·å–çš„é¡µç  (1-30 é¡µ)
    pages = list(range(1, 31))  # ç¬¬ 1-30 é¡µ
    
    print(f"\nğŸ“‹ ä»»åŠ¡é…ç½®:")
    print(f"   å…³é”®è¯: {KEYWORD}")
    print(f"   ä»…å¤©çŒ«: {TMALL}")
    print(f"   æ’åº: {SORT}")
    print(f"   é¡µç èŒƒå›´: {pages[0]}-{pages[-1]} (å…± {len(pages)} é¡µ)")
    
    # 1. åŠ è½½ API Key
    api_key = load_api_key()
    
    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    dirs = create_output_dirs()
    
    # 3. è·å–æ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡
    pages_results, items_index = fetch_and_download(api_key, pages, dirs)
    
    # 4. ä¿å­˜å•†å“ç´¢å¼•
    save_items_index(dirs["root"], items_index)
    
    # 5. ä¿å­˜æ±‡æ€»ä¿¡æ¯
    save_summary(dirs["root"], pages_results, items_index)
    
    print("\n" + "=" * 70)
    print("âœ… ä»»åŠ¡å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœç›®å½•: {dirs['root']}")
    print("=" * 70)


if __name__ == "__main__":
    main()
