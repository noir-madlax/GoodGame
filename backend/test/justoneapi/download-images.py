"""
æ·˜å®å•†å“å›¾ç‰‡ä¸‹è½½è„šæœ¬ (ä»…ä¸‹è½½å›¾ç‰‡ï¼Œä¸è¯·æ±‚ API)
ä»å·²æœ‰çš„ JSON æ•°æ®ä¸­æå–å›¾ç‰‡ URL å¹¶ä¸‹è½½

ä½¿ç”¨é¡µé¢: ç‹¬ç«‹æµ‹è¯•è„šæœ¬
åŠŸèƒ½: 
  1. è¯»å–å·²æœ‰çš„ page_*.json æ–‡ä»¶
  2. ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œä¸‹è½½å•†å“ä¸»å›¾å’Œé™„å›¾
  3. è·³è¿‡å·²ä¸‹è½½çš„å›¾ç‰‡
  4. å»ºç«‹å•†å“ä¸å›¾ç‰‡çš„å…³è”ç´¢å¼•

æ³¨æ„: 
  - ä½¿ç”¨ HTTP åè®®ä¸‹è½½å›¾ç‰‡ï¼Œé¿å… SSL è¯ä¹¦é—®é¢˜
  - ä¸ä¼šé‡æ–°è¯·æ±‚ APIï¼Œåªè¯»å–æœ¬åœ° JSON æ–‡ä»¶
  - å·²å­˜åœ¨çš„å›¾ç‰‡ä¼šè‡ªåŠ¨è·³è¿‡
"""

import os
import json
import requests
import time
import urllib3
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# ç¦ç”¨ SSL è­¦å‘Š (å› ä¸ºæˆ‘ä»¬ä½¿ç”¨ HTTP)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==================== é…ç½®åŒºåŸŸ ====================
# æºæ•°æ®ç›®å½• (åŒ…å« pages å­ç›®å½•çš„æ—¶é—´æˆ³ç›®å½•)
SOURCE_DIR = Path(__file__).parent / "output" / "search-item-list" / "20251202_144931"

# å›¾ç‰‡ URL å‰ç¼€ (ç”¨äºæ‹¼æ¥ picUrlList ä¸­çš„ç›¸å¯¹è·¯å¾„) - ä½¿ç”¨ HTTP
IMAGE_URL_PREFIX = "http://g.search2.alicdn.com/img/bao/uploaded/i4/"

# ä¸‹è½½é…ç½®
DOWNLOAD_TIMEOUT = 30     # ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
RETRY_COUNT = 3           # é‡è¯•æ¬¡æ•°
RETRY_DELAY = 1           # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
MAX_WORKERS = 5           # å¹¶è¡Œä¸‹è½½çº¿ç¨‹æ•°

# ç»Ÿè®¡è®¡æ•°å™¨ (çº¿ç¨‹å®‰å…¨)
stats_lock = Lock()
stats = {
    "downloaded": 0,    # æ–°ä¸‹è½½çš„å›¾ç‰‡æ•°
    "skipped": 0,       # è·³è¿‡çš„å›¾ç‰‡æ•° (å·²å­˜åœ¨)
    "failed": 0         # ä¸‹è½½å¤±è´¥çš„å›¾ç‰‡æ•°
}


def update_stats(key: str, count: int = 1):
    """çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»Ÿè®¡è®¡æ•°"""
    with stats_lock:
        stats[key] += count


def load_page_data(pages_dir: Path) -> list:
    """
    åŠ è½½æ‰€æœ‰é¡µé¢çš„ JSON æ•°æ®
    
    å‚æ•°:
        pages_dir: pages ç›®å½•è·¯å¾„
    
    è¿”å›:
        list: [(page_num, data), ...] æŒ‰é¡µç æ’åº
    """
    pages_data = []
    
    # æŸ¥æ‰¾æ‰€æœ‰ page_*.json æ–‡ä»¶
    for json_file in sorted(pages_dir.glob("page_*.json")):
        try:
            # æå–é¡µç 
            page_num = int(json_file.stem.split("_")[1])
            
            # è¯»å– JSON æ•°æ®
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            pages_data.append((page_num, data))
            
        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {json_file.name} - {e}")
    
    # æŒ‰é¡µç æ’åº
    pages_data.sort(key=lambda x: x[0])
    
    return pages_data


def extract_items_from_response(data: dict) -> list:
    """
    ä» API å“åº”ä¸­æå–å•†å“åˆ—è¡¨
    """
    if data.get("code") != 0:
        return []
    
    api_data = data.get("data", {})
    if isinstance(api_data, dict):
        model = api_data.get("model", {})
        if isinstance(model, dict):
            return model.get("itemList", [])
    
    return []


def get_image_extension(url: str) -> str:
    """ä» URL ä¸­è·å–å›¾ç‰‡æ‰©å±•å"""
    path = url.split("?")[0]
    ext = os.path.splitext(path)[1].lower()
    
    if ext not in [".jpg", ".jpeg", ".png", ".gif", ".webp"]:
        ext = ".jpg"
    
    return ext


def convert_to_http_url(url: str) -> str:
    """å°† HTTPS URL è½¬æ¢ä¸º HTTP URL"""
    if url.startswith("https://"):
        return url.replace("https://", "http://", 1)
    return url


def build_full_image_url(relative_path: str) -> str:
    """å°†ç›¸å¯¹è·¯å¾„æ‹¼æ¥ä¸ºå®Œæ•´çš„å›¾ç‰‡ URL (HTTP)"""
    return f"{IMAGE_URL_PREFIX}{relative_path}"


def download_single_image(url: str, save_path: Path) -> tuple:
    """
    ä¸‹è½½å•å¼ å›¾ç‰‡ (å¸¦é‡è¯•æœºåˆ¶)
    
    å‚æ•°:
        url: å›¾ç‰‡ URL
        save_path: ä¿å­˜è·¯å¾„
    
    è¿”å›:
        tuple: (success: bool, skipped: bool, local_path: str or None)
    """
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if save_path.exists() and save_path.stat().st_size > 0:
        return (True, True, str(save_path.relative_to(save_path.parent.parent)))
    
    # è½¬æ¢ä¸º HTTP
    url = convert_to_http_url(url)
    
    # æ·»åŠ è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.taobao.com/",
        "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }
    
    for attempt in range(RETRY_COUNT):
        try:
            response = requests.get(
                url, 
                headers=headers,
                timeout=DOWNLOAD_TIMEOUT, 
                stream=True,
                verify=False
            )
            response.raise_for_status()
            
            # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(save_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return (True, False, str(save_path.relative_to(save_path.parent.parent)))
            
        except Exception as e:
            if attempt < RETRY_COUNT - 1:
                time.sleep(RETRY_DELAY)
            else:
                return (False, False, None)
    
    return (False, False, None)


def download_item_images_task(item: dict, images_dir: Path, page: int) -> dict:
    """
    ä¸‹è½½å•ä¸ªå•†å“çš„æ‰€æœ‰å›¾ç‰‡ (ç”¨äºçº¿ç¨‹æ± )
    
    å‚æ•°:
        item: å•†å“æ•°æ®
        images_dir: å›¾ç‰‡æ ¹ç›®å½•
        page: é¡µç 
    
    è¿”å›:
        dict: å•†å“ç´¢å¼•æ¡ç›®
    """
    item_id = item.get("itemId")
    if not item_id:
        return None
    
    item_dir = images_dir / str(item_id)
    
    result = {
        "item_id": item_id,
        "main_image": None,
        "sub_images": [],
        "main_url": None,
        "sub_urls": [],
        "main_skipped": False,
        "sub_skipped": 0,
        "sub_downloaded": 0,
        "sub_failed": 0
    }
    
    # ä¸‹è½½ä¸»å›¾ (picUrlFull)
    main_url = item.get("picUrlFull", "")
    if main_url:
        result["main_url"] = main_url
        ext = get_image_extension(main_url)
        main_path = item_dir / f"main{ext}"
        
        success, skipped, local_path = download_single_image(main_url, main_path)
        if success:
            result["main_image"] = local_path
            result["main_skipped"] = skipped
            if skipped:
                update_stats("skipped")
            else:
                update_stats("downloaded")
        else:
            update_stats("failed")
    
    # ä¸‹è½½é™„å›¾ (picUrlList)
    pic_list = item.get("picUrlList", [])
    for idx, relative_url in enumerate(pic_list, start=1):
        full_url = build_full_image_url(relative_url)
        result["sub_urls"].append(full_url)
        
        ext = get_image_extension(relative_url)
        sub_path = item_dir / f"{idx}{ext}"
        
        success, skipped, local_path = download_single_image(full_url, sub_path)
        if success:
            result["sub_images"].append(local_path)
            if skipped:
                result["sub_skipped"] += 1
                update_stats("skipped")
            else:
                result["sub_downloaded"] += 1
                update_stats("downloaded")
        else:
            result["sub_failed"] += 1
            update_stats("failed")
    
    # æ„å»ºå•†å“ç´¢å¼•æ¡ç›®
    index_entry = {
        "item_id": item_id,
        "item_name": item.get("itemName"),
        "shop_id": item.get("shopId"),
        "shop_name": item.get("shopName"),
        "price_yuan": item.get("priceYuanDouble"),
        "discount_price_yuan": item.get("discntPriceYuan"),
        "order_count": item.get("orderPayUV"),
        "item_loc": item.get("itemLoc"),
        "page": page,
        "images": {
            "main": {
                "url": result["main_url"],
                "local_path": result["main_image"]
            },
            "sub": [
                {"url": url, "local_path": path}
                for url, path in zip(result["sub_urls"], result["sub_images"])
            ]
        }
    }
    
    return index_entry


def process_all_items_parallel(pages_data: list, images_dir: Path) -> list:
    """
    ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ‰€æœ‰å•†å“
    
    å‚æ•°:
        pages_data: é¡µé¢æ•°æ®åˆ—è¡¨ [(page, data), ...]
        images_dir: å›¾ç‰‡ç›®å½•
    
    è¿”å›:
        list: å•†å“ç´¢å¼•åˆ—è¡¨
    """
    # æ”¶é›†æ‰€æœ‰å•†å“ä»»åŠ¡
    all_tasks = []
    for page, data in pages_data:
        items = extract_items_from_response(data)
        for item in items:
            all_tasks.append((item, images_dir, page))
    
    total_items = len(all_tasks)
    print(f"\nğŸ“¦ å…± {total_items} ä¸ªå•†å“å¾…å¤„ç†ï¼Œä½¿ç”¨ {MAX_WORKERS} ä¸ªçº¿ç¨‹å¹¶è¡Œä¸‹è½½...")
    
    all_items_index = []
    completed_count = 0
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_item = {
            executor.submit(download_item_images_task, item, images_dir, page): (item, page)
            for item, images_dir, page in all_tasks
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_item):
            item, page = future_to_item[future]
            completed_count += 1
            
            try:
                index_entry = future.result()
                if index_entry:
                    all_items_index.append(index_entry)
                    
                    # æ‰“å°è¿›åº¦
                    item_id = index_entry["item_id"]
                    item_name = (index_entry.get("item_name") or "")[:25]
                    
                    with stats_lock:
                        current_stats = f"[ä¸‹è½½:{stats['downloaded']} è·³è¿‡:{stats['skipped']} å¤±è´¥:{stats['failed']}]"
                    
                    print(f"   [{completed_count}/{total_items}] å•†å“ {item_id}: {item_name}... {current_stats}")
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
    
    # æŒ‰é¡µç å’Œå•†å“IDæ’åº
    all_items_index.sort(key=lambda x: (x["page"], x["item_id"]))
    
    return all_items_index


def save_items_index(root_dir: Path, items_index: list) -> None:
    """ä¿å­˜å•†å“ç´¢å¼•æ–‡ä»¶"""
    index_path = root_dir / "items_index.json"
    
    with open(index_path, "w", encoding="utf-8") as f:
        json.dump(items_index, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“‹ å•†å“ç´¢å¼•å·²ä¿å­˜: {index_path}")
    print(f"   å…± {len(items_index)} ä¸ªå•†å“")


def save_summary(root_dir: Path, pages_data: list, items_index: list) -> None:
    """ä¿å­˜æ±‡æ€»ä¿¡æ¯"""
    total_main_images = sum(
        1 for item in items_index 
        if item.get("images", {}).get("main", {}).get("local_path")
    )
    total_sub_images = sum(
        len([s for s in item.get("images", {}).get("sub", []) if s.get("local_path")])
        for item in items_index
    )
    
    summary = {
        "download_time": datetime.now().isoformat(),
        "source_dir": str(SOURCE_DIR),
        "total_pages": len(pages_data),
        "total_items": len(items_index),
        "total_main_images": total_main_images,
        "total_sub_images": total_sub_images,
        "download_stats": {
            "downloaded": stats["downloaded"],
            "skipped": stats["skipped"],
            "failed": stats["failed"]
        },
        "pages": []
    }
    
    for page, data in pages_data:
        items = extract_items_from_response(data)
        page_info = {
            "page": page,
            "code": data.get("code"),
            "item_count": len(items)
        }
        summary["pages"].append(page_info)
    
    summary_path = root_dir / "summary.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š æ±‡æ€»ä¿¡æ¯å·²ä¿å­˜: {summary_path}")
    print(f"   æ€»é¡µæ•°: {len(pages_data)}")
    print(f"   æ€»å•†å“æ•°: {len(items_index)}")
    print(f"   ä¸»å›¾æ•°é‡: {total_main_images}")
    print(f"   é™„å›¾æ•°é‡: {total_sub_images}")


def main():
    """ä¸»å‡½æ•°: ä»å·²æœ‰ JSON æ•°æ®ä¸‹è½½å•†å“å›¾ç‰‡"""
    print("=" * 70)
    print("ğŸ–¼ï¸  æ·˜å®å•†å“å›¾ç‰‡ä¸‹è½½è„šæœ¬ (å¤šçº¿ç¨‹å¹¶è¡Œä¸‹è½½)")
    print("=" * 70)
    
    pages_dir = SOURCE_DIR / "pages"
    images_dir = SOURCE_DIR / "images"
    
    if not pages_dir.exists():
        print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {pages_dir}")
        return
    
    images_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“‚ æºæ•°æ®ç›®å½•: {SOURCE_DIR}")
    print(f"ğŸ“‚ å›¾ç‰‡è¾“å‡ºç›®å½•: {images_dir}")
    print(f"ğŸ§µ å¹¶è¡Œçº¿ç¨‹æ•°: {MAX_WORKERS}")
    
    # 1. åŠ è½½æ‰€æœ‰é¡µé¢æ•°æ®
    print(f"\nğŸ“– æ­£åœ¨åŠ è½½ JSON æ•°æ®...")
    pages_data = load_page_data(pages_dir)
    print(f"   âœ… å…±åŠ è½½ {len(pages_data)} ä¸ªé¡µé¢")
    
    # 2. å¹¶è¡Œå¤„ç†æ‰€æœ‰å•†å“å¹¶ä¸‹è½½å›¾ç‰‡
    all_items_index = process_all_items_parallel(pages_data, images_dir)
    
    # 3. ä¿å­˜å•†å“ç´¢å¼•
    save_items_index(SOURCE_DIR, all_items_index)
    
    # 4. ä¿å­˜æ±‡æ€»ä¿¡æ¯
    save_summary(SOURCE_DIR, pages_data, all_items_index)
    
    print("\n" + "=" * 70)
    print("âœ… å›¾ç‰‡ä¸‹è½½å®Œæˆï¼")
    print(f"   ğŸ“¥ æ–°ä¸‹è½½: {stats['downloaded']} å¼ ")
    print(f"   â­ï¸  è·³è¿‡: {stats['skipped']} å¼  (å·²å­˜åœ¨)")
    print(f"   âŒ å¤±è´¥: {stats['failed']} å¼ ")
    print(f"ğŸ“ ç»“æœç›®å½•: {SOURCE_DIR}")
    print("=" * 70)


if __name__ == "__main__":
    main()
