"""
å•†å“å›¾ç‰‡å‘é‡åŒ–è„šæœ¬
ä½¿ç”¨ CLIP æ¨¡å‹ç”Ÿæˆå›¾ç‰‡å‘é‡ï¼Œæ”¯æŒå›¾ç‰‡ç›¸ä¼¼åº¦æœç´¢

ä½¿ç”¨é¡µé¢: ç‹¬ç«‹æµ‹è¯•è„šæœ¬
åŠŸèƒ½:
  1. ä»æœ¬åœ°è¯»å–å•†å“å›¾ç‰‡
  2. ä½¿ç”¨ CLIP æ¨¡å‹ç”Ÿæˆå‘é‡
  3. å­˜å‚¨åˆ° gg_taobao_image_embeddings è¡¨

æ¨¡å‹ä¿¡æ¯:
  - æ¨¡å‹: openai/clip-vit-base-patch32
  - å‘é‡ç»´åº¦: 512
  - æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ API è´¹ç”¨

ä¾èµ–å®‰è£…:
  pip install transformers torch pillow
"""

import os
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from supabase import create_client, Client
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

# ==================== é…ç½®åŒºåŸŸ ====================
# æºæ•°æ®ç›®å½•
SOURCE_DIR = Path(__file__).parent / "output" / "search-item-list" / "20251202_144931"
IMAGES_DIR = SOURCE_DIR / "images"

# CLIP æ¨¡å‹é…ç½®
CLIP_MODEL_NAME = "openai/clip-vit-base-patch32"
EMBEDDING_DIMENSIONS = 512  # clip-vit-base-patch32 çš„å‘é‡ç»´åº¦

# å¹¶å‘é…ç½®
MAX_WORKERS = 4  # å›¾ç‰‡å¤„ç†çº¿ç¨‹æ•°
BATCH_SIZE = 20  # æ•°æ®åº“æ‰¹é‡å†™å…¥å¤§å°


class CLIPEmbedder:
    """CLIP å›¾ç‰‡å‘é‡ç”Ÿæˆå™¨"""
    
    def __init__(self, model_name: str = CLIP_MODEL_NAME):
        """
        åˆå§‹åŒ– CLIP æ¨¡å‹
        
        å‚æ•°:
            model_name: CLIP æ¨¡å‹åç§°
        """
        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹: {model_name}")
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"   ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.model = CLIPModel.from_pretrained(model_name).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        print(f"âœ… CLIP æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def get_image_embedding(self, image_path: str) -> list:
        """
        ç”Ÿæˆå•å¼ å›¾ç‰‡çš„å‘é‡
        
        å‚æ•°:
            image_path: å›¾ç‰‡è·¯å¾„
        
        è¿”å›:
            list: 512 ç»´å‘é‡
        """
        try:
            # åŠ è½½å›¾ç‰‡
            image = Image.open(image_path).convert("RGB")
            
            # å¤„ç†å›¾ç‰‡
            inputs = self.processor(images=image, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆå‘é‡
            with torch.no_grad():
                image_features = self.model.get_image_features(**inputs)
            
            # å½’ä¸€åŒ–
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            embedding = image_features.cpu().numpy()[0].tolist()
            
            return embedding
            
        except Exception as e:
            print(f"   âš ï¸ å›¾ç‰‡å¤„ç†å¤±è´¥ {image_path}: {e}")
            return None


def load_supabase_client() -> Client:
    """åŠ è½½ Supabase å®¢æˆ·ç«¯"""
    env_path = Path(__file__).parent.parent.parent / ".env"
    load_dotenv(env_path)
    
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_KEY")
    
    if not url or not key:
        raise ValueError("æœªæ‰¾åˆ° SUPABASE_URL æˆ– SUPABASE_KEY")
    
    print(f"âœ… æˆåŠŸè¿æ¥ Supabase")
    return create_client(url, key)


def get_images_without_embeddings(supabase: Client, include_sub: bool = True) -> list:
    """
    è·å–è¿˜æ²¡æœ‰ç”Ÿæˆå‘é‡çš„å›¾ç‰‡
    
    å‚æ•°:
        supabase: Supabase å®¢æˆ·ç«¯
        include_sub: æ˜¯å¦åŒ…å«å‰¯å›¾ (é»˜è®¤ Trueï¼Œå¤„ç†æ‰€æœ‰å›¾ç‰‡)
    """
    # è·å–æ‰€æœ‰å›¾ç‰‡ (ä¸»å›¾ + å‰¯å›¾)
    # æ³¨æ„: Supabase é»˜è®¤åªè¿”å› 1000 æ¡ï¼Œéœ€è¦åˆ†é¡µè·å–å…¨éƒ¨
    all_images = []
    offset = 0
    limit = 1000
    
    while True:
        query = supabase.table("gg_taobao_product_images").select(
            "id, product_id, item_id, image_type, image_index, storage_path"
        ).range(offset, offset + limit - 1)
        
        if not include_sub:
            query = query.eq("image_type", "main")
        
        result = query.execute()
        all_images.extend(result.data)
        
        if len(result.data) < limit:
            break
        offset += limit
    
    # è·å–å·²æœ‰å‘é‡çš„å›¾ç‰‡ ID (åŒæ ·éœ€è¦åˆ†é¡µ)
    all_embeddings = []
    offset = 0
    
    while True:
        result = supabase.table("gg_taobao_image_embeddings").select(
            "image_id"
        ).range(offset, offset + limit - 1).execute()
        all_embeddings.extend(result.data)
        
        if len(result.data) < limit:
            break
        offset += limit
    
    existing_ids = {row["image_id"] for row in all_embeddings}
    
    # è¿‡æ»¤å‡ºæœªç”Ÿæˆå‘é‡çš„å›¾ç‰‡
    images = []
    for img in all_images:
        if img["id"] not in existing_ids:
            # æ„å»ºæœ¬åœ°è·¯å¾„
            item_id = img["item_id"]
            image_type = img["image_type"]
            image_index = img["image_index"]
            
            # æ ¹æ®å›¾ç‰‡ç±»å‹æ„å»ºæ–‡ä»¶å
            if image_type == "main":
                filename = "main.jpg"
            else:
                filename = f"{image_index}.jpg"
            
            local_path = IMAGES_DIR / str(item_id) / filename
            
            if local_path.exists():
                img["local_path"] = str(local_path)
                images.append(img)
    
    return images


def process_single_image(embedder: CLIPEmbedder, image_info: dict) -> dict:
    """
    å¤„ç†å•å¼ å›¾ç‰‡ï¼Œç”Ÿæˆå‘é‡
    
    å‚æ•°:
        embedder: CLIP å‘é‡ç”Ÿæˆå™¨
        image_info: å›¾ç‰‡ä¿¡æ¯
    
    è¿”å›:
        dict: åŒ…å«å‘é‡çš„ç»“æœ
    """
    local_path = image_info["local_path"]
    
    embedding = embedder.get_image_embedding(local_path)
    
    if embedding:
        return {
            "image_id": image_info["id"],
            "product_id": image_info["product_id"],
            "item_id": image_info["item_id"],
            "embedding_model": CLIP_MODEL_NAME,
            "embedding": embedding,
            "success": True
        }
    else:
        return {
            "image_id": image_info["id"],
            "success": False
        }


def save_embeddings_batch(supabase: Client, embeddings: list) -> int:
    """æ‰¹é‡ä¿å­˜å‘é‡åˆ°æ•°æ®åº“"""
    if not embeddings:
        return 0
    
    # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
    valid_embeddings = [
        {
            "image_id": e["image_id"],
            "product_id": e["product_id"],
            "item_id": e["item_id"],
            "embedding_model": e["embedding_model"],
            "embedding": e["embedding"]
        }
        for e in embeddings if e["success"]
    ]
    
    if not valid_embeddings:
        return 0
    
    try:
        supabase.table("gg_taobao_image_embeddings").upsert(
            valid_embeddings,
            on_conflict="image_id"
        ).execute()
        return len(valid_embeddings)
    except Exception as e:
        print(f"   âš ï¸ æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")
        return 0


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸ–¼ï¸  å•†å“å›¾ç‰‡å‘é‡åŒ– (CLIP)")
    print("=" * 70)
    
    # 1. åŠ è½½ Supabase å®¢æˆ·ç«¯
    supabase = load_supabase_client()
    
    # 2. åŠ è½½ CLIP æ¨¡å‹
    embedder = CLIPEmbedder()
    
    # 3. è·å–å¾…å¤„ç†å›¾ç‰‡ (åŒ…å«ä¸»å›¾å’Œå‰¯å›¾)
    print(f"\nğŸ“‹ æ­£åœ¨è·å–å¾…å¤„ç†å›¾ç‰‡...")
    images = get_images_without_embeddings(supabase, include_sub=True)
    print(f"   å…± {len(images)} å¼ å›¾ç‰‡å¾…å¤„ç† (ä¸»å›¾ + å‰¯å›¾)")
    
    if not images:
        print("\nâœ… æ‰€æœ‰å›¾ç‰‡å·²å®Œæˆå‘é‡åŒ–ï¼")
        return
    
    # 4. å¤„ç†å›¾ç‰‡ (ä½¿ç”¨çº¿ç¨‹æ± )
    print(f"\nğŸš€ å¼€å§‹ç”Ÿæˆå‘é‡...")
    
    total_processed = 0
    total_success = 0
    batch_results = []
    
    start_time = time.time()
    
    # ç”±äº CLIP æ¨¡å‹åœ¨ GPU/MPS ä¸Šè¿è¡Œï¼Œä½¿ç”¨è¾ƒå°‘çš„çº¿ç¨‹é¿å…å†…å­˜é—®é¢˜
    for i, image_info in enumerate(images, 1):
        result = process_single_image(embedder, image_info)
        batch_results.append(result)
        
        if result["success"]:
            total_success += 1
        
        # æ‰¹é‡ä¿å­˜
        if len(batch_results) >= BATCH_SIZE:
            saved = save_embeddings_batch(supabase, batch_results)
            batch_results = []
            
            elapsed = time.time() - start_time
            speed = i / elapsed if elapsed > 0 else 0
            print(f"   âœ… è¿›åº¦: {i}/{len(images)} (æˆåŠŸ: {total_success}, é€Ÿåº¦: {speed:.1f} å¼ /ç§’)")
        
        total_processed += 1
    
    # ä¿å­˜å‰©ä½™çš„
    if batch_results:
        save_embeddings_batch(supabase, batch_results)
    
    # 5. ç»Ÿè®¡
    elapsed = time.time() - start_time
    
    print(f"\nğŸ“Š å¤„ç†å®Œæˆ:")
    print(f"   å¤„ç†å›¾ç‰‡æ•°: {total_processed}")
    print(f"   æˆåŠŸæ•°: {total_success}")
    print(f"   è€—æ—¶: {elapsed:.1f} ç§’")
    print(f"   å¹³å‡é€Ÿåº¦: {total_processed / elapsed:.1f} å¼ /ç§’")
    
    # éªŒè¯
    result = supabase.table("gg_taobao_image_embeddings").select(
        "id", count="exact"
    ).execute()
    
    print(f"   æ•°æ®åº“å‘é‡æ•°: {result.count}")
    
    print("\n" + "=" * 70)
    print("âœ… å›¾ç‰‡å‘é‡åŒ–å®Œæˆï¼")
    print("=" * 70)


if __name__ == "__main__":
    main()

