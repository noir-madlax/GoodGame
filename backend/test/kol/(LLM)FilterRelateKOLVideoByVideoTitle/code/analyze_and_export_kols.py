import os
import json
import time
import asyncio
import aiohttp
import csv
from typing import List, Dict, Set
from supabase import create_client, Client

# é…ç½®
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_MODEL = os.getenv("OPENROUTER_MODEL", "openai/gpt-4o-mini")

# è·¯å¾„é…ç½®
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
OUTPUT_DIR = os.path.join(BASE_DIR, "output")
CACHE_FILE = os.path.join(OUTPUT_DIR, "video_analysis_cache.jsonl")
FULL_KOL_LIST_JSON = os.path.join(OUTPUT_DIR, "all_kols_skincare_stats.json")
FULL_KOL_LIST_CSV = os.path.join(OUTPUT_DIR, "all_kols_skincare_stats.csv")

if not all([SUPABASE_URL, SUPABASE_KEY, OPENROUTER_API_KEY]):
    print("âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def load_cache() -> Dict[str, bool]:
    """åŠ è½½æœ¬åœ°ç¼“å­˜çš„åˆ†æç»“æœ"""
    cache = {}
    if os.path.exists(CACHE_FILE):
        print(f"ğŸ“– è¯»å–æœ¬åœ°ç¼“å­˜: {CACHE_FILE}")
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    data = json.loads(line)
                    cache.update(data)
                except json.JSONDecodeError:
                    continue
        print(f"âœ… å·²åŠ è½½ {len(cache)} æ¡ç¼“å­˜è®°å½•")
    return cache

def append_to_cache(results: Dict[str, bool]):
    """å°†æ–°ç»“æœè¿½åŠ åˆ°æœ¬åœ°ç¼“å­˜"""
    with open(CACHE_FILE, "a", encoding="utf-8") as f:
        for vid, is_related in results.items():
            json.dump({vid: is_related}, f, ensure_ascii=False)
            f.write("\n")

async def analyze_titles_batch(session: aiohttp.ClientSession, titles_batch: List[Dict]) -> Dict[str, bool]:
    """è°ƒç”¨ LLM åˆ†æä¸€æ‰¹æ ‡é¢˜"""
    prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå¸ˆï¼Œä¸“æ³¨äºç¾å¦†å’ŒæŠ¤è‚¤é¢†åŸŸã€‚
è¯·åˆ†æä»¥ä¸‹æŠ–éŸ³è§†é¢‘æ ‡é¢˜ï¼Œåˆ¤æ–­å…¶å†…å®¹æ˜¯å¦ä¸»è¦ä¸"æŠ¤è‚¤"ï¼ˆSkincareï¼‰ã€"ç¾å®¹"ï¼ˆBeautyï¼‰ã€"èº«ä½“ä¿å…»"ï¼ˆBody Careï¼‰æˆ–"ç¾å¦†äº§å“"ï¼ˆCosmeticsï¼‰ç›¸å…³ã€‚

åˆ¤æ–­æ ‡å‡†ï¼š
1. âœ… ç›¸å…³ (true): åŒ…å«çš®è‚¤é—®é¢˜ï¼ˆç—˜ç—˜/é»‘å¤´/ç¾ç™½/æŠ—è€ï¼‰ã€æŠ¤è‚¤æ­¥éª¤ã€æŠ¤è‚¤å“æµ‹è¯„ã€åŒ–å¦†æ•™ç¨‹ã€ç¾å®¹ä»ªå™¨ã€åŒ»ç¾ä½“éªŒç­‰ã€‚
2. âŒ ä¸ç›¸å…³ (false): æ¸¸æˆã€ç¾é£Ÿã€æç¬‘å‰§æƒ…ã€èŒå® ã€æ•°ç ã€æ±½è½¦ã€å•çº¯çš„èˆè¹ˆ/å˜è£…ï¼ˆæ— ç¾å¦†æ•™å­¦ï¼‰ã€ä¸€èˆ¬çš„æ—¥å¸¸ç”Ÿæ´»è®°å½•ï¼ˆæœªæåŠæŠ¤è‚¤å“ï¼‰ã€‚

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒKey ä¸ºè§†é¢‘ IDï¼ŒValue ä¸º å¸ƒå°”å€¼ (true/false)ã€‚

å¾…åˆ†ææ ‡é¢˜åˆ—è¡¨ï¼š
"""
    for item in titles_batch:
        prompt += f'- ID: "{item["id"]}", æ ‡é¢˜: "{item["title"]}"\n'
    
    prompt += "\nç»“æœ JSON:"

    payload = {
        "model": OPENROUTER_MODEL,
        "messages": [
            {"role": "system", "content": "You are a helpful JSON outputting assistant."},
            {"role": "user", "content": prompt}
        ],
        "response_format": {"type": "json_object"}
    }

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://goodgame.ai",
        "X-Title": "GoodGame Content Analysis"
    }

    try:
        async with session.post("https://openrouter.ai/api/v1/chat/completions", json=payload, headers=headers) as resp:
            if resp.status != 200:
                error_text = await resp.text()
                print(f"âš ï¸ API Error: {resp.status} - {error_text[:100]}")
                return {}
            
            result = await resp.json()
            content = result['choices'][0]['message']['content']
            
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
                
            return json.loads(content.strip())
    except Exception as e:
        print(f"âš ï¸ Exception: {e}")
        return {}

async def main():
    # 1. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 2. åŠ è½½ç¼“å­˜
    cached_results = load_cache()
    
    print("ğŸš€ å¼€å§‹è·å–æ•°æ®åº“è§†é¢‘æ•°æ®...")
    try:
        videos = []
        batch_size = 1000
        offset = 0
        while True:
            print(f"  æ­£åœ¨è·å–æ•°æ®: offset={offset}, limit={batch_size}...")
            response = supabase.table("gg_xingtu_kol_videos")\
                .select("id, kol_id, item_title, item_publish_time")\
                .neq("item_title", "")\
                .not_.is_("item_title", "null")\
                .range(offset, offset + batch_size - 1)\
                .execute()
            
            batch_data = response.data
            if not batch_data:
                print("  âš ï¸ æœªè·å–åˆ°æ›´å¤šæ•°æ®ï¼Œåœæ­¢åŠ è½½ã€‚")
                break
            
            videos.extend(batch_data)
            current_count = len(batch_data)
            print(f"  å·²åŠ è½½ {current_count} æ¡æ•°æ® (æ€»è®¡: {len(videos)})")
            
            if current_count < batch_size:
                print("  âš ï¸ æ•°æ®é‡å°äº batch_sizeï¼Œè§†ä¸ºæœ€åä¸€é¡µã€‚")
                break
                
            offset += batch_size
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        return

    print(f"âœ… æ€»å…±è·å–åˆ° {len(videos)} æ¡éç©ºæ ‡é¢˜è§†é¢‘")
    
    # 3. è¿‡æ»¤å‡ºéœ€è¦åˆ†æçš„è§†é¢‘ï¼ˆæœªç¼“å­˜çš„ï¼‰
    videos_to_analyze = [v for v in videos if str(v["id"]) not in cached_results]
    print(f"ğŸ“Š éœ€åˆ†ææ–°è§†é¢‘: {len(videos_to_analyze)} æ¡ (å·²ç¼“å­˜ {len(cached_results)} æ¡)")
    
    # 4. æ‰§è¡Œ LLM åˆ†æ
    if videos_to_analyze:
        BATCH_SIZE = 50
        batches = [videos_to_analyze[i:i + BATCH_SIZE] for i in range(0, len(videos_to_analyze), BATCH_SIZE)]
        
        print(f"ğŸ“¦ åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¤„ç†...")
        
        async with aiohttp.ClientSession() as session:
            sem = asyncio.Semaphore(10) # å¹¶å‘æ§åˆ¶

            async def process_batch(batch, idx):
                async with sem:
                    # å‡†å¤‡æ•°æ®
                    input_data = [{"id": str(v["id"]), "title": v["item_title"]} for v in batch]
                    
                    # è°ƒç”¨ API
                    print(f"  ğŸ”„ å¤„ç†æ‰¹æ¬¡ {idx+1}/{len(batches)}...")
                    batch_result = await analyze_titles_batch(session, input_data)
                    
                    if batch_result:
                        # ç«‹å³ä¿å­˜åˆ°ç¼“å­˜
                        append_to_cache(batch_result)
                        cached_results.update(batch_result)
                    else:
                        print(f"  âš ï¸ æ‰¹æ¬¡ {idx+1} å¤±è´¥")

            tasks = [process_batch(b, i) for i, b in enumerate(batches)]
            await asyncio.gather(*tasks)
    else:
        print("âœ¨ æ‰€æœ‰è§†é¢‘å‡å·²åˆ†æï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®ã€‚")

    # 5. ç»Ÿè®¡ KOL æ•°æ®
    print("ğŸ“ˆ æ­£åœ¨ç»Ÿè®¡ KOL æ•°æ®...")
    kol_stats = {} # kol_id -> stats
    
    # å…ˆåˆå§‹åŒ–æ‰€æœ‰æ¶‰åŠçš„ KOL
    for video in videos:
        kol_id = video["kol_id"]
        if kol_id not in kol_stats:
            kol_stats[kol_id] = {
                "kol_id": kol_id,
                "total_videos": 0,
                "skincare_videos": 0,
                "recent_titles": [],
                "last_publish_time": 0
            }
        
        stats = kol_stats[kol_id]
        stats["total_videos"] += 1
        
        # æ£€æŸ¥æ˜¯å¦æŠ¤è‚¤ç›¸å…³
        vid_str = str(video["id"])
        is_skincare = cached_results.get(vid_str, False)
        # å¤„ç†å¯èƒ½çš„å­—ç¬¦ä¸² "true"/"false"
        if isinstance(is_skincare, str):
            is_skincare = is_skincare.lower() == 'true'
            
        if is_skincare:
            stats["skincare_videos"] += 1
            if len(stats["recent_titles"]) < 5:
                stats["recent_titles"].append(video["item_title"])
        
        # æ›´æ–°æœ€æ–°å‘å¸ƒæ—¶é—´
        pub_time = video.get("item_publish_time")
        # ç®€å•å¤„ç†æ—¶é—´æˆ³æˆ–å­—ç¬¦ä¸²
        # è¿™é‡Œç•¥è¿‡å¤æ‚è½¬æ¢ï¼Œä»…ä½œå‚è€ƒ

    # 6. è·å– KOL åŸºç¡€ä¿¡æ¯ï¼ˆç²‰ä¸æ•°ã€æŠ¥ä»·ç­‰ï¼‰
    # ä¸ºäº†å…¨é‡ï¼Œæˆ‘ä»¬éœ€è¦æŠŠæ‰€æœ‰ kol_ids æ‹¿å»æŸ¥è¯¢ base_info å’Œ price
    all_kol_ids = list(kol_stats.keys())
    kol_details_map = {}
    
    print("ğŸ“¥ è·å– KOL åŸºç¡€ä¿¡æ¯...")
    # åˆ†æ‰¹æŸ¥è¯¢ Supabase (é¿å… URL è¿‡é•¿)
    CHUNK_SIZE = 100
    for i in range(0, len(all_kol_ids), CHUNK_SIZE):
        chunk_ids = all_kol_ids[i:i+CHUNK_SIZE]
        try:
            # æŸ¥è¯¢ base_info
            resp_base = supabase.table("gg_xingtu_kol_base_info")\
                .select("kol_id, kol_name, fans_count, ecom_enabled")\
                .in_("kol_id", chunk_ids)\
                .execute()
            
            # æŸ¥è¯¢ price
            resp_price = supabase.table("gg_xingtu_kol_price")\
                .select("kol_id, video_21_60s_price")\
                .in_("kol_id", chunk_ids)\
                .execute()
            
            # åˆå¹¶ä¿¡æ¯
            for item in resp_base.data:
                k_id = item["kol_id"]
                if k_id not in kol_details_map:
                    kol_details_map[k_id] = {}
                kol_details_map[k_id].update(item)
                
            for item in resp_price.data:
                k_id = item["kol_id"]
                if k_id not in kol_details_map:
                    kol_details_map[k_id] = {}
                kol_details_map[k_id]["video_21_60s_price"] = item.get("video_21_60s_price", 0)
                
        except Exception as e:
            print(f"âš ï¸ è·å–KOLä¿¡æ¯å¤±è´¥ (Chunk {i}): {e}")

    # 7. æ•´åˆæœ€ç»ˆåˆ—è¡¨
    final_list = []
    for kol_id, stats in kol_stats.items():
        details = kol_details_map.get(kol_id, {})
        
        ratio = 0
        if stats["total_videos"] > 0:
            ratio = round(stats["skincare_videos"] / stats["total_videos"] * 100, 2)
            
        entry = {
            "kol_id": kol_id,
            "kol_name": details.get("kol_name", "Unknown"),
            "fans_count": details.get("fans_count", 0),
            "is_ecom_enabled": details.get("ecom_enabled", False),
            "price_20_60s": details.get("video_21_60s_price", 0),
            "total_videos_analyzed": stats["total_videos"],
            "skincare_videos_count": stats["skincare_videos"],
            "skincare_ratio": f"{ratio}%",
            "skincare_ratio_num": ratio,
            "sample_titles": stats["recent_titles"]
        }
        final_list.append(entry)

    # 8. æ’åºå’Œä¿å­˜
    # æŒ‰æŠ¤è‚¤è§†é¢‘æ•°é‡å€’åºï¼Œç„¶åç²‰ä¸æ•°å€’åº
    final_list.sort(key=lambda x: (x["skincare_videos_count"], x["fans_count"]), reverse=True)
    
    # ä¿å­˜ JSON
    with open(FULL_KOL_LIST_JSON, "w", encoding="utf-8") as f:
        json.dump(final_list, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ å…¨é‡ JSON å·²ä¿å­˜: {FULL_KOL_LIST_JSON}")
    
    # ä¿å­˜ CSV
    with open(FULL_KOL_LIST_CSV, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        header = ["KOL ID", "æ˜µç§°", "ç²‰ä¸æ•°", "æŠ¤è‚¤ç›¸å…³åº¦", "æŠ¤è‚¤è§†é¢‘æ•°", "æ€»è§†é¢‘æ•°", "20-60sæŠ¥ä»·", "æ˜¯å¦ç”µå•†", "ç¤ºä¾‹æ ‡é¢˜"]
        writer.writerow(header)
        for item in final_list:
            writer.writerow([
                item["kol_id"],
                item["kol_name"],
                item["fans_count"],
                item["skincare_ratio"],
                item["skincare_videos_count"],
                item["total_videos_analyzed"],
                item["price_20_60s"],
                "æ˜¯" if item["is_ecom_enabled"] else "å¦",
                " | ".join(item["sample_titles"][:3])
            ])
    print(f"ğŸ’¾ å…¨é‡ CSV å·²ä¿å­˜: {FULL_KOL_LIST_CSV}")
    
    # 9. è¾“å‡ºæ‘˜è¦
    skincare_kols = [k for k in final_list if k["skincare_videos_count"] > 0]
    print("\nğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
    print(f"- è¦†ç›–è¾¾äººæ€»æ•°: {len(final_list)}")
    print(f"- å‘å¸ƒè¿‡æŠ¤è‚¤å†…å®¹çš„è¾¾äºº: {len(skincare_kols)}")
    print(f"- 100% å‚ç›´æŠ¤è‚¤è¾¾äºº: {len([k for k in skincare_kols if k['skincare_ratio_num'] == 100])}")

if __name__ == "__main__":
    asyncio.run(main())

