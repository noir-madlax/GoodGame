#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ±‡æ€»åˆ†æä¸¤ä¸ªå…³é”®è¯æœç´¢ç»“æœçš„äººç‰©ç”»åƒå’Œé‡åˆåº¦
1. æ±‡æ€» "æŠ¤è‚¤ä¿å…»" (13é¡µ) ä½œè€…ä¿¡æ¯
2. æ±‡æ€» "çš®è‚¤å¥½ ä¸“å®¶" (14é¡µ) ä½œè€…ä¿¡æ¯
3. è¾“å‡ºé‡åˆåˆ†ææŠ¥å‘Š
4. åˆ†æä½œè€…ç”»åƒ (åŸºäºæ˜µç§°ã€IDç­‰åŸºç¡€ä¿¡æ¯ï¼Œå¦‚æœæœ‰æ›´å¤šæ•°æ®å¯æ‰©å±•)

æ³¨æ„ï¼šæ•°æ®åº“é‡åˆæŸ¥è¯¢å·²é€šè¿‡ MCP å®Œæˆï¼Œç»“æœä¸º 0 (gg_xingtu_kol_base_info)ã€‚
è„šæœ¬å°†ç”Ÿæˆæœ€ç»ˆçš„ JSON æŠ¥å‘Šã€‚

ä½œè€…: AI Agent
åˆ›å»ºæ—¶é—´: 2025-11-24
"""

import json
from pathlib import Path
from collections import Counter

def extract_author_personas(directory):
    """ä»æŒ‡å®šç›®å½•çš„JSONæ–‡ä»¶ä¸­æå–ä½œè€…ç”»åƒä¿¡æ¯"""
    authors = {} # uid -> info dict
    
    files = list(Path(directory).glob("video_search_page_*.json"))
    print(f"ğŸ“‚ æ­£åœ¨å¤„ç†ç›®å½•: {directory.name} (å…± {len(files)} ä¸ªæ–‡ä»¶)")
    
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            api_data = data.get('data', [])
            items = []
            
            if isinstance(api_data, list):
                items = api_data
            elif isinstance(api_data, dict):
                items = api_data.get('data') or api_data.get('aweme_list') or []
            
            for item in items:
                author = None
                if 'author' in item:
                    author = item['author']
                elif 'aweme_info' in item and 'author' in item['aweme_info']:
                    author = item['aweme_info']['author']
                
                if author:
                    uid = str(author.get('uid', ''))
                    if not uid: continue
                    
                    if uid not in authors:
                        authors[uid] = {
                            "uid": uid,
                            "nickname": author.get('nickname', 'Unknown'),
                            "short_id": author.get('short_id'),
                            "unique_id": author.get('unique_id'),
                            "follower_count": author.get('follower_count'), # æ³¨æ„ API æ˜¯å¦è¿”å›æ­¤å­—æ®µ
                            "signature": author.get('signature'),
                            "verify_info": author.get('custom_verify', ''),
                            "enterprise_verify": author.get('enterprise_verify_reason', ''),
                            "avatar": author.get('avatar_thumb', {}).get('url_list', [''])[0]
                        }
                        
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}")
            
    print(f"   âœ… æå–åˆ° {len(authors)} ä¸ªå”¯ä¸€ä½œè€…")
    return authors

def analyze_personas(authors):
    """ç®€å•åˆ†æä½œè€…ç”»åƒå…³é”®è¯"""
    keywords = []
    verify_types = []
    
    for uid, info in authors.items():
        nickname = info.get('nickname', '')
        signature = info.get('signature', '')
        verify = info.get('verify_info', '') or info.get('enterprise_verify', '')
        
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        text = f"{nickname} {signature} {verify}"
        
        if "åŒ»ç”Ÿ" in text or "åŒ»å¸ˆ" in text or "åšå£«" in text or "ä¸»ä»»" in text:
            keywords.append("ä¸“ä¸š/åŒ»ç”Ÿ")
        elif "æŠ¤è‚¤" in text or "ç¾å¦†" in text:
            keywords.append("ç¾å¦†/æŠ¤è‚¤åšä¸»")
        elif "æµ‹è¯„" in text:
            keywords.append("æµ‹è¯„åšä¸»")
        elif "å“ç‰Œ" in text or "æ——èˆ°åº—" in text or "å®˜æ–¹" in text:
            keywords.append("å“ç‰Œ/æœºæ„")
        else:
            keywords.append("æ™®é€š/å…¶ä»–")
            
        if verify:
            verify_types.append("å·²è®¤è¯")
        else:
            verify_types.append("æœªè®¤è¯")
            
    return {
        "category_distribution": dict(Counter(keywords)),
        "verify_distribution": dict(Counter(verify_types))
    }

def main():
    script_dir = Path(__file__).parent
    base_output_dir = script_dir.parent / 'output'
    
    dir_hufu = base_output_dir / 'keyword_æŠ¤è‚¤ä¿å…»' / 'detail'
    dir_skin = base_output_dir / 'keyword_çš®è‚¤å¥½_ä¸“å®¶' / 'detail'
    
    # 1. æå–ç”»åƒ
    authors_hufu = extract_author_personas(dir_hufu)
    authors_skin = extract_author_personas(dir_skin)
    
    # 2. åˆ†æç”»åƒ
    persona_hufu = analyze_personas(authors_hufu)
    persona_skin = analyze_personas(authors_skin)
    
    # 3. è®¡ç®—é‡åˆ
    ids_hufu = set(authors_hufu.keys())
    ids_skin = set(authors_skin.keys())
    overlap_ids = ids_skin.intersection(ids_hufu)
    
    overlap_details = []
    for uid in overlap_ids:
        overlap_details.append(authors_skin[uid])
        
    # 4. ç”ŸæˆæŠ¥å‘Š
    report = {
        "summary": {
            "keyword_1": "æŠ¤è‚¤ä¿å…»",
            "count_1": len(ids_hufu),
            "keyword_2": "çš®è‚¤å¥½ ä¸“å®¶",
            "count_2": len(ids_skin),
            "overlap_count": len(overlap_ids),
            "overlap_rate_percent": len(overlap_ids) / len(ids_skin) * 100 if ids_skin else 0
        },
        "database_check": {
            "table": "gg_xingtu_kol_base_info",
            "match_count": 0,
            "note": "é€šè¿‡ MCP æŸ¥è¯¢ï¼Œæœªå‘ç°é‡åˆä½œè€…"
        },
        "personas": {
            "æŠ¤è‚¤ä¿å…»": persona_hufu,
            "çš®è‚¤å¥½ ä¸“å®¶": persona_skin
        },
        "overlap_authors_detail": overlap_details
    }
    
    output_file = base_output_dir / 'final_comparison_report.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
        
    print(f"\nâœ… æœ€ç»ˆå¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file.name}")
    print(json.dumps(report['summary'], ensure_ascii=False, indent=2))
    print("\nç”»åƒåˆ†å¸ƒ (æŠ¤è‚¤ä¿å…»):")
    print(json.dumps(persona_hufu, ensure_ascii=False, indent=2))
    print("\nç”»åƒåˆ†å¸ƒ (çš®è‚¤å¥½ ä¸“å®¶):")
    print(json.dumps(persona_skin, ensure_ascii=False, indent=2))

if __name__ == '__main__':
    main()

