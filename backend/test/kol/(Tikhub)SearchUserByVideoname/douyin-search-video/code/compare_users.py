#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”ä¸¤ä¸ªå…³é”®è¯æœç´¢ç»“æœçš„äººç‰©é‡åˆåº¦ï¼Œå¹¶ä¸æ•°æ®åº“è¿›è¡Œå¯¹æ¯”
1. è·å– "æŠ¤è‚¤ä¿å…»" (13é¡µæ•°æ®) çš„æ‰€æœ‰ä½œè€…ID
2. è·å– "çš®è‚¤å¥½ ä¸“å®¶" (3é¡µæ•°æ®) çš„æ‰€æœ‰ä½œè€…ID
3. è®¡ç®—é‡åˆåº¦
4. è¿æ¥æ•°æ®åº“ï¼Œå¯¹æ¯” gg_xingtu_kol_base_info è¡¨ä¸­çš„ä½œè€…ID

ä½œè€…: AI Agent
åˆ›å»ºæ—¶é—´: 2025-11-24
"""

import json
import os
from pathlib import Path
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor

def load_env():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    # å‘ä¸ŠæŸ¥æ‰¾ .env
    backend_dir = Path(__file__).resolve().parents[5]
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
    else:
        cwd_env = Path.cwd() / '.env'
        if cwd_env.exists():
            load_dotenv(cwd_env)

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    load_env()
    
    # ä» Supabase URL è§£æè¿æ¥ä¿¡æ¯
    # SUPABASE_URL=postgres://user:password@host:port/dbname
    db_url = os.getenv("SUPABASE_URL")
    
    if not db_url:
        print("âš ï¸ æœªæ‰¾åˆ° SUPABASE_URL ç¯å¢ƒå˜é‡ï¼Œæ— æ³•è¿æ¥æ•°æ®åº“")
        return None
        
    try:
        conn = psycopg2.connect(db_url)
        return conn
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return None

def extract_author_ids(directory):
    """ä»æŒ‡å®šç›®å½•çš„JSONæ–‡ä»¶ä¸­æå–ä½œè€…ID"""
    author_ids = set()
    author_info = {} # id -> nickname
    
    files = list(Path(directory).glob("video_search_page_*.json"))
    print(f"ğŸ“‚ æ­£åœ¨å¤„ç†ç›®å½•: {directory.name} (å…± {len(files)} ä¸ªæ–‡ä»¶)")
    
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # æ ¹æ® API è¿”å›ç»“æ„æå–ä½œè€…ä¿¡æ¯
            # ç»“æ„é€šå¸¸æ˜¯ data.data[] æˆ–è€… data.aweme_list[]
            # æ¯ä¸ª item åŒ…å« author å­—æ®µ
            
            # æ£€æŸ¥ data ç»“æ„
            api_data = data.get('data', [])
            items = []
            
            if isinstance(api_data, list):
                items = api_data
            elif isinstance(api_data, dict):
                # å°è¯•å¸¸è§çš„åˆ—è¡¨å­—æ®µå
                items = api_data.get('data') or api_data.get('aweme_list') or []
            
            for item in items:
                # æœ‰äº›ç»“æ„å¯èƒ½æ˜¯ item['aweme_info']['author'] æˆ–è€…ç›´æ¥ item['author']
                # è§†é¢‘æœç´¢è¿”å›çš„ç»“æ„é€šå¸¸æ¯”è¾ƒå¤æ‚ï¼ŒåŒ…å«ä¸åŒç±»å‹çš„å¡ç‰‡
                
                author = None
                
                # æƒ…å†µ1: ç›´æ¥åœ¨ item ä¸­
                if 'author' in item:
                    author = item['author']
                # æƒ…å†µ2: åœ¨ aweme_info ä¸­
                elif 'aweme_info' in item and 'author' in item['aweme_info']:
                    author = item['aweme_info']['author']
                
                if author:
                    uid = author.get('uid')
                    sec_uid = author.get('sec_uid') # sec_uid å¾€å¾€æ›´ç¨³å®šï¼Œä½†è¿™é‡Œå…ˆç”¨ uid
                    nickname = author.get('nickname', 'Unknown')
                    
                    # æ³¨æ„: æœ‰æ—¶å€™ uid æ˜¯å­—ç¬¦ä¸²ï¼Œæœ‰æ—¶å€™æ˜¯æ•°å­—ï¼Œç»Ÿä¸€è½¬å­—ç¬¦ä¸²
                    if uid:
                        uid_str = str(uid)
                        author_ids.add(uid_str)
                        author_info[uid_str] = nickname
                        
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}")
            
    print(f"   âœ… æå–åˆ° {len(author_ids)} ä¸ªå”¯ä¸€ä½œè€…ID")
    return author_ids, author_info

def check_db_overlap(author_ids):
    """æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¿™äº›ä½œè€…"""
    existing_ids = set()
    
    # ç”±äº author_ids å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—ï¼Œè¿™é‡Œéœ€è¦å¤„ç†ä¸€ä¸‹
    # ä» list_tables çŸ¥é“ï¼Œgg_platform_author å®é™…ä¸Šæ˜¯ gg_authors è¡¨?
    # ç”¨æˆ·ä¹‹å‰æåˆ° gg_platform_authorï¼Œä½† list_tables æ˜¾ç¤ºæœ‰ gg_authors å’Œ gg_xingtu_kol_base_info
    # gg_authors è¡¨æœ‰ platform_author_id å­—æ®µ
    # gg_xingtu_kol_base_info è¡¨æœ‰ kol_id å­—æ®µ
    
    # æˆ‘ä»¬ä½¿ç”¨ MCP execute_sql å·¥å…·æ¥æŸ¥è¯¢ï¼Œä½†å½“å‰è„šæœ¬æ˜¯åœ¨æœ¬åœ° Python ç¯å¢ƒè¿è¡Œ
    # æ— æ³•ç›´æ¥è°ƒç”¨ MCP å·¥å…·ã€‚
    # ç”¨æˆ·è¦æ±‚ "è¿™æ¬¡å¯ä»¥æˆåŠŸçš„ç”¨mcpç»“åˆæ•°æ®åº“çš„kolä¹Ÿå¯¹æ¯”å®Œæˆ"
    # æ„å‘³ç€æˆ‘ä»¬éœ€è¦åœ¨ chat ä¸­è°ƒç”¨ MCP å·¥å…·è·å–æ•°æ®ï¼Œæˆ–è€…é…ç½® Python è„šæœ¬è¿æ¥æ•°æ®åº“ã€‚
    # ä½†ä¹‹å‰çš„ Python è„šæœ¬è¿æ¥å¤±è´¥ã€‚
    
    # æ—¢ç„¶ç”¨æˆ·åœ¨ chat ä¸­ï¼Œæˆ‘ä½œä¸º Assistant å¯ä»¥è°ƒç”¨ MCPã€‚
    # ä½†æ˜¯è„šæœ¬æœ¬èº«æ— æ³•è°ƒç”¨ MCPã€‚
    # æ‰€ä»¥ç­–ç•¥æ˜¯ï¼š
    # 1. è„šæœ¬åªè´Ÿè´£æ–‡ä»¶å±‚é¢çš„å¯¹æ¯”ï¼Œå¹¶è¾“å‡ºæ‰€æœ‰æ–°ä½œè€… ID åˆ—è¡¨åˆ°ä¸€ä¸ªæ–‡ä»¶ã€‚
    # 2. Assistant è¯»å–è¯¥æ–‡ä»¶ï¼Œç„¶åä½¿ç”¨ MCP execute_sql æŸ¥è¯¢æ•°æ®åº“ã€‚
    # 3. Assistant ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚
    
    # æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬åªè¿”å›ç©ºï¼Œæˆ–è€…ä¿®æ”¹é€»è¾‘è®©ä¸»æµç¨‹çŸ¥é“éœ€è¦å¤–éƒ¨ååŠ©ã€‚
    print("âš ï¸ è„šæœ¬æ— æ³•ç›´æ¥è¿æ¥æ•°æ®åº“ (DSNé”™è¯¯)ã€‚å°†å¯¼å‡ºIDåˆ—è¡¨ä¾› MCP æŸ¥è¯¢ã€‚")
    return existing_ids

def main():
    script_dir = Path(__file__).parent
    base_output_dir = script_dir.parent / 'output'
    
    # 1. å®šä¹‰ç›®å½•
    dir_hufu = base_output_dir / 'keyword_æŠ¤è‚¤ä¿å…»' / 'detail'
    dir_new = base_output_dir / 'keyword_çš®è‚¤å¥½_ä¸“å®¶' / 'detail'
    
    # 2. æå– "æŠ¤è‚¤ä¿å…»" çš„ä½œè€…
    print(f"--- åˆ†æ 'æŠ¤è‚¤ä¿å…»' æ•°æ® ---")
    ids_hufu, info_hufu = extract_author_ids(dir_hufu)
    
    # 3. æå– "çš®è‚¤å¥½ ä¸“å®¶" çš„ä½œè€…
    print(f"\n--- åˆ†æ 'çš®è‚¤å¥½ ä¸“å®¶' æ•°æ® ---")
    ids_new, info_new = extract_author_ids(dir_new)
    
    # 4. è®¡ç®—é‡åˆåº¦
    overlap = ids_new.intersection(ids_hufu)
    overlap_rate = len(overlap) / len(ids_new) * 100 if ids_new else 0
    
    print(f"\n--- å¯¹æ¯”ç»“æœ ---")
    print(f"æŠ¤è‚¤ä¿å…» (åŸºå‡†): {len(ids_hufu)} äºº")
    print(f"çš®è‚¤å¥½ ä¸“å®¶ (æ–°): {len(ids_new)} äºº")
    print(f"é‡åˆäººæ•°: {len(overlap)} äºº")
    print(f"é‡åˆç‡ (ç›¸å¯¹äºæ–°æ•°æ®): {overlap_rate:.2f}%")
    
    if overlap:
        print("\né‡åˆä½œè€…ç¤ºä¾‹:")
        for uid in list(overlap)[:5]:
            print(f" - {info_new.get(uid)} (ID: {uid})")
            
    # 5. å¯¼å‡ºå¾…æŸ¥è¯¢ ID åˆ—è¡¨ (ä¸ºäº† MCP)
    ids_to_check = list(ids_new)
    id_list_file = dir_new / 'author_ids_to_check.json'
    with open(id_list_file, 'w', encoding='utf-8') as f:
        json.dump(ids_to_check, f)
    print(f"\nğŸ“‹ å·²å¯¼å‡º {len(ids_to_check)} ä¸ªä½œè€…ID åˆ° {id_list_file.name}ï¼Œè¯·ä½¿ç”¨ MCP æŸ¥è¯¢æ•°æ®åº“ã€‚")

    # 6. ä¿å­˜åˆæ­¥ç»“æœ (ä¸å«æ•°æ®åº“å¯¹æ¯”)
    result = {
        "keyword_base": "æŠ¤è‚¤ä¿å…»",
        "keyword_new": "çš®è‚¤å¥½ ä¸“å®¶",
        "base_count": len(ids_hufu),
        "new_count": len(ids_new),
        "overlap_count": len(overlap),
        "overlap_rate_percent": overlap_rate,
        "overlap_authors": [
            {"id": uid, "nickname": info_new.get(uid)} for uid in overlap
        ],
        "new_unique_authors": [
            {"id": uid, "nickname": info_new.get(uid)} 
            for uid in ids_new - ids_hufu
        ]
    }
    
    output_file = dir_new / 'overlap_analysis_local.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
        
    print(f"\nâœ… åˆæ­¥åˆ†æç»“æœå·²ä¿å­˜è‡³: {output_file.name}")

if __name__ == '__main__':
    main()

