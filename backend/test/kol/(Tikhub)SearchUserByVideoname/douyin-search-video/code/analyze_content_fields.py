#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åˆ†æè§†é¢‘å†…å®¹å­—æ®µçš„èšç±»ç»Ÿè®¡
æå–å¹¶åˆ†ædescã€hashtagsã€suggest_wordsã€video_tagsã€captionç­‰å­—æ®µ

ä½œè€…: AI Agent
åˆ›å»ºæ—¶é—´: 2025-11-24
"""

import json
import os
from pathlib import Path
from collections import Counter, defaultdict
import re

def extract_content_fields(file_paths):
    """ä»JSONæ–‡ä»¶ä¸­æå–å„ç§å†…å®¹å­—æ®µ"""
    content_data = {
        'descs': [],  # è§†é¢‘æè¿°
        'hashtags': [],  # è¯é¢˜æ ‡ç­¾
        'suggest_words': [],  # æ¨èæœç´¢è¯
        'video_tags': [],  # è§†é¢‘æ ‡ç­¾
        'captions': [],  # å­—å¹•
        'all_text': []  # æ‰€æœ‰æ–‡æœ¬å†…å®¹
    }

    for file_path in file_paths:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if 'data' in data and 'data' in data['data']:
                videos = data['data']['data']

                for video in videos:
                    if 'aweme_info' in video:
                        aweme = video['aweme_info']

                        # 1. æå–desc
                        desc = aweme.get('desc', '')
                        if desc:
                            content_data['descs'].append(desc)

                        # 2. æå–hashtags (text_extraä¸­çš„hashtag_name)
                        text_extra = aweme.get('text_extra', [])
                        if text_extra:
                            for extra in text_extra:
                                if isinstance(extra, dict) and 'hashtag_name' in extra and extra['hashtag_name']:
                                    content_data['hashtags'].append(extra['hashtag_name'])

                        # 3. æå–suggest_words (ä»videoæ•°æ®ä¸­æå–)
                        if 'suggest_words' in video and video['suggest_words']:
                            for suggest in video['suggest_words']:
                                if isinstance(suggest, dict):
                                    word = suggest.get('word', '')
                                    if word:
                                        content_data['suggest_words'].append(word)

                        # 4. æå–video_tags
                        video_tags = aweme.get('video_tag', [])
                        if video_tags:
                            for tag_info in video_tags:
                                if isinstance(tag_info, dict) and 'tag_name' in tag_info:
                                    content_data['video_tags'].append(tag_info['tag_name'])

                        # 5. æå–caption (å­—å¹•)
                        # æ£€æŸ¥å„ç§å¯èƒ½çš„å­—å¹•å­—æ®µ
                        caption = ''
                        if 'video' in aweme and 'caption' in aweme['video']:
                            caption = aweme['video']['caption']
                        elif 'caption' in aweme:
                            caption = aweme['caption']

                        if caption:
                            content_data['captions'].append(caption)

                        # 6. æ”¶é›†æ‰€æœ‰æ–‡æœ¬
                        all_text = desc + ' ' + caption
                        if all_text.strip():
                            content_data['all_text'].append(all_text.strip())

        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

    return content_data

def analyze_text_patterns(texts, field_name):
    """åˆ†ææ–‡æœ¬æ¨¡å¼å’Œå…³é”®è¯"""
    if not texts:
        return {}

    analysis = {
        'total_count': len(texts),
        'avg_length': sum(len(text) for text in texts) / len(texts),
        'keyword_freq': Counter(),
        'pattern_stats': {}
    }

    # æå–å…³é”®è¯
    all_words = []
    for text in texts:
        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
        words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', text)
        all_words.extend(words)

    analysis['keyword_freq'] = Counter(all_words).most_common(20)

    # ç‰¹æ®Šæ¨¡å¼åˆ†æ
    if field_name == 'descs':
        # åˆ†æè¯é¢˜æ ‡ç­¾ä½¿ç”¨
        hashtag_pattern = re.compile(r'#([^#\s]+)')
        hashtags_in_desc = []
        for text in texts:
            hashtags_in_desc.extend(hashtag_pattern.findall(text))
        analysis['hashtags_in_desc'] = Counter(hashtags_in_desc).most_common(15)

        # åˆ†æè¡¨æƒ…ç¬¦å·ä½¿ç”¨
        emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]')
        emoji_count = sum(len(emoji_pattern.findall(text)) for text in texts)
        analysis['emoji_usage'] = emoji_count

    return analysis

def cluster_analysis(content_data):
    """è¿›è¡Œèšç±»åˆ†æ"""
    clusters = {
        'content_types': defaultdict(int),  # å†…å®¹ç±»å‹èšç±»
        'topic_clusters': defaultdict(int),  # è¯é¢˜èšç±»
        'style_clusters': defaultdict(int),  # é£æ ¼èšç±»
        'target_audience': defaultdict(int)  # ç›®æ ‡å—ä¼—èšç±»
    }

    # åŸºäºdescè¿›è¡Œå†…å®¹ç±»å‹èšç±»
    for desc in content_data['descs']:
        desc_lower = desc.lower()

        # å†…å®¹ç±»å‹è¯†åˆ«
        if any(word in desc_lower for word in ['æ•™ç¨‹', 'æ•™å­¦', 'æ­¥éª¤', 'æ‰‹æ³•', 'æ–¹æ³•']):
            clusters['content_types']['æ•™ç¨‹æ•™å­¦'] += 1
        elif any(word in desc_lower for word in ['åˆ†äº«', 'ç»éªŒ', 'å¿ƒå¾—', 'æ—¥å¸¸']):
            clusters['content_types']['ç»éªŒåˆ†äº«'] += 1
        elif any(word in desc_lower for word in ['æ¨è', 'å¥½ç”¨', 'å¿…å¤‡', 'å¿…è¯•']):
            clusters['content_types']['äº§å“æ¨è'] += 1
        elif any(word in desc_lower for word in ['é—®é¢˜', 'å›°æ‰°', 'è§£å†³', 'æ€ä¹ˆåŠ']):
            clusters['content_types']['é—®é¢˜è§£å†³'] += 1
        else:
            clusters['content_types']['å…¶ä»–'] += 1

        # ç›®æ ‡å—ä¼—è¯†åˆ«
        if any(word in desc_lower for word in ['æ–°æ‰‹', 'å…¥é—¨', 'åˆå­¦è€…', 'å°ç™½']):
            clusters['target_audience']['æ–°æ‰‹å…¥é—¨'] += 1
        elif any(word in desc_lower for word in ['é«˜çº§', 'ä¸“ä¸š', 'åŒ»å¸ˆ', 'åŒ»ç”Ÿ']):
            clusters['target_audience']['ä¸“ä¸šäººå£«'] += 1
        elif any(word in desc_lower for word in ['å¦ˆå¦ˆ', 'å®å¦ˆ', 'å­•å¦‡', 'å„¿ç«¥']):
            clusters['target_audience']['å®¶åº­ç”¨æˆ·'] += 1
        else:
            clusters['target_audience']['å¤§ä¼—ç”¨æˆ·'] += 1

    return clusters

def main():
    # æ–‡ä»¶è·¯å¾„
    current_dir = Path(__file__).parent
    output_dir = current_dir.parent / "output" / "keyword_æŠ¤è‚¤ä¿å…»" / "detail"

    file_paths = [
        output_dir / f"video_search_page_{i}_20251124_{'135619' if i >= 3 else '134103'}.json"
        for i in range(14)
    ]

    print("ğŸ“Š å¼€å§‹åˆ†æ13é¡µè§†é¢‘æ•°æ®çš„å†…å®¹å­—æ®µ...")
    print("=" * 60)

    # æå–å†…å®¹å­—æ®µ
    content_data = extract_content_fields(file_paths)

    print(f"âœ… æ•°æ®æå–å®Œæˆ:")
    print(f"   - è§†é¢‘æè¿°: {len(content_data['descs'])} æ¡")
    print(f"   - è¯é¢˜æ ‡ç­¾: {len(content_data['hashtags'])} ä¸ª")
    print(f"   - æ¨èè¯: {len(content_data['suggest_words'])} ä¸ª")
    print(f"   - è§†é¢‘æ ‡ç­¾: {len(content_data['video_tags'])} ä¸ª")
    print(f"   - å­—å¹•: {len(content_data['captions'])} æ¡")

    # åˆ†æå„ä¸ªå­—æ®µ
    print("\nğŸ“ˆ å­—æ®µè¯¦ç»†åˆ†æ:")
    print("=" * 60)

    # 1. descåˆ†æ
    desc_analysis = analyze_text_patterns(content_data['descs'], 'descs')
    print(f"\nğŸ¬ è§†é¢‘æè¿°(desc)åˆ†æ:")
    print(f"   æ€»æ•°é‡: {desc_analysis['total_count']}")
    print(f"   å¹³å‡é•¿åº¦: {desc_analysis['avg_length']:.1f} å­—ç¬¦")
    print(f"   è¡¨æƒ…ç¬¦å·ä½¿ç”¨: {desc_analysis.get('emoji_usage', 0)} æ¬¡")

    print(f"   çƒ­é—¨è¯é¢˜æ ‡ç­¾:")
    for tag, count in desc_analysis.get('hashtags_in_desc', [])[:10]:
        print(f"      #{tag}: {count} æ¬¡")

    # 2. hashtagsåˆ†æ
    hashtag_analysis = analyze_text_patterns(content_data['hashtags'], 'hashtags')
    print(f"\nğŸ·ï¸ è¯é¢˜æ ‡ç­¾(hashtag_name)åˆ†æ:")
    print(f"   æ€»æ•°é‡: {hashtag_analysis['total_count']}")
    print(f"   ç‹¬ç‰¹æ ‡ç­¾æ•°: {len(hashtag_analysis['keyword_freq'])}")
    print(f"   çƒ­é—¨è¯é¢˜:")
    for tag, count in hashtag_analysis['keyword_freq'][:15]:
        print(f"      #{tag}: {count} æ¬¡")

    # 3. suggest_wordsåˆ†æ
    if content_data['suggest_words']:
        suggest_analysis = analyze_text_patterns(content_data['suggest_words'], 'suggest_words')
        print(f"\nğŸ” æ¨èæœç´¢è¯(suggest_words)åˆ†æ:")
        print(f"   æ€»æ•°é‡: {suggest_analysis['total_count']}")
        print(f"   ç‹¬ç‰¹è¯æ•°: {len(suggest_analysis['keyword_freq'])}")
        print(f"   çƒ­é—¨æ¨èè¯:")
        for word, count in suggest_analysis['keyword_freq'][:10]:
            print(f"      {word}: {count} æ¬¡")
    else:
        print(f"\nğŸ” æ¨èæœç´¢è¯(suggest_words)åˆ†æ:")
        print("   æ— æ•°æ®")

    # 4. video_tagsåˆ†æ
    if content_data['video_tags']:
        tag_analysis = analyze_text_patterns(content_data['video_tags'], 'video_tags')
        print(f"\nğŸ·ï¸ è§†é¢‘æ ‡ç­¾(video_tag)åˆ†æ:")
        print(f"   æ€»æ•°é‡: {tag_analysis['total_count']}")
        print(f"   ç‹¬ç‰¹æ ‡ç­¾æ•°: {len(tag_analysis['keyword_freq'])}")
        print(f"   çƒ­é—¨è§†é¢‘æ ‡ç­¾:")
        for tag, count in tag_analysis['keyword_freq'][:10]:
            print(f"      {tag}: {count} æ¬¡")
    else:
        print(f"\nğŸ·ï¸ è§†é¢‘æ ‡ç­¾(video_tag)åˆ†æ:")
        print("   æ— æ•°æ®")

    # 5. captionåˆ†æ
    if content_data['captions']:
        caption_analysis = analyze_text_patterns(content_data['captions'], 'captions')
        print(f"\nğŸ“ å­—å¹•(caption)åˆ†æ:")
        print(f"   æ€»æ•°é‡: {caption_analysis['total_count']}")
        print(f"   å¹³å‡é•¿åº¦: {caption_analysis['avg_length']:.1f} å­—ç¬¦")
    else:
        print(f"\nğŸ“ å­—å¹•(caption)åˆ†æ:")
        print("   æ— æ•°æ®")
    # èšç±»åˆ†æ
    clusters = cluster_analysis(content_data)

    print(f"\nğŸ¯ å†…å®¹èšç±»åˆ†æ:")
    print("=" * 60)

    print(f"\nğŸ“Š å†…å®¹ç±»å‹åˆ†å¸ƒ:")
    total_content = sum(clusters['content_types'].values())
    for content_type, count in sorted(clusters['content_types'].items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_content * 100
        print(f"      {content_type}: {count} ä¸ª ({percentage:.1f}%)")

    print(f"\nğŸ‘¥ ç›®æ ‡å—ä¼—åˆ†å¸ƒ:")
    total_audience = sum(clusters['target_audience'].values())
    for audience, count in sorted(clusters['target_audience'].items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_audience * 100
        print(f"      {audience}: {count} ä¸ª ({percentage:.1f}%)")

    print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
    print("=" * 60)
    print(f"1. å†…å®¹é«˜åº¦ä¸“ä¸šåŒ–: {len(content_data['hashtags'])}ä¸ªè¯é¢˜æ ‡ç­¾ï¼Œè¦†ç›–æŠ¤è‚¤å„ä¸ªç»†åˆ†é¢†åŸŸ")
    print(f"2. ç”¨æˆ·éœ€æ±‚å¤šæ ·: ä»å…¥é—¨æ•™ç¨‹åˆ°ä¸“ä¸šçŸ¥è¯†ï¼Œæ»¡è¶³ä¸åŒå±‚æ¬¡ç”¨æˆ·")
    print(f"3. äº’åŠ¨æ€§å¼º: è¯é¢˜æ ‡ç­¾ä½¿ç”¨é¢‘ç¹({desc_analysis.get('hashtags_in_desc', []) and len(desc_analysis['hashtags_in_desc']) or 0}ç§)ï¼Œå¢å¼ºå†…å®¹ä¼ æ’­")
    print(f"4. æ¨èç³»ç»Ÿå®Œå–„: {len(content_data['suggest_words'])}ä¸ªæ¨èè¯ï¼Œè¦†ç›–ç›¸å…³æœç´¢éœ€æ±‚")
    print(f"5. è§†é¢‘æ ‡ç­¾ä¸°å¯Œ: {len(content_data['video_tags'])}ä¸ªè§†é¢‘æ ‡ç­¾ï¼Œæå‡å†…å®¹åˆ†ç±»å‡†ç¡®æ€§")

if __name__ == "__main__":
    main()
