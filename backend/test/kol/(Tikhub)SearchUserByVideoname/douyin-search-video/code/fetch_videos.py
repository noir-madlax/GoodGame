#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æœç´¢æŠ–éŸ³è§†é¢‘ - å…³é”®è¯ "æŠ¤è‚¤ä¿å…»"
æ¥å£: /api/v1/douyin/search/fetch_video_search_v4

åŠŸèƒ½ï¼š
1. æœç´¢æŒ‡å®šå…³é”®è¯çš„è§†é¢‘
2. è·å–å‰3é¡µæ•°æ®
3. è‡ªåŠ¨å¤„ç†ç¿»é¡µå‚æ•° (offset, page, backtrace, search_id)
4. ä¿å­˜åŸå§‹JSONæ•°æ®

ä½œè€…: AI Agent
åˆ›å»ºæ—¶é—´: 2025-11-24
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
import threading

class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨ï¼šæ¯ç§’æœ€å¤šNä¸ªè¯·æ±‚"""
    def __init__(self, max_per_second=5):
        self.max_per_second = max_per_second
        self.min_interval = 1.0 / max_per_second
        self.last_request_time = 0
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        with self.lock:
            current_time = time.time()
            time_since_last = current_time - self.last_request_time
            
            if time_since_last < self.min_interval:
                sleep_time = self.min_interval - time_since_last
                time.sleep(sleep_time)
            
            self.last_request_time = time.time()

def load_api_key():
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key"""
    # æ ¹æ®è„šæœ¬ä½ç½®å‘ä¸ŠæŸ¥æ‰¾ .env
    # å½“å‰: backend/test/kol/video/douyin-search-video/code/fetch_videos.py
    # .env: backend/.env
    # parents[0]=code, [1]=douyin-search-video, [2]=video, [3]=kol, [4]=test, [5]=backend
    backend_dir = Path(__file__).resolve().parents[5]
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        # å°è¯•ä»å½“å‰å·¥ä½œç›®å½•æŸ¥æ‰¾
        cwd_env = Path.cwd() / '.env'
        if cwd_env.exists():
             load_dotenv(cwd_env)
             print(f"âœ… ä» {cwd_env} åŠ è½½ç¯å¢ƒå˜é‡")
        else:
             print(f"âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®")
    
    return api_key

def fetch_video_search_page(keyword, page, offset, backtrace, search_id, api_key, rate_limiter):
    """
    è·å–å•é¡µè§†é¢‘æœç´¢ç»“æœ
    """
    # ä½¿ç”¨å¤§é™†å¯è®¿é—®åŸŸå
    url_list = [
        "https://api.tikhub.dev/api/v1/douyin/search/fetch_video_search_v4",
        "https://api.tikhub.io/api/v1/douyin/search/fetch_video_search_v4"
    ]
    
    payload = {
        "keyword": keyword,
        "offset": offset,
        "page": page,
        "backtrace": backtrace,
        "search_id": search_id
    }
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    rate_limiter.wait_if_needed()
    
    print(f"â¡ï¸ è¯·æ±‚ç¬¬ {page} é¡µ (offset={offset}, backtrace={backtrace[:10]}..., search_id={search_id[:10]}...)")
    print(f"   Payload: {json.dumps(payload, ensure_ascii=False)}")
    
    for url in url_list:
        try:
            print(f"   å°è¯• URL: {url}")
            response = requests.post(url, json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"   âŒ URL {url} è¯·æ±‚å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"   å“åº”å†…å®¹: {e.response.text}")
            # Continue to next URL
            continue
            
    return None

def get_latest_state(output_dir):
    """
    ä»è¾“å‡ºç›®å½•ä¸­è·å–æœ€æ–°çš„é¡µé¢çŠ¶æ€
    è¿”å›: (next_page_index, next_offset, next_backtrace, next_search_id)
    """
    if not output_dir.exists():
        return 0, 0, "", ""

    files = list(output_dir.glob("video_search_page_*.json"))
    if not files:
        return 0, 0, "", ""
        
    # æå–é¡µç å¹¶æ’åº
    page_files = []
    for f in files:
        try:
            # æ–‡ä»¶åæ ¼å¼: video_search_page_{page}_{timestamp}.json
            parts = f.stem.split('_')
            page_num = int(parts[3])
            page_files.append((page_num, f))
        except (IndexError, ValueError):
            continue
            
    if not page_files:
        return 0, 0, "", ""
        
    # æŒ‰é¡µç æ’åºï¼Œå–æœ€å¤§
    page_files.sort(key=lambda x: x[0])
    last_page_num, last_file = page_files[-1]
    
    print(f"ğŸ” å‘ç°å·²æœ‰è¿›åº¦ï¼Œæœ€åé¡µç : {last_page_num}ï¼Œæ–‡ä»¶: {last_file.name}")
    
    try:
        with open(last_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        resp_data = data.get('data', {})
        config = resp_data.get('config', {})
        next_page_info = config.get('next_page', {})
        
        next_offset = next_page_info.get('cursor')
        next_search_id = next_page_info.get('search_id')
        next_backtrace = config.get('backtrace')
        
        # å¦‚æœç¼ºå°‘å…³é”®ç¿»é¡µä¿¡æ¯ï¼Œåˆ™æ— æ³•ç»§ç»­
        if next_offset is None or not next_search_id:
            print("âš ï¸ ä¸Šä¸€é¡µæ•°æ®ç¼ºå°‘ç¿»é¡µå‚æ•° (cursor/search_id)ï¼Œæ— æ³•æ–­ç‚¹ç»­ä¼ ")
            return last_page_num + 1, 0, "", "" # å°è¯•ä½†ä¸ä¸€å®šæˆåŠŸ
            
        return last_page_num + 1, next_offset, next_backtrace, next_search_id
        
    except Exception as e:
        print(f"âŒ è¯»å–ä¸Šä¸€é¡µæ•°æ®å¤±è´¥: {e}")
        return 0, 0, "", ""

def main():
    # å…³é”®è¯é…ç½®
    keyword = "çš®è‚¤å¥½ ä¸“å®¶"
    
    print("\n" + "="*70)
    print(f"å¼€å§‹è·å–æŠ–éŸ³è§†é¢‘æœç´¢æ•°æ® - å…³é”®è¯: {keyword}")
    print("="*70 + "\n")
    
    try:
        api_key = load_api_key()
    except Exception as e:
        print(f"âŒ {e}")
        return

    # è®¾ç½®è¾“å‡ºç›®å½•
    script_dir = Path(__file__).parent
    # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿ï¼Œé¿å…è·¯å¾„é—®é¢˜
    safe_keyword = keyword.replace(" ", "_")
    output_dir = script_dir.parent / 'output' / f'keyword_{safe_keyword}' / 'detail'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    rate_limiter = RateLimiter(max_per_second=2) # ä¿å®ˆä¸€ç‚¹
    
    # æ¢å¤çŠ¶æ€
    start_page, current_offset, current_backtrace, current_search_id = get_latest_state(output_dir)
    
    if start_page > 0:
        print(f"ğŸ“‹ æ¢å¤è¿›åº¦: ä»ç¬¬ {start_page} é¡µå¼€å§‹ (Offset: {current_offset})")
    else:
        print(f"ğŸ“‹ æ–°ä»»åŠ¡: ä»ç¬¬ 0 é¡µå¼€å§‹")
    
    # åˆå§‹å‚æ•°
    current_page = start_page
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ç›®æ ‡: è·å–æ‰€æœ‰é¡µï¼Œç›´åˆ°æ²¡æœ‰æ•°æ®
    # target_page_count = 3
    # end_page = start_page + target_page_count
    
    # è®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„ä¸Šé™ä»¥é˜²æ­»å¾ªç¯ï¼Œä½†ä¸»è¦ä¾èµ– has_more
    max_pages_limit = 100 
    
    while current_page < max_pages_limit:
        print(f"\nğŸ“„ æ­£åœ¨è·å–ç¬¬ {current_page} é¡µ...")
        
        data = fetch_video_search_page(
            keyword, 
            current_page, 
            current_offset, 
            current_backtrace, 
            current_search_id, 
            api_key, 
            rate_limiter
        )
        
        if not data or data.get('code') != 200:
            print(f"âŒ è·å–å¤±è´¥æˆ–è¿”å›é”™è¯¯ç : {data.get('code') if data else 'No Data'}")
            if data:
                print(f"   æ¶ˆæ¯: {data.get('message')}")
                # é‡åˆ°é”™è¯¯æ˜¯å¦ç»ˆæ­¢ï¼Ÿå¦‚æœæ˜¯ä¸´æ—¶çš„ç½‘ç»œé”™è¯¯å¯èƒ½éœ€è¦é‡è¯•ï¼Œè¿™é‡Œç®€å•å¤„ç†å…ˆç»ˆæ­¢
                # å¯ä»¥æ ¹æ®é”™è¯¯ç å†³å®š
            break
            
        # ä¿å­˜æ•°æ®
        filename = f"video_search_page_{current_page}_{timestamp}.json"
        filepath = output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ä¿å­˜: {filepath.name}")
        
        # æå–ä¸‹ä¸€é¡µå‚æ•°
        resp_data = data.get('data', {})
        config = resp_data.get('config', {})
        next_page_info = config.get('next_page', {})
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šæ•°æ®
        has_more = config.get('has_more', 0)
        print(f"   Has More: {has_more}")
        
        # è·å–ç¿»é¡µå‚æ•°
        next_offset = next_page_info.get('cursor')
        next_search_id = next_page_info.get('search_id')
        next_backtrace = config.get('backtrace')
        
        print(f"   Next Offset: {next_offset}")
        print(f"   Next Search ID: {next_search_id}")
        
        # éªŒè¯ç¿»é¡µæ˜¯å¦æœ‰æ•ˆ
        if current_page > 0 and next_offset == current_offset:
             print("âš ï¸ è­¦å‘Š: Offset æ²¡æœ‰å˜åŒ–ï¼Œå¯èƒ½é™·å…¥å¾ªç¯")
             # å¯ä»¥åœ¨è¿™é‡Œå¢åŠ é‡è¯•æˆ–è€…é€€å‡ºé€»è¾‘
             # æš‚æ—¶å…ˆè§‚å¯Ÿï¼Œå¦‚æœ API è®¾è®¡å°±æ˜¯è¿™æ ·å¯èƒ½ä¼šæœ‰é—®é¢˜
             
        # æ›´æ–°å‚æ•°
        if next_offset is not None:
            current_offset = next_offset
        else:
            print("âš ï¸ æœªæ‰¾åˆ° offset/cursor å­—æ®µ")
            
        if next_search_id:
            current_search_id = next_search_id
            
        if next_backtrace:
            current_backtrace = next_backtrace
            
        current_page += 1
        
        # å¦‚æœæ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œåœæ­¢ç¿»é¡µ
        if has_more == 0:
             print("âœ… has_more=0ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µ")
             break
             
        # ç®€å•çš„åçˆ¬è™«å»¶è¿Ÿ
        time.sleep(1)

    if current_page >= max_pages_limit:
        print(f"âš ï¸ è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ ({max_pages_limit})ï¼Œåœæ­¢è·å–")

    print(f"\n{'='*70}")
    print(f"âœ… ä»»åŠ¡å®Œæˆ")
    print(f"{'='*70}\n")

if __name__ == '__main__':
    main()

