#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç»§ç»­æœç´¢æŠ¤è‚¤è¾¾äººè„šæœ¬ï¼ˆä»ç¬¬4é¡µå¼€å§‹ï¼‰

åŠŸèƒ½ï¼š
1. ä»ç¬¬4é¡µå¼€å§‹ç»§ç»­æœç´¢
2. é€é¡µåˆ†æè…°éƒ¨è¾¾äººæƒ…å†µ
3. æ£€æŸ¥æ•°æ®é‡å¤æƒ…å†µ
4. å¦‚æœæ²¡æœ‰æ›´å¤šæ•°æ®åˆ™åœæ­¢
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv


def load_api_key():
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key"""
    backend_dir = Path(__file__).parent.parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®")
    
    return api_key


def fetch_user_search_v4(api_key: str, keyword: str, cursor: int = 0, offset: int = 0, 
                        page: int = 0, search_id: str = "", count: int = 10, 
                        max_retries: int = 4) -> dict:
    """
    è°ƒç”¨ TikHub API çš„ fetch_user_search_v4 æ¥å£ï¼ˆæ”¯æŒé‡è¯•ï¼‰
    
    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆç¬¬1æ¬¡é—´éš”2ç§’ï¼Œåç»­10ç§’ã€20ç§’ã€40ç§’ï¼‰
    """
    url = "https://api.tikhub.io/api/v1/douyin/search/fetch_user_search_v4"
    
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    payload = {
        'keyword': keyword,
        'cursor': cursor,
        'offset': offset,
        'page': page,
        'search_id': search_id,
        'count': count,
        'search_channel': 'aweme_user_web',
        'sort_type': 0,
        'publish_time': 0
    }
    
    # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼šç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’
    retry_delays = [2, 10, 20, 40]
    
    for attempt in range(max_retries):
        if attempt > 0:
            delay = retry_delays[attempt - 1]
            print(f"   ğŸ”„ ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼ˆç­‰å¾… {delay} ç§’ï¼‰...")
            time.sleep(delay)
        else:
            print(f"\nğŸ“¡ å‘é€è¯·æ±‚: cursor={cursor}, offset={offset}, page={page}, search_id={search_id[:20] if search_id else '(ç©º)'}...")
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                code = result.get('code', -1)
                
                if code == 200:
                    result['_request_payload'] = payload
                    result['_request_headers'] = {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()}
                    result['_attempt'] = attempt + 1
                    print(f"   âœ… è¯·æ±‚æˆåŠŸï¼ˆå°è¯• {attempt + 1} æ¬¡ï¼‰")
                    return result
                else:
                    print(f"   âŒ API è¿”å›é”™è¯¯ç : {code}")
                    result['_request_payload'] = payload
                    result['_request_headers'] = {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()}
                    result['_attempt'] = attempt + 1
                    result['_error'] = f"API error code: {code}"
                    
                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        continue
                    return result
            else:
                print(f"   âŒ HTTP è¯·æ±‚å¤±è´¥: {response.status_code}")
                error_result = {
                    "error": f"HTTP {response.status_code}",
                    "response_text": response.text[:500],
                    "_request_payload": payload,
                    "_request_headers": {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()},
                    "_attempt": attempt + 1
                }
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                if attempt < max_retries - 1:
                    continue
                return error_result
                
        except Exception as e:
            print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
            error_result = {
                "error": str(e),
                "_request_payload": payload,
                "_request_headers": {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()},
                "_attempt": attempt + 1
            }
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
            if attempt < max_retries - 1:
                continue
            return error_result
    
    # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
    return {
        "error": "All retry attempts failed",
        "_request_payload": payload,
        "_request_headers": {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()},
        "_attempt": max_retries
    }


def analyze_page_waist_kols(users: list, page_num: int) -> dict:
    """åˆ†æå•é¡µçš„è…°éƒ¨è¾¾äººæƒ…å†µ"""
    waist_kols = []
    
    for user in users:
        user_info = user.get('user_info', {})
        follower_count = user_info.get('follower_count', 0)
        
        # è…°éƒ¨è¾¾äººå®šä¹‰ï¼š10ä¸‡~100ä¸‡ç²‰ä¸
        if 100_000 <= follower_count <= 1_000_000:
            waist_kols.append({
                'nickname': user_info.get('nickname', 'N/A'),
                'follower_count': follower_count,
                'uid': user_info.get('uid', ''),
                'unique_id': user_info.get('unique_id', ''),
                'signature': user_info.get('signature', '')[:50]
            })
    
    # æŒ‰ç²‰ä¸æ•°æ’åº
    waist_kols.sort(key=lambda x: x['follower_count'], reverse=True)
    
    return {
        'page_num': page_num,
        'total_users': len(users),
        'waist_kol_count': len(waist_kols),
        'waist_kols': waist_kols
    }


def load_previous_uids(output_dir: str) -> set:
    """åŠ è½½ä¹‹å‰å·²ç»è·å–çš„ç”¨æˆ·UID"""
    previous_uids = set()
    
    # è¯»å–ä¹‹å‰çš„æœç´¢ç»“æœ
    search_files = [f for f in os.listdir(output_dir) if f.startswith('search_results_3pages_')]
    if search_files:
        latest_file = sorted(search_files)[-1]
        file_path = os.path.join(output_dir, latest_file)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            users = data.get('users', [])
            
            for user in users:
                user_info = user.get('user_info', {})
                uid = user_info.get('uid', '')
                if uid:
                    previous_uids.add(uid)
    
    return previous_uids


def main():
    """ä¸»å‡½æ•°ï¼šç»§ç»­æœç´¢ç¬¬21-30é¡µï¼ˆä»ä¸Šæ¬¡å‚æ•°ç»§ç»­ï¼‰"""
    
    print("=" * 60)
    print("æŠ–éŸ³æŠ¤è‚¤è¾¾äººæœç´¢å·¥å…· - ç»§ç»­æœç´¢ï¼ˆç¬¬21-30é¡µï¼‰")
    print("è¯´æ˜ï¼šè¯·æ±‚é—´éš”2ç§’ï¼Œå¤±è´¥åé‡è¯•æœ€å¤š4æ¬¡ï¼ˆç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’ï¼‰")
    print("=" * 60)
    
    # 1. åŠ è½½ API Key
    print("\n1ï¸âƒ£ åŠ è½½ API é…ç½®...")
    try:
        api_key = load_api_key()
        print(f"âœ… API Key å·²åŠ è½½")
    except ValueError as e:
        print(f"âŒ {e}")
        return
    
    # 2. å‡†å¤‡è¾“å‡ºç›®å½•
    script_dir = Path(__file__).parent.parent
    output_dir = script_dir / "output"
    detail_dir = output_dir / "detail"
    os.makedirs(detail_dir, exist_ok=True)
    
    # 3. åŠ è½½ä¹‹å‰çš„UIDï¼ˆç”¨äºå»é‡æ£€æµ‹ï¼‰
    print("\n2ï¸âƒ£ åŠ è½½å·²æœ‰æ•°æ®...")
    previous_uids = load_previous_uids(str(output_dir))
    print(f"âœ… å·²åŠ è½½ {len(previous_uids)} ä¸ªå·²æœ‰ç”¨æˆ·UID")
    
    # 4. ä»ç¬¬20é¡µçš„å“åº”ä¸­è¯»å–å‚æ•°ï¼Œç»§ç»­è¯·æ±‚
    print("\n3ï¸âƒ£ ä»ä¸Šæ¬¡ç»“æœç»§ç»­æœç´¢...")
    keyword = "æŠ¤è‚¤"
    count_per_page = 20
    
    # è¯»å–ç¬¬20é¡µçš„å“åº”ï¼Œè·å–ç¿»é¡µå‚æ•°
    page_20_file = detail_dir / 'page_20_request_response.json'
    if not page_20_file.exists():
        print(f"âŒ æœªæ‰¾åˆ°ç¬¬20é¡µçš„å“åº”æ–‡ä»¶: {page_20_file}")
        return
    
    with open(page_20_file, 'r', encoding='utf-8') as f:
        page_20_data = json.load(f)
    
    # æå–ç¿»é¡µå‚æ•°
    response_data = page_20_data.get('response', {}).get('data', {})
    config = response_data.get('config', {})
    next_page_info = config.get('next_page', {})
    
    cursor = next_page_info.get('cursor', 0)
    offset = next_page_info.get('offset', 0)
    search_id = next_page_info.get('search_id', '') or next_page_info.get('search_request_id', '')
    page = 20  # ä»ç¬¬20é¡µç»§ç»­ï¼Œä¸‹ä¸€é¡µæ˜¯21
    
    print(f"âœ… ä»ç¬¬20é¡µç»§ç»­ï¼Œå‚æ•°: cursor={cursor}, offset={offset}, page={page}, search_id={search_id[:20]}...")
    
    all_new_users = []
    all_page_analyses = []
    total_duplicates = 0
    
    for page_num in range(21, 31):  # ç¬¬21-30é¡µ
        print(f"\n{'='*60}")
        print(f"[ç¬¬ {page_num} é¡µ]")
        print(f"{'='*60}")
        
        # è°ƒç”¨ APIï¼ˆæ”¯æŒé‡è¯•ï¼‰
        result = fetch_user_search_v4(api_key, keyword, cursor, offset, page, search_id, count_per_page, max_retries=4)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        has_error = 'error' in result or result.get('code') != 200
        
        # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½ä¿å­˜è¯¦ç»†ä¿¡æ¯
        if has_error:
            # ä¿å­˜é”™è¯¯å“åº”åˆ° error_ å¼€å¤´çš„æ–‡ä»¶
            error_file = detail_dir / f'error_page_{page_num}_request_response.json'
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'page_num': page_num,
                    'request_payload': result.get('_request_payload', {}),
                    'request_headers': result.get('_request_headers', {}),
                    'error': result.get('error', 'Unknown error'),
                    'attempt': result.get('_attempt', 1),
                    'response': result
                }, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å·²ä¿å­˜é”™è¯¯è¯¦æƒ…åˆ°: {error_file.name}")
            print(f"âš ï¸ ç¬¬ {page_num} é¡µè·å–å¤±è´¥ï¼ˆå·²é‡è¯•{result.get('_attempt', 1)}æ¬¡ï¼‰ï¼Œåœæ­¢æœç´¢")
            break
        
        # æå–æ•°æ®
        data = result.get('data', {})
        inner_data = data.get('data', [])
        config = data.get('config', {})
        user_list = inner_data if isinstance(inner_data, list) else []
        
        if not user_list:
            # ä¿å­˜ç©ºæ•°æ®çš„è¯¦æƒ…
            detail_file = detail_dir / f'page_{page_num}_request_response.json'
            with open(detail_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'page_num': page_num,
                    'request_payload': result.get('_request_payload', {}),
                    'request_headers': result.get('_request_headers', {}),
                    'response': result
                }, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å·²ä¿å­˜è¯¦æƒ…åˆ°: {detail_file.name}")
            print(f"âš ï¸ ç¬¬ {page_num} é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢æœç´¢")
            break
        
        print(f"âœ… è·å–åˆ° {len(user_list)} ä¸ªç”¨æˆ·")
        
        # ä¿å­˜è¯¦ç»†è¯·æ±‚/å“åº”
        detail_file = detail_dir / f'page_{page_num}_request_response.json'
        with open(detail_file, 'w', encoding='utf-8') as f:
            json.dump({
                'page_num': page_num,
                'request_payload': result.get('_request_payload', {}),
                'request_headers': result.get('_request_headers', {}),
                'attempt': result.get('_attempt', 1),
                'response': result
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å·²ä¿å­˜è¯¦æƒ…åˆ°: {detail_file.name}")
        
        # æ£€æŸ¥é‡å¤
        new_users = []
        duplicate_count = 0
        
        for user in user_list:
            user_info = user.get('user_info', {})
            uid = user_info.get('uid', '')
            
            if uid in previous_uids:
                duplicate_count += 1
            else:
                if uid:
                    previous_uids.add(uid)
                new_users.append(user)
        
        all_new_users.extend(new_users)
        total_duplicates += duplicate_count
        
        print(f"\nğŸ“Š æœ¬é¡µç»Ÿè®¡:")
        print(f"   åŸå§‹ç”¨æˆ·æ•°: {len(user_list)}")
        print(f"   æ–°å¢ç”¨æˆ·æ•°: {len(new_users)}")
        print(f"   é‡å¤ç”¨æˆ·æ•°: {duplicate_count}")
        
        # åˆ†ææœ¬é¡µçš„è…°éƒ¨è¾¾äºº
        page_analysis = analyze_page_waist_kols(user_list, page_num)
        all_page_analyses.append(page_analysis)
        
        print(f"\nğŸ¯ æœ¬é¡µè…°éƒ¨è¾¾äººåˆ†æ (10ä¸‡~100ä¸‡ç²‰ä¸):")
        print(f"   è…°éƒ¨è¾¾äººæ•°: {page_analysis['waist_kol_count']}")
        print(f"   å æœ¬é¡µæ¯”ä¾‹: {(page_analysis['waist_kol_count']/len(user_list)*100):.1f}%")
        
        if page_analysis['waist_kols']:
            print(f"\n   æœ¬é¡µè…°éƒ¨è¾¾äºº TOP 5:")
            for i, kol in enumerate(page_analysis['waist_kols'][:5], 1):
                print(f"   {i}. {kol['nickname']} - ç²‰ä¸: {kol['follower_count']:,}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
        has_more = config.get('has_more', 0) == 1
        next_page_info = config.get('next_page', {})
        
        if not has_more:
            print(f"\nâœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {page_num} é¡µ")
            break
        
        # æ›´æ–°ç¿»é¡µå‚æ•°
        if next_page_info:
            cursor = next_page_info.get('cursor', cursor)
            if not search_id and 'search_request_id' in next_page_info:
                search_id = next_page_info.get('search_request_id', '')
            elif not search_id and 'search_id' in next_page_info:
                search_id = next_page_info.get('search_id', '')
        
        page += 1
        
        # è¯·æ±‚é—´éš”2ç§’ï¼ˆéµå®ˆRPSé™æµï¼‰
        if page_num < 30:
            print(f"\nâ³ ç­‰å¾… 2 ç§’åç»§ç»­ï¼ˆéµå®ˆ RPS é™æµï¼‰...")
            time.sleep(2)
    
    # 5. ä¿å­˜æ±‡æ€»ç»“æœ
    print(f"\n{'='*60}")
    print(f"4ï¸âƒ£ ä¿å­˜æ±‡æ€»ç»“æœ...")
    print(f"{'='*60}")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜æ–°å¢ç”¨æˆ·æ•°æ®
    new_users_file = output_dir / f'search_results_pages_21_to_{page_num}_{timestamp}.json'
    with open(new_users_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'keyword': keyword,
                'search_date': datetime.now().isoformat(),
                'page_range': f'21-{page_num}',
                'total_new_users': len(all_new_users),
                'total_duplicates': total_duplicates,
                'request_interval': '2ç§’',
                'retry_policy': 'å¤±è´¥åé‡è¯•æœ€å¤š4æ¬¡ï¼Œç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’'
            },
            'users': all_new_users
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ æ–°å¢ç”¨æˆ·æ•°æ®: {new_users_file.name}")
    
    # ä¿å­˜é€é¡µåˆ†æç»“æœ
    page_analysis_file = output_dir / f'page_by_page_analysis_pages_21_to_{page_num}_{timestamp}.json'
    with open(page_analysis_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'keyword': keyword,
                'analysis_date': datetime.now().isoformat(),
                'page_range': f'21-{page_num}',
                'total_pages': len(all_page_analyses),
                'request_interval': '2ç§’',
                'retry_policy': 'å¤±è´¥åé‡è¯•æœ€å¤š4æ¬¡ï¼Œç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’'
            },
            'page_analyses': all_page_analyses
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ é€é¡µåˆ†æç»“æœ: {page_analysis_file.name}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    report_file = output_dir / f'continue_search_report_pages_21_to_{page_num}_{timestamp}.md'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"# æŠ¤è‚¤è¾¾äººç»§ç»­æœç´¢æŠ¥å‘Šï¼ˆç¬¬21-{page_num}é¡µï¼‰\n\n")
        f.write(f"**æœç´¢å…³é”®è¯**: {keyword}\n")
        f.write(f"**æœç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**é¡µé¢èŒƒå›´**: ç¬¬21é¡µ - ç¬¬{page_num}é¡µ\n")
        f.write(f"**è¯·æ±‚é—´éš”**: 2ç§’\n")
        f.write(f"**é‡è¯•ç­–ç•¥**: å¤±è´¥åé‡è¯•æœ€å¤š4æ¬¡ï¼Œç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’\n\n")
        
        f.write(f"## æ€»ä½“ç»Ÿè®¡\n\n")
        f.write(f"- æ–°å¢ç”¨æˆ·æ•°: {len(all_new_users)}\n")
        f.write(f"- é‡å¤ç”¨æˆ·æ•°: {total_duplicates}\n")
        f.write(f"- æ€»é¡µæ•°: {len(all_page_analyses)}\n\n")
        
        f.write(f"## é€é¡µåˆ†æ\n\n")
        
        total_waist_kols = 0
        for analysis in all_page_analyses:
            total_waist_kols += analysis['waist_kol_count']
            
            f.write(f"### ç¬¬ {analysis['page_num']} é¡µ\n\n")
            f.write(f"- ç”¨æˆ·æ€»æ•°: {analysis['total_users']}\n")
            f.write(f"- è…°éƒ¨è¾¾äººæ•°: {analysis['waist_kol_count']}\n")
            f.write(f"- è…°éƒ¨è¾¾äººå æ¯”: {(analysis['waist_kol_count']/analysis['total_users']*100):.1f}%\n\n")
            
            if analysis['waist_kols']:
                f.write(f"#### TOP 3 è…°éƒ¨è¾¾äºº\n\n")
                f.write(f"| æ’å | æ˜µç§° | ç²‰ä¸æ•° | æŠ–éŸ³å· |\n")
                f.write(f"|------|------|--------|--------|\n")
                
                for i, kol in enumerate(analysis['waist_kols'][:3], 1):
                    f.write(f"| {i} | {kol['nickname']} | {kol['follower_count']:,} | {kol['unique_id']} |\n")
                
                f.write(f"\n")
        
        f.write(f"## è…°éƒ¨è¾¾äººæ±‡æ€»\n\n")
        f.write(f"- æ€»è…°éƒ¨è¾¾äººæ•°: {total_waist_kols}\n")
        if all_page_analyses:
            f.write(f"- å¹³å‡æ¯é¡µè…°éƒ¨è¾¾äººæ•°: {(total_waist_kols/len(all_page_analyses)):.1f}\n")
        else:
            f.write(f"- å¹³å‡æ¯é¡µè…°éƒ¨è¾¾äººæ•°: 0\n")
    
    print(f"ğŸ’¾ æ±‡æ€»æŠ¥å‘Š: {report_file.name}")
    
    # æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*60}")
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"ğŸ“Œ æ±‡æ€»ä¿¡æ¯:")
    print(f"   æœç´¢é¡µé¢: ç¬¬21é¡µ - ç¬¬{page_num}é¡µ")
    print(f"   æ–°å¢ç”¨æˆ·: {len(all_new_users)} äºº")
    print(f"   é‡å¤ç”¨æˆ·: {total_duplicates} äºº")
    print(f"   æ–°å¢è…°éƒ¨è¾¾äºº: {total_waist_kols} äºº")
    print(f"   è¯·æ±‚é—´éš”: 2ç§’")
    print(f"   é‡è¯•ç­–ç•¥: å¤±è´¥åé‡è¯•æœ€å¤š4æ¬¡ï¼Œç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’")


if __name__ == "__main__":
    main()

