#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®Œæ•´æœç´¢æŠ¤è‚¤è¾¾äººåšä¸»è„šæœ¬ï¼ˆ1-30é¡µï¼‰

åŠŸèƒ½ï¼š
1. æœç´¢å…³é”®è¯ï¼šæŠ¤è‚¤ è¾¾äºº åšä¸»
2. è·å–1-30é¡µæ•°æ®
3. è¯·æ±‚é—´éš”2ç§’
4. å¤±è´¥é‡è¯•æœ€å¤š4æ¬¡ï¼ˆç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’ï¼‰
5. è¾“å‡ºåˆ°ç‹¬ç«‹ç›®å½•
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv


def load_api_key():
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key"""
    backend_dir = Path(__file__).parent.parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®")
    
    return api_key


def fetch_user_search_v4(api_key: str, keyword: str, cursor: int = 0, offset: int = 0, 
                        page: int = 0, search_id: str = "", count: int = 10, 
                        max_retries: int = 4) -> dict:
    """
    è°ƒç”¨ TikHub API çš„ fetch_user_search_v4 æ¥å£ï¼ˆæ”¯æŒé‡è¯•ï¼‰
    
    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆç¬¬1æ¬¡é—´éš”2ç§’ï¼Œåç»­10ç§’ã€20ç§’ã€40ç§’ï¼‰
    """
    url = "https://api.tikhub.io/api/v1/douyin/search/fetch_user_search_v4"
    
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    payload = {
        'keyword': keyword,
        'cursor': cursor,
        'offset': offset,
        'page': page,
        'search_id': search_id,
        'count': count,
        'search_channel': 'aweme_user_web',
        'sort_type': 0,
        'publish_time': 0
    }
    
    # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼šç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’
    retry_delays = [2, 10, 20, 40]
    
    for attempt in range(max_retries):
        if attempt > 0:
            delay = retry_delays[attempt - 1]
            print(f"   ğŸ”„ ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼ˆç­‰å¾… {delay} ç§’ï¼‰...")
            time.sleep(delay)
        else:
            print(f"\nğŸ“¡ å‘é€è¯·æ±‚: cursor={cursor}, offset={offset}, page={page}, search_id={search_id[:20] if search_id else '(ç©º)'}...")
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                code = result.get('code', -1)
                
                if code == 200:
                    result['_request_payload'] = payload
                    result['_request_headers'] = {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()}
                    result['_attempt'] = attempt + 1
                    print(f"   âœ… è¯·æ±‚æˆåŠŸï¼ˆå°è¯• {attempt + 1} æ¬¡ï¼‰")
                    return result
                else:
                    print(f"   âŒ API è¿”å›é”™è¯¯ç : {code}")
                    result['_request_payload'] = payload
                    result['_request_headers'] = {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()}
                    result['_attempt'] = attempt + 1
                    result['_error'] = f"API error code: {code}"
                    
                    if attempt < max_retries - 1:
                        continue
                    return result
            else:
                print(f"   âŒ HTTP è¯·æ±‚å¤±è´¥: {response.status_code}")
                error_result = {
                    "error": f"HTTP {response.status_code}",
                    "response_text": response.text[:500],
                    "_request_payload": payload,
                    "_request_headers": {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()},
                    "_attempt": attempt + 1
                }
                
                if attempt < max_retries - 1:
                    continue
                return error_result
                
        except Exception as e:
            print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
            error_result = {
                "error": str(e),
                "_request_payload": payload,
                "_request_headers": {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()},
                "_attempt": attempt + 1
            }
            
            if attempt < max_retries - 1:
                continue
            return error_result
    
    return {
        "error": "All retry attempts failed",
        "_request_payload": payload,
        "_request_headers": {k: v if k != 'Authorization' else f"{v[:20]}..." for k, v in headers.items()},
        "_attempt": max_retries
    }


def analyze_page_waist_kols(users: list, page_num: int) -> dict:
    """åˆ†æå•é¡µçš„è…°éƒ¨è¾¾äººæƒ…å†µ"""
    waist_kols = []
    
    for user in users:
        user_info = user.get('user_info', {})
        follower_count = user_info.get('follower_count', 0)
        
        # è…°éƒ¨è¾¾äººå®šä¹‰ï¼š10ä¸‡~100ä¸‡ç²‰ä¸
        if 100_000 <= follower_count <= 1_000_000:
            waist_kols.append({
                'nickname': user_info.get('nickname', 'N/A'),
                'follower_count': follower_count,
                'uid': user_info.get('uid', ''),
                'unique_id': user_info.get('unique_id', ''),
                'signature': user_info.get('signature', '')[:50]
            })
    
    # æŒ‰ç²‰ä¸æ•°æ’åº
    waist_kols.sort(key=lambda x: x['follower_count'], reverse=True)
    
    return {
        'page_num': page_num,
        'total_users': len(users),
        'waist_kol_count': len(waist_kols),
        'waist_kols': waist_kols
    }


def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´æœç´¢1-30é¡µ"""
    
    print("=" * 60)
    print("æŠ–éŸ³æŠ¤è‚¤è¾¾äººåšä¸»æœç´¢å·¥å…· - å®Œæ•´æœç´¢ï¼ˆ1-30é¡µï¼‰")
    print("å…³é”®è¯ï¼šæŠ¤è‚¤ è¾¾äºº åšä¸»")
    print("è¯´æ˜ï¼šè¯·æ±‚é—´éš”2ç§’ï¼Œå¤±è´¥åé‡è¯•æœ€å¤š4æ¬¡ï¼ˆç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’ï¼‰")
    print("=" * 60)
    
    # 1. åŠ è½½ API Key
    print("\n1ï¸âƒ£ åŠ è½½ API é…ç½®...")
    try:
        api_key = load_api_key()
        print(f"âœ… API Key å·²åŠ è½½")
    except ValueError as e:
        print(f"âŒ {e}")
        return
    
    # 2. å‡†å¤‡è¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨æ–°ç›®å½•ï¼‰
    script_dir = Path(__file__).parent.parent
    timestamp_prefix = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = script_dir / f"output_kol_full_{timestamp_prefix}"
    detail_dir = output_dir / "detail"
    os.makedirs(detail_dir, exist_ok=True)
    print(f"\n2ï¸âƒ£ è¾“å‡ºç›®å½•: {output_dir}")
    
    # 3. å¼€å§‹æœç´¢
    print("\n3ï¸âƒ£ å¼€å§‹æœç´¢...")
    keyword = "æŠ¤è‚¤ è¾¾äºº åšä¸»"
    count_per_page = 20
    
    # åˆå§‹åŒ–ç¿»é¡µå‚æ•°
    cursor = 0
    offset = 0
    page = 0
    search_id = ""
    
    all_users = []
    seen_uids = set()
    all_page_analyses = []
    total_duplicates = 0
    
    for page_num in range(1, 31):  # ç¬¬1-30é¡µ
        print(f"\n{'='*60}")
        print(f"[ç¬¬ {page_num} é¡µ]")
        print(f"{'='*60}")
        
        # è°ƒç”¨ APIï¼ˆæ”¯æŒé‡è¯•ï¼‰
        result = fetch_user_search_v4(api_key, keyword, cursor, offset, page, search_id, count_per_page, max_retries=4)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        has_error = 'error' in result or result.get('code') != 200
        
        # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½ä¿å­˜è¯¦ç»†ä¿¡æ¯
        if has_error:
            # ä¿å­˜é”™è¯¯å“åº”
            error_file = detail_dir / f'error_page_{page_num}_request_response.json'
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'page_num': page_num,
                    'request_payload': result.get('_request_payload', {}),
                    'request_headers': result.get('_request_headers', {}),
                    'error': result.get('error', 'Unknown error'),
                    'attempt': result.get('_attempt', 1),
                    'response': result
                }, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å·²ä¿å­˜é”™è¯¯è¯¦æƒ…åˆ°: {error_file.name}")
            print(f"âš ï¸ ç¬¬ {page_num} é¡µè·å–å¤±è´¥ï¼ˆå·²é‡è¯•{result.get('_attempt', 1)}æ¬¡ï¼‰ï¼Œåœæ­¢æœç´¢")
            break
        
        # æå–æ•°æ®
        data = result.get('data', {})
        inner_data = data.get('data', [])
        config = data.get('config', {})
        user_list = inner_data if isinstance(inner_data, list) else []
        
        if not user_list:
            # ä¿å­˜ç©ºæ•°æ®çš„è¯¦æƒ…
            detail_file = detail_dir / f'page_{page_num}_request_response.json'
            with open(detail_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'page_num': page_num,
                    'request_payload': result.get('_request_payload', {}),
                    'request_headers': result.get('_request_headers', {}),
                    'response': result
                }, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å·²ä¿å­˜è¯¦æƒ…åˆ°: {detail_file.name}")
            print(f"âš ï¸ ç¬¬ {page_num} é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢æœç´¢")
            break
        
        print(f"âœ… è·å–åˆ° {len(user_list)} ä¸ªç”¨æˆ·")
        
        # ä¿å­˜è¯¦ç»†è¯·æ±‚/å“åº”
        detail_file = detail_dir / f'page_{page_num}_request_response.json'
        with open(detail_file, 'w', encoding='utf-8') as f:
            json.dump({
                'page_num': page_num,
                'request_payload': result.get('_request_payload', {}),
                'request_headers': result.get('_request_headers', {}),
                'attempt': result.get('_attempt', 1),
                'response': result
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å·²ä¿å­˜è¯¦æƒ…åˆ°: {detail_file.name}")
        
        # æ£€æŸ¥é‡å¤å¹¶æ·»åŠ ç”¨æˆ·
        new_users = 0
        duplicate_count = 0
        
        for user in user_list:
            user_info = user.get('user_info', {})
            uid = user_info.get('uid', '')
            
            if uid and uid not in seen_uids:
                seen_uids.add(uid)
                all_users.append(user)
                new_users += 1
            else:
                duplicate_count += 1
        
        total_duplicates += duplicate_count
        
        print(f"\nğŸ“Š æœ¬é¡µç»Ÿè®¡:")
        print(f"   åŸå§‹ç”¨æˆ·æ•°: {len(user_list)}")
        print(f"   æ–°å¢ç”¨æˆ·æ•°: {new_users}")
        print(f"   é‡å¤ç”¨æˆ·æ•°: {duplicate_count}")
        
        # åˆ†ææœ¬é¡µçš„è…°éƒ¨è¾¾äºº
        page_analysis = analyze_page_waist_kols(user_list, page_num)
        all_page_analyses.append(page_analysis)
        
        print(f"\nğŸ¯ æœ¬é¡µè…°éƒ¨è¾¾äººåˆ†æ (10ä¸‡~100ä¸‡ç²‰ä¸):")
        print(f"   è…°éƒ¨è¾¾äººæ•°: {page_analysis['waist_kol_count']}")
        print(f"   å æœ¬é¡µæ¯”ä¾‹: {(page_analysis['waist_kol_count']/len(user_list)*100):.1f}%")
        
        if page_analysis['waist_kols']:
            print(f"\n   æœ¬é¡µè…°éƒ¨è¾¾äºº TOP 3:")
            for i, kol in enumerate(page_analysis['waist_kols'][:3], 1):
                print(f"   {i}. {kol['nickname']} - ç²‰ä¸: {kol['follower_count']:,}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
        has_more = config.get('has_more', 0) == 1
        next_page_info = config.get('next_page', {})
        
        if not has_more:
            print(f"\nâœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {page_num} é¡µ")
            break
        
        # æ›´æ–°ç¿»é¡µå‚æ•°
        if next_page_info:
            cursor = next_page_info.get('cursor', cursor)
            if not search_id and 'search_request_id' in next_page_info:
                search_id = next_page_info.get('search_request_id', '')
            elif not search_id and 'search_id' in next_page_info:
                search_id = next_page_info.get('search_id', '')
        
        page += 1
        
        # è¯·æ±‚é—´éš”2ç§’
        if page_num < 30:
            print(f"\nâ³ ç­‰å¾… 2 ç§’åç»§ç»­...")
            time.sleep(2)
    
    # 4. ä¿å­˜æ±‡æ€»ç»“æœ
    print(f"\n{'='*60}")
    print(f"4ï¸âƒ£ ä¿å­˜æ±‡æ€»ç»“æœ...")
    print(f"{'='*60}")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜æ‰€æœ‰ç”¨æˆ·æ•°æ®
    all_users_file = output_dir / f'all_users_{timestamp}.json'
    with open(all_users_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'keyword': keyword,
                'search_date': datetime.now().isoformat(),
                'total_pages': len(all_page_analyses),
                'total_unique_users': len(all_users),
                'total_duplicates': total_duplicates,
                'request_interval': '2ç§’',
                'retry_policy': 'å¤±è´¥åé‡è¯•æœ€å¤š4æ¬¡ï¼Œç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’'
            },
            'users': all_users
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ æ‰€æœ‰ç”¨æˆ·æ•°æ®: {all_users_file.name}")
    
    # ä¿å­˜é€é¡µåˆ†æç»“æœ
    page_analysis_file = output_dir / f'page_by_page_analysis_{timestamp}.json'
    with open(page_analysis_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'keyword': keyword,
                'analysis_date': datetime.now().isoformat(),
                'total_pages': len(all_page_analyses),
                'request_interval': '2ç§’',
                'retry_policy': 'å¤±è´¥åé‡è¯•æœ€å¤š4æ¬¡ï¼Œç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’'
            },
            'page_analyses': all_page_analyses
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ é€é¡µåˆ†æç»“æœ: {page_analysis_file.name}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    report_file = output_dir / f'search_report_{timestamp}.md'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"# æŠ¤è‚¤è¾¾äººåšä¸»æœç´¢æŠ¥å‘Š\n\n")
        f.write(f"**æœç´¢å…³é”®è¯**: {keyword}\n")
        f.write(f"**æœç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**é¡µé¢èŒƒå›´**: ç¬¬1é¡µ - ç¬¬{len(all_page_analyses)}é¡µ\n")
        f.write(f"**è¯·æ±‚é—´éš”**: 2ç§’\n")
        f.write(f"**é‡è¯•ç­–ç•¥**: å¤±è´¥åé‡è¯•æœ€å¤š4æ¬¡ï¼Œç¬¬1æ¬¡2ç§’ï¼Œåç»­10-20-40ç§’\n\n")
        
        f.write(f"## æ€»ä½“ç»Ÿè®¡\n\n")
        f.write(f"- å”¯ä¸€ç”¨æˆ·æ•°: {len(all_users)}\n")
        f.write(f"- é‡å¤ç”¨æˆ·æ•°: {total_duplicates}\n")
        f.write(f"- æ€»é¡µæ•°: {len(all_page_analyses)}\n\n")
        
        f.write(f"## é€é¡µåˆ†æ\n\n")
        
        total_waist_kols = 0
        for analysis in all_page_analyses:
            total_waist_kols += analysis['waist_kol_count']
            
            f.write(f"### ç¬¬ {analysis['page_num']} é¡µ\n\n")
            f.write(f"- ç”¨æˆ·æ€»æ•°: {analysis['total_users']}\n")
            f.write(f"- è…°éƒ¨è¾¾äººæ•°: {analysis['waist_kol_count']}\n")
            f.write(f"- è…°éƒ¨è¾¾äººå æ¯”: {(analysis['waist_kol_count']/analysis['total_users']*100):.1f}%\n\n")
            
            if analysis['waist_kols'] and len(analysis['waist_kols']) > 0:
                f.write(f"#### TOP 3 è…°éƒ¨è¾¾äºº\n\n")
                f.write(f"| æ’å | æ˜µç§° | ç²‰ä¸æ•° | æŠ–éŸ³å· |\n")
                f.write(f"|------|------|--------|--------|\n")
                
                for i, kol in enumerate(analysis['waist_kols'][:3], 1):
                    f.write(f"| {i} | {kol['nickname']} | {kol['follower_count']:,} | {kol['unique_id']} |\n")
                
                f.write(f"\n")
        
        f.write(f"## è…°éƒ¨è¾¾äººæ±‡æ€»\n\n")
        f.write(f"- æ€»è…°éƒ¨è¾¾äººæ•°: {total_waist_kols}\n")
        if all_page_analyses:
            f.write(f"- å¹³å‡æ¯é¡µè…°éƒ¨è¾¾äººæ•°: {(total_waist_kols/len(all_page_analyses)):.1f}\n")
    
    print(f"ğŸ’¾ æ±‡æ€»æŠ¥å‘Š: {report_file.name}")
    
    # æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*60}")
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"ğŸ“Œ æ±‡æ€»ä¿¡æ¯:")
    print(f"   æœç´¢å…³é”®è¯: {keyword}")
    print(f"   æœç´¢é¡µæ•°: ç¬¬1é¡µ - ç¬¬{len(all_page_analyses)}é¡µ")
    print(f"   å”¯ä¸€ç”¨æˆ·æ•°: {len(all_users)} äºº")
    print(f"   é‡å¤ç”¨æˆ·æ•°: {total_duplicates} äºº")
    print(f"   è…°éƒ¨è¾¾äººæ•°: {total_waist_kols} äºº")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")


if __name__ == "__main__":
    main()

