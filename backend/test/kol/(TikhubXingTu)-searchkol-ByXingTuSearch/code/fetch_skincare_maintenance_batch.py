#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è·å–"æŠ¤è‚¤ä¿å…»"å…³é”®è¯æ•°æ®ï¼ˆæ”¯æŒé€Ÿç‡é™åˆ¶å’Œé‡è¯•ï¼‰

åŠŸèƒ½ï¼š
1. æ£€æŸ¥å·²æœ‰çš„é¡µé¢æ•°æ®ï¼Œé¿å…é‡å¤è·å–
2. æ‰¹é‡è·å–æŒ‡å®šèŒƒå›´çš„é¡µé¢
3. å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š3æ¬¡ï¼‰
4. é€Ÿç‡é™åˆ¶ï¼ˆæ¯ç§’æœ€å¤š5ä¸ªè¯·æ±‚ï¼‰
5. å®æ—¶æ˜¾ç¤ºè¿›åº¦

ä½œè€…: AI Agent
åˆ›å»ºæ—¶é—´: 2025-11-18
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
import threading
from queue import Queue


class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨ï¼šæ¯ç§’æœ€å¤šNä¸ªè¯·æ±‚"""
    def __init__(self, max_per_second=5):
        self.max_per_second = max_per_second  # æ¯ç§’æœ€å¤šè¯·æ±‚æ•°
        self.min_interval = 1.0 / max_per_second  # æœ€å°é—´éš”æ—¶é—´
        self.last_request_time = 0  # ä¸Šæ¬¡è¯·æ±‚æ—¶é—´
        self.lock = threading.Lock()  # çº¿ç¨‹é”
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…åˆ°å¯ä»¥å‘èµ·ä¸‹ä¸€ä¸ªè¯·æ±‚"""
        with self.lock:
            current_time = time.time()
            time_since_last = current_time - self.last_request_time
            
            if time_since_last < self.min_interval:
                sleep_time = self.min_interval - time_since_last
                time.sleep(sleep_time)
            
            self.last_request_time = time.time()


def load_api_key():
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key"""
    backend_dir = Path(__file__).parent.parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®")
    
    return api_key


def search_kol_with_retry(keyword, page, api_key, rate_limiter, max_retries=3):
    """
    è°ƒç”¨ TikHub API æœç´¢æ˜Ÿå›¾ KOLï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        page: é¡µç 
        api_key: APIå¯†é’¥
        rate_limiter: é€Ÿç‡é™åˆ¶å™¨
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        APIå“åº”æ•°æ®ï¼Œå¤±è´¥è¿”å›None
    """
    # ä½¿ç”¨å¤§é™†å¯è®¿é—®åŸŸåï¼Œé¿å…é˜²ç«å¢™æ‹¦æˆª
    url = "https://api.tikhub.dev/api/v1/douyin/xingtu/search_kol_v1"
    
    params = {
        "keyword": keyword,
        "page": str(page),
        "count": "20",
        "sort_type": "1",
        "platformSource": "_1"
    }
    
    headers = {
        "Authorization": f"Bearer {api_key}"
    }
    
    for attempt in range(max_retries):
        try:
            # ç­‰å¾…é€Ÿç‡é™åˆ¶
            rate_limiter.wait_if_needed()
            
            # å‘èµ·è¯·æ±‚
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # æ£€æŸ¥å“åº”æ˜¯å¦æˆåŠŸ
            if data.get('code') == 200:
                return data
            else:
                print(f"   âš ï¸ APIè¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")
                if attempt < max_retries - 1:
                    print(f"   â³ 1ç§’åé‡è¯• (å°è¯• {attempt + 2}/{max_retries})...")
                    time.sleep(1)
                    continue
                else:
                    return None
        
        except requests.exceptions.RequestException as e:
            print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            if attempt < max_retries - 1:
                print(f"   â³ 1ç§’åé‡è¯• (å°è¯• {attempt + 2}/{max_retries})...")
                time.sleep(1)
                continue
            else:
                return None
    
    return None


def get_existing_pages(detail_dir):
    """è·å–å·²å­˜åœ¨çš„é¡µç åˆ—è¡¨"""
    existing_pages = []
    for file in Path(detail_dir).glob('raw_page_*.json'):
        try:
            page_num = int(file.stem.split('_')[2])
            existing_pages.append(page_num)
        except:
            pass
    return sorted(existing_pages)


def fetch_page_range(keyword, start_page, end_page, output_dir, api_key):
    """
    è·å–æŒ‡å®šèŒƒå›´çš„é¡µé¢æ•°æ®
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        start_page: èµ·å§‹é¡µç 
        end_page: ç»“æŸé¡µç ï¼ˆåŒ…å«ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        api_key: APIå¯†é’¥
    
    Returns:
        æˆåŠŸé¡µæ•°, å¤±è´¥é¡µæ•°, è·³è¿‡é¡µæ•°
    """
    detail_dir = output_dir / 'detail'
    detail_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥å·²æœ‰é¡µé¢
    existing_pages = get_existing_pages(detail_dir)
    
    print(f"\n{'='*70}")
    print(f"ğŸ“‹ æ‰¹é‡è·å–ä»»åŠ¡")
    print(f"{'='*70}")
    print(f"å…³é”®è¯: '{keyword}'")
    print(f"ç›®æ ‡èŒƒå›´: ç¬¬ {start_page} - {end_page} é¡µ")
    print(f"å·²æœ‰é¡µé¢: {len(existing_pages)} é¡µ")
    print(f"{'='*70}\n")
    
    # åˆ›å»ºé€Ÿç‡é™åˆ¶å™¨ï¼ˆæ¯ç§’æœ€å¤š5ä¸ªè¯·æ±‚ï¼‰
    rate_limiter = RateLimiter(max_per_second=5)
    
    success_count = 0
    failure_count = 0
    skip_count = 0
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    total_pages = end_page - start_page + 1
    
    for page in range(start_page, end_page + 1):
        current = page - start_page + 1
        progress = f"[{current}/{total_pages}]"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if page in existing_pages:
            print(f"â­ï¸  {progress} ç¬¬ {page} é¡µå·²å­˜åœ¨ï¼Œè·³è¿‡")
            skip_count += 1
            continue
        
        print(f"ğŸ“„ {progress} æ­£åœ¨è·å–ç¬¬ {page} é¡µ...", end='', flush=True)
        
        # è°ƒç”¨APIï¼ˆå¸¦é‡è¯•ï¼‰
        data = search_kol_with_retry(keyword, page, api_key, rate_limiter, max_retries=3)
        
        if data and data.get('code') == 200:
            # ä¿å­˜æ•°æ®
            filename = f"raw_page_{page}_{timestamp}.json"
            filepath = detail_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            authors = data.get('data', {}).get('authors', [])
            has_more = data.get('data', {}).get('pagination', {}).get('has_more', False)
            
            print(f" âœ… æˆåŠŸ (è¾¾äººæ•°: {len(authors)}, has_more: {has_more})")
            success_count += 1
            
            # å¦‚æœæ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œæå‰ç»“æŸ
            if not has_more:
                print(f"\nâš ï¸ ç¬¬ {page} é¡µæ˜¾ç¤º has_more=falseï¼Œå¯èƒ½å·²åˆ°è¾¾æ•°æ®æœ«å°¾")
                break
        else:
            print(f" âŒ å¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰")
            failure_count += 1
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š æ‰¹é‡è·å–å®Œæˆ")
    print(f"{'='*70}")
    print(f"âœ… æˆåŠŸ: {success_count} é¡µ")
    print(f"âŒ å¤±è´¥: {failure_count} é¡µ")
    print(f"â­ï¸  è·³è¿‡: {skip_count} é¡µï¼ˆå·²å­˜åœ¨ï¼‰")
    print(f"{'='*70}\n")
    
    return success_count, failure_count, skip_count


def verify_no_duplicates(detail_dir):
    """éªŒè¯æ˜¯å¦æœ‰é‡å¤çš„è¾¾äººæ•°æ®"""
    print("\nğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    
    all_author_ids = set()
    duplicate_ids = set()
    page_author_count = {}
    
    files = sorted(Path(detail_dir).glob('raw_page_*.json'))
    
    for file in files:
        try:
            page_num = int(file.stem.split('_')[2])
            
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                authors = data.get('data', {}).get('authors', [])
                
                page_ids = []
                for author in authors:
                    author_id = author.get('attribute_datas', {}).get('id')
                    if author_id:
                        page_ids.append(author_id)
                        if author_id in all_author_ids:
                            duplicate_ids.add(author_id)
                        all_author_ids.add(author_id)
                
                page_author_count[page_num] = len(page_ids)
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file}: {e}")
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»é¡µæ•°: {len(files)}")
    print(f"   æ€»è¾¾äººæ•°ï¼ˆåŸå§‹ï¼‰: {sum(page_author_count.values())}")
    print(f"   æ€»è¾¾äººæ•°ï¼ˆå»é‡ï¼‰: {len(all_author_ids)}")
    print(f"   é‡å¤è¾¾äººæ•°: {len(duplicate_ids)}")
    
    if duplicate_ids:
        print(f"\nâš ï¸ å‘ç° {len(duplicate_ids)} ä¸ªé‡å¤è¾¾äººID")
        print(f"   é‡å¤IDç¤ºä¾‹: {list(duplicate_ids)[:5]}")
    else:
        print(f"\nâœ… æ— é‡å¤æ•°æ®ï¼Œæ•°æ®è´¨é‡è‰¯å¥½")
    
    # æ˜¾ç¤ºé¡µé¢èŒƒå›´
    if page_author_count:
        min_page = min(page_author_count.keys())
        max_page = max(page_author_count.keys())
        print(f"\nğŸ“„ é¡µé¢èŒƒå›´: ç¬¬ {min_page} - {max_page} é¡µ")
    
    return len(all_author_ids), len(duplicate_ids)


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("æ‰¹é‡è·å–'æŠ¤è‚¤ä¿å…»'å…³é”®è¯æ•°æ®")
    print("="*70 + "\n")
    
    try:
        # 1. åŠ è½½ API Key
        api_key = load_api_key()
        
        # 2. è®¾ç½®ç›®å½•
        script_dir = Path(__file__).parent
        output_dir = script_dir.parent / 'output' / 'keyword_æŠ¤è‚¤ä¿å…»'
        
        # 3. è·å–ç”¨æˆ·è¾“å…¥ï¼ˆæˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        # è¿™é‡Œè®¾ç½®è·å– 79-150 é¡µï¼ˆå‰©ä½™æ‰€æœ‰æ•°æ®ï¼‰
        start_page = 79
        end_page = 150
        
        print(f"ğŸ“‹ ä»»åŠ¡é…ç½®:")
        print(f"   å…³é”®è¯: æŠ¤è‚¤ä¿å…»")
        print(f"   èµ·å§‹é¡µ: {start_page}")
        print(f"   ç»“æŸé¡µ: {end_page}")
        print(f"   æ€»é¡µæ•°: {end_page - start_page + 1}")
        print(f"   é€Ÿç‡é™åˆ¶: æ¯ç§’æœ€å¤š5ä¸ªè¯·æ±‚")
        print(f"   å¤±è´¥é‡è¯•: æœ€å¤š3æ¬¡ï¼Œé—´éš”1ç§’\n")
        
        # 4. æ‰§è¡Œæ‰¹é‡è·å–
        success, failure, skip = fetch_page_range(
            keyword='æŠ¤è‚¤ä¿å…»',
            start_page=start_page,
            end_page=end_page,
            output_dir=output_dir,
            api_key=api_key
        )
        
        # 5. éªŒè¯æ•°æ®å®Œæ•´æ€§
        detail_dir = output_dir / 'detail'
        total_unique, duplicates = verify_no_duplicates(detail_dir)
        
        print(f"\n{'='*70}")
        print(f"âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        print(f"{'='*70}\n")
        
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ä»»åŠ¡")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

