#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ˜Ÿå›¾KOLæ•°æ®å¯¼å…¥Supabaseè„šæœ¬
ä»æ–°æ¥å£ search_kol_v1 çš„è¿”å›æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python import_to_supabase.py

ä¾èµ–:
    pip install python-dotenv supabase
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv
from supabase import create_client, Client

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(Path(__file__).parent.parent.parent.parent.parent / '.env')

# Supabaseé…ç½®
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    print("é”™è¯¯: æœªæ‰¾åˆ°SUPABASE_URLæˆ–SUPABASE_KEYç¯å¢ƒå˜é‡")
    sys.exit(1)

# åˆ›å»ºSupabaseå®¢æˆ·ç«¯
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


def safe_int(value: Any, default: int = 0) -> int:
    """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
    if value is None or value == '':
        return default
    try:
        return int(float(str(value)))
    except (ValueError, TypeError):
        return default


def safe_float(value: Any, default: float = 0.0) -> float:
    """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
    if value is None or value == '':
        return default
    try:
        return float(value)
    except (ValueError, TypeError):
        return default


def safe_bool(value: Any, default: bool = False) -> bool:
    """å®‰å…¨è½¬æ¢ä¸ºå¸ƒå°”å€¼"""
    if value is None or value == '':
        return default
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        return value.lower() in ('true', '1', 'yes')
    return bool(value)


def parse_json_field(value: Any) -> Optional[Dict]:
    """è§£æJSONå­—ç¬¦ä¸²å­—æ®µ"""
    if value is None or value == '':
        return None
    if isinstance(value, dict):
        return value
    if isinstance(value, str):
        try:
            return json.loads(value)
        except json.JSONDecodeError:
            return None
    return None


def timestamp_to_datetime(ts: Any) -> Optional[datetime]:
    """æ—¶é—´æˆ³è½¬datetime"""
    if ts is None or ts == '':
        return None
    try:
        ts_int = int(float(str(ts)))
        return datetime.fromtimestamp(ts_int)
    except (ValueError, TypeError, OSError):
        return None


def parse_kol_base_info(author: Dict, fetch_date: datetime) -> Dict:
    """
    è§£æKOLåŸºç¡€ä¿¡æ¯
    ä»authorçš„attribute_datasä¸­æå–å­—æ®µ
    """
    attr = author.get('attribute_datas', {})
    
    # è§£æå†…å®¹æ ‡ç­¾
    tags_relation = parse_json_field(attr.get('tags_relation'))
    content_theme_labels_str = attr.get('content_theme_labels_180d', '[]')
    content_theme_labels = parse_json_field(content_theme_labels_str)
    
    # è§£ælast_10_itemsç»Ÿè®¡å¹³å‡å€¼
    last_10_items_str = attr.get('last_10_items', '[]')
    last_10_items = parse_json_field(last_10_items_str) or []
    
    recent_works_count = len(last_10_items)
    avg_play = 0
    avg_like = 0
    avg_comment = 0
    avg_share = 0
    
    if last_10_items:
        total_vv = sum(safe_int(item.get('vv')) for item in last_10_items)
        total_like = sum(safe_int(item.get('like_cnt')) for item in last_10_items)
        total_comment = sum(safe_int(item.get('comment_cnt')) for item in last_10_items)
        total_share = sum(safe_int(item.get('share_cnt')) for item in last_10_items)
        
        avg_play = total_vv // recent_works_count if recent_works_count > 0 else 0
        avg_like = total_like // recent_works_count if recent_works_count > 0 else 0
        avg_comment = total_comment // recent_works_count if recent_works_count > 0 else 0
        avg_share = total_share // recent_works_count if recent_works_count > 0 else 0
    
    return {
        'kol_id': attr.get('id') or author.get('star_id'),  # ä½¿ç”¨idæˆ–star_id
        'kol_name': attr.get('nick_name'),
        'kol_avatar': attr.get('avatar_uri'),
        'fans_count': safe_int(attr.get('follower')),
        'aweme_count': None,  # æ–°æ¥å£æ²¡æœ‰ç›´æ¥æä¾›
        'vertical_category': None,  # æ–°æ¥å£æ²¡æœ‰ç›´æ¥æä¾›
        'tags': None,  # æš‚ä¸å¤„ç†tagsæ•°ç»„
        
        # äº’åŠ¨æ•°æ®
        'interact_rate_30d': safe_float(attr.get('interact_rate_within_30d')),
        'interaction_median_30d': safe_int(attr.get('interaction_median_30d')),
        'vv_median_30d': safe_int(attr.get('vv_median_30d')),
        'play_over_rate_30d': safe_float(attr.get('play_over_rate_within_30d')),
        
        # ç²‰ä¸å¢é•¿
        'fans_increment_15d': safe_int(attr.get('fans_increment_within_15d')),
        'fans_increment_30d': safe_float(attr.get('fans_increment_within_30d')),
        'fans_increment_rate_15d': safe_float(attr.get('fans_increment_rate_within_15d')),
        
        # é¢„ä¼°æ•°æ®
        'expected_play_num': safe_int(attr.get('expected_play_num')),
        'expected_natural_play_num': safe_int(attr.get('expected_natural_play_num')),
        
        # æ˜Ÿå›¾è¯„åˆ†
        'star_index': safe_float(attr.get('star_index')),
        'link_shopping_index': safe_float(attr.get('link_shopping_index') or attr.get('link_recommend_index_by_industry')),
        'link_convert_index': safe_float(attr.get('link_convert_index')),
        'link_spread_index': safe_float(attr.get('link_spread_index')),
        'link_star_index': safe_float(attr.get('link_star_index')),
        
        # ç‰¹æ®Šæ ‡è®°
        'is_black_horse': safe_bool(attr.get('is_black_horse_author')),
        'is_excellent': safe_bool(attr.get('is_excellenct_author')),
        'is_cocreate': safe_bool(attr.get('is_cocreate_author')),
        'is_short_drama': safe_bool(attr.get('is_short_drama')),
        
        # ç”µå•†èƒ½åŠ›åŸºç¡€
        'ecom_level': attr.get('author_ecom_level'),
        'ecom_enabled': safe_bool(attr.get('e_commerce_enable')),
        
        # å†…å®¹æ ‡ç­¾
        'content_tags': tags_relation,
        'content_theme_labels': content_theme_labels,
        
        # åœ°ç†ä½ç½®
        'province': attr.get('province') or None,
        'city': attr.get('city') or None,
        'gender': safe_int(attr.get('gender')) if attr.get('gender') else None,
        
        # è¾¾äººç±»å‹
        'author_type': safe_int(attr.get('author_type')) if attr.get('author_type') else None,
        'account_status': safe_int(attr.get('author_status')) if attr.get('author_status') else None,
        
        # è¿‘æœŸä½œå“ç»Ÿè®¡
        'recent_works_count': recent_works_count,
        'avg_play_count': avg_play,
        'avg_like_count': avg_like,
        'avg_comment_count': avg_comment,
        'avg_share_count': avg_share,
        
        # å…ƒæ•°æ®
        'raw_data': author,  # ä¿å­˜å®Œæ•´åŸå§‹æ•°æ®
        'fetch_date': fetch_date.isoformat(),  # è½¬æ¢ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
    }


def parse_kol_price(author: Dict, fetch_date: datetime) -> Dict:
    """
    è§£æKOLæŠ¥ä»·ä¿¡æ¯
    ä»attribute_dataså’Œtask_infosä¸­æå–
    """
    attr = author.get('attribute_datas', {})
    task_infos = author.get('task_infos', [])
    
    # åŸºç¡€æŠ¥ä»·å­—æ®µ (ä»attribute_datas)
    price_data = {
        'kol_id': attr.get('id') or author.get('star_id'),
        'video_1_20s_price': safe_int(attr.get('price_1_20')),
        'video_21_60s_price': safe_int(attr.get('price_20_60')),
        'video_60s_plus_price': safe_int(attr.get('price_60')),
        
        # é¢„ä¼°CPM
        'prospective_cpm_1_20s': safe_float(attr.get('prospective_1_20_cpm')),
        'prospective_cpm_20_60s': safe_float(attr.get('prospective_20_60_cpm')),
        'prospective_cpm_60s_plus': safe_float(attr.get('prospective_60_cpm')),
        
        # é¢„ä¼°CPE
        'prospective_cpe_1_20s': safe_float(attr.get('sn_prospective_1_20_cpe')),
        'prospective_cpe_20_60s': safe_float(attr.get('sn_prospective_20_60_cpe')),
        'prospective_cpe_60s_plus': safe_float(attr.get('sn_prospective_60_cpe')),
        
        'fetch_date': fetch_date.isoformat(),  # è½¬æ¢ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
        'raw_data': {'attribute_datas': attr, 'task_infos': task_infos},
    }
    
    # ä»task_infosä¸­æå–ä»·æ ¼å†å²å’ŒèŒƒå›´ä¿¡æ¯
    # æ‰¾20-60ç§’è§†é¢‘çš„price_extra_info
    for task_info in task_infos:
        price_infos = task_info.get('price_infos', [])
        for price_info in price_infos:
            video_type = price_info.get('video_type')
            if video_type == 2:  # 20-60ç§’
                extra_info = price_info.get('price_extra_info', {})
                price_data['price_last_month_20_60s'] = safe_int(extra_info.get('price_last_month'))
                price_data['price_discount_range_20_60s'] = safe_int(extra_info.get('price_discount_range'))
                price_data['price_margin_last_20_60s'] = safe_int(extra_info.get('price_margin_last'))
                # è½¬æ¢æ—¶é—´ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²æˆ–None
                start_time = timestamp_to_datetime(price_info.get('start_time'))
                end_time = timestamp_to_datetime(price_info.get('end_time'))
                price_data['price_start_time'] = start_time.isoformat() if start_time else None
                price_data['price_end_time'] = end_time.isoformat() if end_time else None
            elif video_type == 92:  # CPMæ¨¡å¼æœ‰ceilingå’Œfloor
                extra_info = price_info.get('price_extra_info', {})
                price_data['ceiling_price'] = safe_int(extra_info.get('ceiling_price'))
                price_data['floor_price'] = safe_int(extra_info.get('floor_price'))
    
    return price_data


def parse_kol_ecommerce(author: Dict, fetch_date: datetime) -> Dict:
    """è§£æKOLç”µå•†æ•°æ®"""
    attr = author.get('attribute_datas', {})
    
    return {
        'kol_id': attr.get('id') or author.get('star_id'),
        'ecom_level': attr.get('author_ecom_level'),
        'ecom_enabled': safe_bool(attr.get('e_commerce_enable')),
        'gmv_30d_range': attr.get('ecom_gmv_30d_range') or None,
        'gpm_30d_range': attr.get('ecom_gpm_30d_range') or None,
        'avg_order_value_30d_range': attr.get('ecom_avg_order_value_30d_range') or None,
        'ecom_video_num_30d': safe_int(attr.get('ecom_video_product_num_30d')),
        'star_ecom_video_num_30d': safe_int(attr.get('star_ecom_video_num_30d')),
        'ecom_video_product_num_30d': safe_int(attr.get('ecom_video_product_num_30d')),
        'raw_data': attr,
        'fetch_date': fetch_date.isoformat(),  # è½¬æ¢ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
    }


def parse_kol_videos(author: Dict, fetch_date: datetime) -> List[Dict]:
    """
    è§£æKOLè§†é¢‘æ•°æ®
    ä»itemså’Œlast_10_itemsä¸¤ä¸ªæ¥æº
    """
    attr = author.get('attribute_datas', {})
    kol_id = attr.get('id') or author.get('star_id')
    
    videos = []
    fetch_date_str = fetch_date.isoformat()  # è½¬æ¢ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
    
    # 1. è§£æitems (ä»£è¡¨ä½œå“)
    items = author.get('items', [])
    for item in items:
        videos.append({
            'kol_id': kol_id,
            'item_id': item.get('item_id'),
            'video_tag': safe_int(item.get('video_tag')),
            'vv': safe_int(item.get('vv')),
            'source': 'items',
            'fetch_date': fetch_date_str,
        })
    
    # 2. è§£ælast_10_items (æœ€è¿‘10ä¸ªä½œå“)
    last_10_items_str = attr.get('last_10_items', '[]')
    last_10_items = parse_json_field(last_10_items_str) or []
    
    for item in last_10_items:
        # è½¬æ¢æ—¶é—´ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²æˆ–None
        publish_time = timestamp_to_datetime(item.get('item_publish_time'))
        create_time = timestamp_to_datetime(item.get('item_create_time'))
        
        videos.append({
            'kol_id': kol_id,
            'item_id': item.get('item_id'),
            'video_tag': None,  # last_10_itemsä¸­æ²¡æœ‰video_tag
            'vv': safe_int(item.get('vv')),
            'comment_cnt': safe_int(item.get('comment_cnt')),
            'like_cnt': safe_int(item.get('like_cnt')),
            'share_cnt': safe_int(item.get('share_cnt')),
            'item_title': item.get('item_title'),
            'item_publish_time': publish_time.isoformat() if publish_time else None,
            'item_create_time': create_time.isoformat() if create_time else None,
            'is_high_quality': safe_bool(item.get('is_high_quality_item')),
            'source': 'last_10_items',
            'fetch_date': fetch_date_str,
        })
    
    return videos


def import_page_data(page_file: Path) -> Dict[str, int]:
    """
    å¯¼å…¥å•é¡µæ•°æ®åˆ°æ•°æ®åº“
    
    è¿”å›:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    print(f"\nğŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {page_file.name}")
    
    # è¯»å–JSONæ–‡ä»¶
    with open(page_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    authors = data.get('data', {}).get('authors', [])
    fetch_date = datetime.now()
    
    stats = {
        'total_kols': len(authors),
        'success_base': 0,
        'success_price': 0,
        'success_ecom': 0,
        'success_videos': 0,
        'error_base': 0,
        'error_price': 0,
        'error_ecom': 0,
        'error_videos': 0,
    }
    
    for idx, author in enumerate(authors, 1):
        try:
            kol_id = author.get('attribute_datas', {}).get('id') or author.get('star_id')
            print(f"  [{idx}/{len(authors)}] å¤„ç†KOL: {kol_id}")
            
            # 1. å¯¼å…¥åŸºç¡€ä¿¡æ¯
            try:
                base_info = parse_kol_base_info(author, fetch_date)
                result = supabase.table('gg_xingtu_kol_base_info').upsert(
                    base_info, 
                    on_conflict='kol_id'
                ).execute()
                stats['success_base'] += 1
                print(f"    âœ… åŸºç¡€ä¿¡æ¯å·²ä¿å­˜")
            except Exception as e:
                stats['error_base'] += 1
                print(f"    âŒ åŸºç¡€ä¿¡æ¯ä¿å­˜å¤±è´¥: {str(e)}")
            
            # 2. å¯¼å…¥æŠ¥ä»·ä¿¡æ¯
            try:
                price_info = parse_kol_price(author, fetch_date)
                result = supabase.table('gg_xingtu_kol_price').upsert(
                    price_info,
                    on_conflict='kol_id'
                ).execute()
                stats['success_price'] += 1
                print(f"    âœ… æŠ¥ä»·ä¿¡æ¯å·²ä¿å­˜")
            except Exception as e:
                stats['error_price'] += 1
                print(f"    âŒ æŠ¥ä»·ä¿¡æ¯ä¿å­˜å¤±è´¥: {str(e)}")
            
            # 3. å¯¼å…¥ç”µå•†ä¿¡æ¯
            try:
                ecom_info = parse_kol_ecommerce(author, fetch_date)
                result = supabase.table('gg_xingtu_kol_ecommerce').upsert(
                    ecom_info,
                    on_conflict='kol_id'
                ).execute()
                stats['success_ecom'] += 1
                print(f"    âœ… ç”µå•†ä¿¡æ¯å·²ä¿å­˜")
            except Exception as e:
                stats['error_ecom'] += 1
                print(f"    âŒ ç”µå•†ä¿¡æ¯ä¿å­˜å¤±è´¥: {str(e)}")
            
            # 4. å¯¼å…¥è§†é¢‘æ•°æ®
            try:
                videos = parse_kol_videos(author, fetch_date)
                if videos:
                    # æ‰¹é‡æ’å…¥ï¼Œå¦‚æœå­˜åœ¨åˆ™å¿½ç•¥
                    for video in videos:
                        try:
                            supabase.table('gg_xingtu_kol_videos').insert(video).execute()
                        except Exception:
                            # å¿½ç•¥é‡å¤é”®é”™è¯¯
                            pass
                    stats['success_videos'] += len(videos)
                    print(f"    âœ… è§†é¢‘æ•°æ®å·²ä¿å­˜ ({len(videos)}æ¡)")
            except Exception as e:
                stats['error_videos'] += 1
                print(f"    âŒ è§†é¢‘æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}")
            
        except Exception as e:
            print(f"    âŒ KOLæ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
    
    return stats


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ æ˜Ÿå›¾KOLæ•°æ®å¯¼å…¥å·¥å…·")
    print("=" * 80)
    
    # æ•°æ®ç›®å½•
    detail_dir = Path(__file__).parent.parent / 'output' / 'keyword_æŠ¤è‚¤ä¿å…»' / 'detail'
    
    # å¯¼å…¥å…¨éƒ¨104é¡µæ•°æ®
    page_files = sorted(detail_dir.glob('raw_page_*.json'))
    
    if not page_files:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(page_files)} ä¸ªé¡µé¢æ–‡ä»¶")
    
    total_stats = {
        'total_kols': 0,
        'success_base': 0,
        'success_price': 0,
        'success_ecom': 0,
        'success_videos': 0,
        'error_base': 0,
        'error_price': 0,
        'error_ecom': 0,
        'error_videos': 0,
    }
    
    for page_file in page_files:
        stats = import_page_data(page_file)
        for key in total_stats:
            total_stats[key] += stats[key]
    
    # æ‰“å°æ±‡æ€»ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“ˆ å¯¼å…¥ç»Ÿè®¡æ±‡æ€»")
    print("=" * 80)
    print(f"æ€»KOLæ•°: {total_stats['total_kols']}")
    print(f"\nåŸºç¡€ä¿¡æ¯:")
    print(f"  âœ… æˆåŠŸ: {total_stats['success_base']}")
    print(f"  âŒ å¤±è´¥: {total_stats['error_base']}")
    print(f"\næŠ¥ä»·ä¿¡æ¯:")
    print(f"  âœ… æˆåŠŸ: {total_stats['success_price']}")
    print(f"  âŒ å¤±è´¥: {total_stats['error_price']}")
    print(f"\nç”µå•†ä¿¡æ¯:")
    print(f"  âœ… æˆåŠŸ: {total_stats['success_ecom']}")
    print(f"  âŒ å¤±è´¥: {total_stats['error_ecom']}")
    print(f"\nè§†é¢‘æ•°æ®:")
    print(f"  âœ… æˆåŠŸ: {total_stats['success_videos']}æ¡")
    print(f"  âŒ å¤±è´¥: {total_stats['error_videos']}")
    print("=" * 80)


if __name__ == '__main__':
    main()

