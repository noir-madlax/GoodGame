#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç»§ç»­è·å–"æŠ¤è‚¤ä¿å…»"å…³é”®è¯æ•°æ®åˆ°28é¡µ

åŠŸèƒ½ï¼š
1. æ£€æŸ¥å·²æœ‰çš„é¡µé¢æ•°æ®
2. ç»§ç»­è·å–ç¼ºå¤±çš„é¡µé¢ç›´åˆ°28é¡µ
3. ä¸åŸå§‹"æŠ¤è‚¤"æ•°æ®è¿›è¡Œå…¨é¢å¯¹æ¯”åˆ†æ
4. æŸ¥æ‰¾ç›®æ ‡è¾¾äºº"æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ"
5. ç»Ÿè®¡å®Œå…¨æ— å…³çš„è¾¾äºº

ä½œè€…: AI Agent
åˆ›å»ºæ—¶é—´: 2025-11-18
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from collections import defaultdict, Counter


def load_api_key():
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key"""
    backend_dir = Path(__file__).parent.parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®")
    
    return api_key


def search_kol(keyword, page=1, count=20, sort_type=1, api_key=None):
    """è°ƒç”¨ TikHub API æœç´¢æ˜Ÿå›¾ KOL"""
    url = "https://api.tikhub.io/api/v1/douyin/xingtu/search_kol_v1"
    
    params = {
        "keyword": keyword,
        "page": str(page),
        "count": str(count),
        "sort_type": str(sort_type),
        "platformSource": "_1"
    }
    
    headers = {
        "Authorization": f"Bearer {api_key}"
    }
    
    try:
        response = requests.get(url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"âŒ API è¯·æ±‚å¤±è´¥: {e}")
        return None


def get_existing_pages(detail_dir):
    """è·å–å·²å­˜åœ¨çš„é¡µç åˆ—è¡¨"""
    existing_pages = []
    for file in Path(detail_dir).glob('raw_page_*.json'):
        try:
            page_num = int(file.stem.split('_')[2])
            existing_pages.append(page_num)
        except:
            pass
    return sorted(existing_pages)


def fetch_missing_pages(keyword, target_pages, output_dir, api_key):
    """è·å–ç¼ºå¤±çš„é¡µé¢æ•°æ®"""
    detail_dir = output_dir / 'detail'
    detail_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥å·²æœ‰é¡µé¢
    existing_pages = get_existing_pages(detail_dir)
    missing_pages = [p for p in range(1, target_pages + 1) if p not in existing_pages]
    
    print(f"\n{'='*60}")
    print(f"å…³é”®è¯: '{keyword}'")
    print(f"ç›®æ ‡é¡µæ•°: {target_pages}")
    print(f"å·²æœ‰é¡µé¢: {len(existing_pages)} é¡µ - {existing_pages}")
    print(f"ç¼ºå¤±é¡µé¢: {len(missing_pages)} é¡µ - {missing_pages}")
    print(f"{'='*60}\n")
    
    if not missing_pages:
        print("âœ… æ‰€æœ‰é¡µé¢å·²å­˜åœ¨ï¼Œæ— éœ€è·å–")
        return []
    
    all_new_authors = []
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    for page in missing_pages:
        print(f"ğŸ“„ æ­£åœ¨è·å–ç¬¬ {page}/{target_pages} é¡µ...")
        
        data = search_kol(keyword=keyword, page=page, api_key=api_key)
        
        if data and data.get('code') == 200:
            filename = f"raw_page_{page}_{timestamp}.json"
            filepath = detail_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            authors = data.get('data', {}).get('authors', [])
            all_new_authors.extend(authors)
            
            print(f"âœ… ç¬¬ {page} é¡µè·å–æˆåŠŸï¼Œè¾¾äººæ•°: {len(authors)}")
            print(f"   å·²ä¿å­˜åˆ°: {filepath}")
        else:
            print(f"âš ï¸ ç¬¬ {page} é¡µè·å–å¤±è´¥æˆ–æ— æ•°æ®")
            if data:
                print(f"   é”™è¯¯ä¿¡æ¯: {data.get('message', 'Unknown error')}")
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        if page != missing_pages[-1]:
            time.sleep(2)
    
    print(f"\nâœ… æ–°å¢æ•°æ®è·å–å®Œæˆï¼Œå…± {len(all_new_authors)} ä¸ªè¾¾äºº\n")
    return all_new_authors


def load_all_authors(detail_dir):
    """åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰è¾¾äººæ•°æ®"""
    authors = []
    author_ids = set()
    
    for file in sorted(Path(detail_dir).glob('raw_page_*.json')):
        try:
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                page_authors = data.get('data', {}).get('authors', [])
                
                for author in page_authors:
                    author_id = author.get('attribute_datas', {}).get('id')
                    if author_id and author_id not in author_ids:
                        authors.append(author)
                        author_ids.add(author_id)
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file}: {e}")
    
    return authors


def find_target_author(authors, target_name):
    """æŸ¥æ‰¾ç›®æ ‡è¾¾äºº"""
    for author in authors:
        nick_name = author.get('attribute_datas', {}).get('nick_name', '')
        if target_name in nick_name:
            return author
    return None


def analyze_tags(authors):
    """åˆ†ææ ‡ç­¾åˆ†å¸ƒ"""
    tag_counts = Counter()
    category_tags = defaultdict(Counter)
    author_tags = {}
    
    skincare_keywords = ['æŠ¤è‚¤', 'ç¾å¦†', 'ä¿å…»', 'çš®è‚¤', 'é¢éƒ¨', 'åŒ–å¦†', 'ç¾å®¹']
    
    for author in authors:
        author_id = author.get('attribute_datas', {}).get('id', 'unknown')
        tags_relation_str = author.get('attribute_datas', {}).get('tags_relation', '{}')
        
        try:
            tags_relation = json.loads(tags_relation_str)
            author_tag_list = []
            
            for category, tags in tags_relation.items():
                if isinstance(tags, list):
                    for tag in tags:
                        tag_counts[tag] += 1
                        category_tags[category][tag] += 1
                        author_tag_list.append(tag)
                else:
                    tag_counts[tags] += 1
                    category_tags[category][tags] += 1
                    author_tag_list.append(tags)
            
            author_tags[author_id] = {
                'nick_name': author.get('attribute_datas', {}).get('nick_name', ''),
                'tags': author_tag_list,
                'has_skincare': any(any(kw in tag for kw in skincare_keywords) for tag in author_tag_list)
            }
        except:
            author_tags[author_id] = {
                'nick_name': author.get('attribute_datas', {}).get('nick_name', ''),
                'tags': [],
                'has_skincare': False
            }
    
    return tag_counts, category_tags, author_tags


def generate_comparison_report(original_authors, maintenance_authors, 
                               target_author_found, output_dir):
    """ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = output_dir / f'æŠ¤è‚¤vsæŠ¤è‚¤ä¿å…»_è¯¦ç»†å¯¹æ¯”_{timestamp}.md'
    
    # åˆ†ææ•°æ®
    orig_ids = {a.get('attribute_datas', {}).get('id'): a for a in original_authors}
    maint_ids = {a.get('attribute_datas', {}).get('id'): a for a in maintenance_authors}
    
    overlap_ids = set(orig_ids.keys()) & set(maint_ids.keys())
    only_orig_ids = set(orig_ids.keys()) - set(maint_ids.keys())
    only_maint_ids = set(maint_ids.keys()) - set(orig_ids.keys())
    
    # æ ‡ç­¾åˆ†æ
    orig_tag_counts, orig_category_tags, orig_author_tags = analyze_tags(original_authors)
    maint_tag_counts, maint_category_tags, maint_author_tags = analyze_tags(maintenance_authors)
    
    # ç»Ÿè®¡æ— å…³è¾¾äºº
    orig_unrelated = [aid for aid, info in orig_author_tags.items() if not info['has_skincare']]
    maint_unrelated = [aid for aid, info in maint_author_tags.items() if not info['has_skincare']]
    
    skincare_keywords = ['æŠ¤è‚¤', 'ç¾å¦†', 'ä¿å…»', 'çš®è‚¤', 'é¢éƒ¨', 'åŒ–å¦†', 'ç¾å®¹']
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# æŠ¤è‚¤ vs æŠ¤è‚¤ä¿å…» - 28é¡µæ•°æ®è¯¦ç»†å¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ•°æ®èŒƒå›´**: 28é¡µæ•°æ®\n\n")
        f.write("="*80 + "\n\n")
        
        # ä¸€ã€æ•°æ®æ¦‚è§ˆ
        f.write("## ä¸€ã€æ•°æ®æ¦‚è§ˆ\n\n")
        f.write("### 1.1 åŸºç¡€ç»Ÿè®¡\n\n")
        f.write("| æ•°æ®é›† | æ€»è¾¾äººæ•° | å»é‡å | æœ‰æ ‡ç­¾ | æ ‡ç­¾ç§ç±» |\n")
        f.write("|--------|---------|--------|--------|----------|\n")
        f.write(f"| æŠ¤è‚¤ | {len(original_authors)} | {len(orig_ids)} | {len(orig_author_tags)} | {len(orig_tag_counts)} |\n")
        f.write(f"| æŠ¤è‚¤ä¿å…» | {len(maintenance_authors)} | {len(maint_ids)} | {len(maint_author_tags)} | {len(maint_tag_counts)} |\n\n")
        
        # äºŒã€é‡åˆåº¦åˆ†æ
        f.write("## äºŒã€é‡åˆåº¦åˆ†æ\n\n")
        f.write("### 2.1 è¾¾äººé‡åˆæƒ…å†µ\n\n")
        f.write("| æŒ‡æ ‡ | æ•°å€¼ |\n")
        f.write("|------|------|\n")
        f.write(f"| æŠ¤è‚¤æ€»è¾¾äººæ•° | {len(orig_ids)} |\n")
        f.write(f"| æŠ¤è‚¤ä¿å…»æ€»è¾¾äººæ•° | {len(maint_ids)} |\n")
        f.write(f"| é‡åˆè¾¾äººæ•° | {len(overlap_ids)} |\n")
        f.write(f"| é‡åˆç‡ï¼ˆç›¸å¯¹æŠ¤è‚¤ï¼‰ | {len(overlap_ids)/len(orig_ids)*100:.2f}% |\n")
        f.write(f"| é‡åˆç‡ï¼ˆç›¸å¯¹æŠ¤è‚¤ä¿å…»ï¼‰ | {len(overlap_ids)/len(maint_ids)*100:.2f}% |\n")
        f.write(f"| ä»…åœ¨æŠ¤è‚¤ | {len(only_orig_ids)} |\n")
        f.write(f"| ä»…åœ¨æŠ¤è‚¤ä¿å…» | {len(only_maint_ids)} |\n\n")
        
        # ä¸‰ã€ç›®æ ‡è¾¾äººæŸ¥æ‰¾
        f.write("## ä¸‰ã€ç›®æ ‡è¾¾äººæŸ¥æ‰¾\n\n")
        f.write("### 3.1 æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ\n\n")
        
        if target_author_found['in_original']:
            author = target_author_found['original_data']
            f.write("**åœ¨'æŠ¤è‚¤'æœç´¢ä¸­**: âœ… æ‰¾åˆ°\n\n")
            f.write(f"- è¾¾äººID: {author.get('attribute_datas', {}).get('id')}\n")
            f.write(f"- æ˜µç§°: {author.get('attribute_datas', {}).get('nick_name')}\n")
            f.write(f"- ç²‰ä¸æ•°: {author.get('attribute_datas', {}).get('follower')}\n")
            tags_str = author.get('attribute_datas', {}).get('tags_relation', '{}')
            f.write(f"- æ ‡ç­¾: {tags_str}\n\n")
        else:
            f.write("**åœ¨'æŠ¤è‚¤'æœç´¢ä¸­**: âŒ æœªæ‰¾åˆ°\n\n")
        
        if target_author_found['in_maintenance']:
            author = target_author_found['maintenance_data']
            f.write("**åœ¨'æŠ¤è‚¤ä¿å…»'æœç´¢ä¸­**: âœ… æ‰¾åˆ°\n\n")
            f.write(f"- è¾¾äººID: {author.get('attribute_datas', {}).get('id')}\n")
            f.write(f"- æ˜µç§°: {author.get('attribute_datas', {}).get('nick_name')}\n")
            f.write(f"- ç²‰ä¸æ•°: {author.get('attribute_datas', {}).get('follower')}\n")
            tags_str = author.get('attribute_datas', {}).get('tags_relation', '{}')
            f.write(f"- æ ‡ç­¾: {tags_str}\n\n")
        else:
            f.write("**åœ¨'æŠ¤è‚¤ä¿å…»'æœç´¢ä¸­**: âŒ æœªæ‰¾åˆ°\n\n")
        
        # å››ã€æ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”
        f.write("## å››ã€æ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”\n\n")
        
        f.write("### 4.1 'æŠ¤è‚¤' æ ‡ç­¾åˆ†å¸ƒï¼ˆå‰30åï¼‰\n\n")
        f.write("| æ’å | æ ‡ç­¾ | è¾¾äººæ•° | å æ¯” |\n")
        f.write("|------|------|--------|------|\n")
        for idx, (tag, count) in enumerate(orig_tag_counts.most_common(30), 1):
            rate = count / len(orig_ids) * 100
            skincare_mark = "âœ…" if any(kw in tag for kw in skincare_keywords) else ""
            f.write(f"| {idx} | {tag} {skincare_mark} | {count} | {rate:.2f}% |\n")
        f.write("\n")
        
        f.write("### 4.2 'æŠ¤è‚¤ä¿å…»' æ ‡ç­¾åˆ†å¸ƒï¼ˆå‰30åï¼‰\n\n")
        f.write("| æ’å | æ ‡ç­¾ | è¾¾äººæ•° | å æ¯” |\n")
        f.write("|------|------|--------|------|\n")
        for idx, (tag, count) in enumerate(maint_tag_counts.most_common(30), 1):
            rate = count / len(maint_ids) * 100
            skincare_mark = "âœ…" if any(kw in tag for kw in skincare_keywords) else ""
            f.write(f"| {idx} | {tag} {skincare_mark} | {count} | {rate:.2f}% |\n")
        f.write("\n")
        
        # äº”ã€æŠ¤è‚¤ç›¸å…³åº¦ç»Ÿè®¡
        f.write("## äº”ã€æŠ¤è‚¤ç›¸å…³åº¦ç»Ÿè®¡\n\n")
        
        orig_skincare_tags = {tag: count for tag, count in orig_tag_counts.items() 
                             if any(kw in tag for kw in skincare_keywords)}
        maint_skincare_tags = {tag: count for tag, count in maint_tag_counts.items() 
                              if any(kw in tag for kw in skincare_keywords)}
        
        f.write("### 5.1 'æŠ¤è‚¤' æŠ¤è‚¤ç›¸å…³æ ‡ç­¾\n\n")
        f.write("| æ ‡ç­¾ | è¾¾äººæ•° | å æ¯” |\n")
        f.write("|------|--------|------|\n")
        for tag, count in sorted(orig_skincare_tags.items(), key=lambda x: x[1], reverse=True):
            rate = count / len(orig_ids) * 100
            f.write(f"| {tag} | {count} | {rate:.2f}% |\n")
        f.write(f"| **åˆè®¡** | **{sum(orig_skincare_tags.values())}** | **{sum(orig_skincare_tags.values())/len(orig_ids)*100:.2f}%** |\n\n")
        
        f.write("### 5.2 'æŠ¤è‚¤ä¿å…»' æŠ¤è‚¤ç›¸å…³æ ‡ç­¾\n\n")
        f.write("| æ ‡ç­¾ | è¾¾äººæ•° | å æ¯” |\n")
        f.write("|------|--------|------|\n")
        for tag, count in sorted(maint_skincare_tags.items(), key=lambda x: x[1], reverse=True):
            rate = count / len(maint_ids) * 100
            f.write(f"| {tag} | {count} | {rate:.2f}% |\n")
        f.write(f"| **åˆè®¡** | **{sum(maint_skincare_tags.values())}** | **{sum(maint_skincare_tags.values())/len(maint_ids)*100:.2f}%** |\n\n")
        
        # å…­ã€å®Œå…¨æ— å…³è¾¾äººç»Ÿè®¡
        f.write("## å…­ã€å®Œå…¨æ— å…³è¾¾äººç»Ÿè®¡\n\n")
        f.write(f"**å®šä¹‰**: æ ‡ç­¾ä¸­ä¸åŒ…å«ä»»ä½•æŠ¤è‚¤ç›¸å…³å…³é”®è¯ï¼ˆ{', '.join(skincare_keywords)}ï¼‰çš„è¾¾äºº\n\n")
        
        f.write("### 6.1 'æŠ¤è‚¤' ä¸­çš„æ— å…³è¾¾äºº\n\n")
        f.write(f"- æ€»æ•°: {len(orig_unrelated)} äºº\n")
        f.write(f"- å æ¯”: {len(orig_unrelated)/len(orig_ids)*100:.2f}%\n\n")
        
        if len(orig_unrelated) > 0:
            f.write("å‰20ä¸ªæ— å…³è¾¾äººç¤ºä¾‹:\n\n")
            f.write("| æ˜µç§° | æ ‡ç­¾ |\n")
            f.write("|------|------|\n")
            for aid in list(orig_unrelated)[:20]:
                info = orig_author_tags[aid]
                tags_str = ', '.join(info['tags']) if info['tags'] else 'æ— æ ‡ç­¾'
                f.write(f"| {info['nick_name']} | {tags_str} |\n")
            f.write("\n")
        
        f.write("### 6.2 'æŠ¤è‚¤ä¿å…»' ä¸­çš„æ— å…³è¾¾äºº\n\n")
        f.write(f"- æ€»æ•°: {len(maint_unrelated)} äºº\n")
        f.write(f"- å æ¯”: {len(maint_unrelated)/len(maint_ids)*100:.2f}%\n\n")
        
        if len(maint_unrelated) > 0:
            f.write("å‰20ä¸ªæ— å…³è¾¾äººç¤ºä¾‹:\n\n")
            f.write("| æ˜µç§° | æ ‡ç­¾ |\n")
            f.write("|------|------|\n")
            for aid in list(maint_unrelated)[:20]:
                info = maint_author_tags[aid]
                tags_str = ', '.join(info['tags']) if info['tags'] else 'æ— æ ‡ç­¾'
                f.write(f"| {info['nick_name']} | {tags_str} |\n")
            f.write("\n")
        
        # ä¸ƒã€ç»“è®ºä¸å»ºè®®
        f.write("## ä¸ƒã€ç»“è®ºä¸å»ºè®®\n\n")
        
        overlap_rate = len(overlap_ids) / len(orig_ids) * 100
        orig_skincare_rate = sum(orig_skincare_tags.values()) / len(orig_ids) * 100
        maint_skincare_rate = sum(maint_skincare_tags.values()) / len(maint_ids) * 100
        
        f.write(f"1. **æ•°æ®é‡åˆåº¦**: {overlap_rate:.2f}%ï¼Œè¯´æ˜")
        if overlap_rate > 80:
            f.write("'æŠ¤è‚¤ä¿å…»'åŸºæœ¬æ˜¯'æŠ¤è‚¤'çš„å­é›†\n")
        elif overlap_rate > 50:
            f.write("ä¸¤è€…æœ‰è¾ƒé«˜é‡åˆåº¦ï¼Œä½†'æŠ¤è‚¤ä¿å…»'æ›´ç²¾å‡†\n")
        else:
            f.write("ä¸¤è€…å·®å¼‚è¾ƒå¤§ï¼Œå¯äº’è¡¥ä½¿ç”¨\n")
        
        f.write(f"\n2. **æŠ¤è‚¤ç›¸å…³åº¦å¯¹æ¯”**:\n")
        f.write(f"   - æŠ¤è‚¤: {orig_skincare_rate:.2f}%\n")
        f.write(f"   - æŠ¤è‚¤ä¿å…»: {maint_skincare_rate:.2f}%\n")
        f.write(f"   - {'âœ… æŠ¤è‚¤ä¿å…»' if maint_skincare_rate > orig_skincare_rate else 'âš ï¸ æŠ¤è‚¤'} ç›¸å…³åº¦æ›´é«˜\n")
        
        f.write(f"\n3. **æ— å…³è¾¾äººå æ¯”**:\n")
        f.write(f"   - æŠ¤è‚¤: {len(orig_unrelated)/len(orig_ids)*100:.2f}%\n")
        f.write(f"   - æŠ¤è‚¤ä¿å…»: {len(maint_unrelated)/len(maint_ids)*100:.2f}%\n")
        
        f.write(f"\n4. **ç›®æ ‡è¾¾äººæŸ¥æ‰¾**: ")
        if target_author_found['in_original'] or target_author_found['in_maintenance']:
            f.write("âœ… æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿå·²æ‰¾åˆ°\n")
        else:
            f.write("âŒ æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿæœªæ‰¾åˆ°\n")
        
        f.write("\n---\n\n")
        f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    print(f"âœ… å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return report_path


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("ç»§ç»­è·å–'æŠ¤è‚¤ä¿å…»'æ•°æ®å¹¶è¿›è¡Œè¯¦ç»†å¯¹æ¯”åˆ†æ")
    print("="*80 + "\n")
    
    try:
        # 1. åŠ è½½ API Key
        api_key = load_api_key()
        
        # 2. è®¾ç½®ç›®å½•
        script_dir = Path(__file__).parent
        output_dir = script_dir.parent / 'output'
        
        maintenance_dir = output_dir / 'keyword_æŠ¤è‚¤ä¿å…»'
        original_detail_dir = output_dir / 'detail'
        
        # 3. ç»§ç»­è·å–"æŠ¤è‚¤ä¿å…»"æ•°æ®åˆ°28é¡µ
        print("â³ ç»§ç»­è·å–'æŠ¤è‚¤ä¿å…»'æ•°æ®...\n")
        fetch_missing_pages('æŠ¤è‚¤ä¿å…»', 28, maintenance_dir, api_key)
        
        # 4. åŠ è½½æ‰€æœ‰æ•°æ®
        print("\nğŸ“Š åŠ è½½æ•°æ®è¿›è¡Œå¯¹æ¯”åˆ†æ...\n")
        
        original_authors = load_all_authors(original_detail_dir)
        maintenance_authors = load_all_authors(maintenance_dir / 'detail')
        
        print(f"âœ… æŠ¤è‚¤: {len(original_authors)} äºº")
        print(f"âœ… æŠ¤è‚¤ä¿å…»: {len(maintenance_authors)} äºº\n")
        
        # 5. æŸ¥æ‰¾ç›®æ ‡è¾¾äºº
        print("ğŸ” æŸ¥æ‰¾ç›®æ ‡è¾¾äºº'æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ'...\n")
        
        target_in_orig = find_target_author(original_authors, 'æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ')
        target_in_maint = find_target_author(maintenance_authors, 'æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ')
        
        target_author_found = {
            'in_original': target_in_orig is not None,
            'original_data': target_in_orig,
            'in_maintenance': target_in_maint is not None,
            'maintenance_data': target_in_maint
        }
        
        if target_in_orig:
            print(f"âœ… åœ¨'æŠ¤è‚¤'ä¸­æ‰¾åˆ°: {target_in_orig.get('attribute_datas', {}).get('nick_name')}")
        else:
            print(f"âŒ åœ¨'æŠ¤è‚¤'ä¸­æœªæ‰¾åˆ°")
        
        if target_in_maint:
            print(f"âœ… åœ¨'æŠ¤è‚¤ä¿å…»'ä¸­æ‰¾åˆ°: {target_in_maint.get('attribute_datas', {}).get('nick_name')}")
        else:
            print(f"âŒ åœ¨'æŠ¤è‚¤ä¿å…»'ä¸­æœªæ‰¾åˆ°")
        
        print()
        
        # 6. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        print("ğŸ“ ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š...\n")
        report_path = generate_comparison_report(
            original_authors, 
            maintenance_authors,
            target_author_found,
            output_dir
        )
        
        print(f"\n{'='*80}")
        print(f"âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        print(f"{'='*80}\n")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

