#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢è„šæœ¬

åŠŸèƒ½ï¼š
1. è°ƒç”¨ TikHub API çš„æ˜Ÿå›¾ search_kol_v1 æ¥å£æœç´¢"æŠ¤è‚¤è¾¾äºº"
2. è·å– 3 é¡µæœç´¢ç»“æœæ•°æ®
3. åˆ†æè…°éƒ¨è¾¾äººï¼ˆç²‰ä¸æ•° 10ä¸‡~100ä¸‡ï¼‰çš„æ•°é‡å’Œç²‰ä¸æ•°åˆ†å¸ƒ
4. å°†ç»“æœä¿å­˜åˆ° output ç›®å½•

æ¥å£æ–‡æ¡£: https://api.tikhub.io/#/Douyin-Xingtu-API/search_kol_v1_api_v1_douyin_xingtu_search_kol_v1_get
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv


def load_api_key():
    """
    ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key
    
    Returns:
        str: API Key
    
    Raises:
        ValueError: å¦‚æœ API Key æœªè®¾ç½®
    """
    # å®šä½åˆ° backend/.env æ–‡ä»¶
    # ä» backend/test/kol/xingtu-searchkol/code/ éœ€è¦ä¸Š 4 çº§åˆ° backend/
    backend_dir = Path(__file__).parent.parent.parent.parent.parent  # è¿”å›åˆ° backend ç›®å½•
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨ {env_path} æ–‡ä»¶ä¸­é…ç½®")
    
    return api_key


def search_kol_v1(api_key: str, keyword: str, page: int = 1, count: int = 20, sort_type: int = 1, platform_source: str = "_1", save_raw: bool = False, output_dir: str = None) -> dict:
    """
    è°ƒç”¨ TikHub API çš„æ˜Ÿå›¾ search_kol_v1 æ¥å£æœç´¢ KOL è¾¾äºº
    
    Args:
        api_key: TikHub API å¯†é’¥
        keyword: æœç´¢å…³é”®è¯
        page: é¡µç ï¼Œä» 1 å¼€å§‹
        count: æ¯é¡µè¿”å›æ•°é‡ï¼Œå»ºè®® 10-50ï¼Œé»˜è®¤ 20
        sort_type: æ’åºç±»å‹ï¼ˆ1=ç»¼åˆæ’åº, 2=ç²‰ä¸æ•°ä»é«˜åˆ°ä½, 3=ç²‰ä¸æ•°ä»ä½åˆ°é«˜ï¼‰
        platform_source: å¹³å°æ¥æºï¼ˆ"_1"=æŠ–éŸ³ï¼‰
        save_raw: æ˜¯å¦ä¿å­˜åŸå§‹è¯·æ±‚å’Œå“åº”
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        dict: API å“åº”çš„ JSON æ•°æ®
    """
    # API ç«¯ç‚¹ï¼ˆGET è¯·æ±‚ï¼‰
    url = "https://api.tikhub.io/api/v1/douyin/xingtu/search_kol_v1"
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key[:20]}...(éšè—)'  # ç”¨äºè®°å½•ï¼Œéšè—å®Œæ•´ key
    }
    
    # å®é™…è¯·æ±‚å¤´
    actual_headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    # è®¾ç½®æŸ¥è¯¢å‚æ•°ï¼ˆGET è¯·æ±‚çš„å‚æ•°ï¼‰
    params = {
        'keyword': keyword,              # æœç´¢å…³é”®è¯
        'page': page,                    # é¡µç 
        'count': count,                  # æ¯é¡µæ•°é‡
        'sort_type': sort_type,          # æ’åºç±»å‹
        'platformSource': platform_source # å¹³å°æ¥æº
    }
    
    print(f"\nğŸ“¡ å‘é€æ˜Ÿå›¾ KOL æœç´¢è¯·æ±‚...")
    print(f"   å…³é”®è¯: {keyword}")
    print(f"   é¡µç : {page}")
    print(f"   æ•°é‡: {count}")
    print(f"   æ’åº: {sort_type}")
    print(f"   å¹³å°: {platform_source}")
    
    try:
        # å‘é€ GET è¯·æ±‚ï¼ˆä½¿ç”¨å®é™…çš„ headersï¼‰
        response = requests.get(url, headers=actual_headers, params=params, timeout=30)
        
        print(f"   çŠ¶æ€ç : {response.status_code}")
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code == 200:
            result = response.json()
            
            # ä¿å­˜åŸå§‹è¯·æ±‚å’Œå“åº”
            if save_raw and output_dir:
                os.makedirs(output_dir, exist_ok=True)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                raw_file = os.path.join(output_dir, f'api_request_response_page_{page}_{timestamp}.json')
                
                raw_data = {
                    'request': {
                        'url': url,
                        'method': 'GET',
                        'headers': headers,  # ä½¿ç”¨éšè— key çš„ç‰ˆæœ¬
                        'params': params,
                        'timestamp': datetime.now().isoformat()
                    },
                    'response': {
                        'status_code': response.status_code,
                        'headers': dict(response.headers),
                        'body': result,
                        'timestamp': datetime.now().isoformat()
                    }
                }
                
                with open(raw_file, 'w', encoding='utf-8') as f:
                    json.dump(raw_data, f, ensure_ascii=False, indent=2)
                print(f"   ğŸ’¾ åŸå§‹è¯·æ±‚å“åº”å·²ä¿å­˜: {raw_file}")
            
            # æ‰“å°å“åº”ç»“æ„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            print(f"   å“åº”ç»“æ„: {list(result.keys())}")
            
            # æ£€æŸ¥å“åº”ä»£ç 
            code = result.get('code', -1)
            message = result.get('message', 'Unknown')

            print(f"   å“åº”æ¶ˆæ¯: code={code}, message={message}")

            # æ‰“å° data éƒ¨åˆ†çš„ç»“æ„
            if 'data' in result:
                data = result.get('data', {})
                if isinstance(data, dict):
                    print(f"   data ç»“æ„: {list(data.keys())}")
                else:
                    print(f"   data ç±»å‹: {type(data)}")

            if code == 200:  # æ˜Ÿå›¾ API æˆåŠŸè¿”å› code=200
                data = result.get('data', {})

                # è·å–ä½œè€…åˆ—è¡¨ (æ–°APIç»“æ„ä½¿ç”¨authorsè€Œä¸æ˜¯kol_list)
                authors = data.get('authors', [])
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é¡µä¿¡æ¯
                pagination = data.get('pagination', {})
                has_more = pagination.get('has_more', False)
                cursor = pagination.get('cursor', 0)

                print(f"   âœ… æˆåŠŸè·å– {len(authors)} ä¸ªä½œè€…")
                print(f"   è¿˜æœ‰æ›´å¤šæ•°æ®: {has_more}")
                print(f"   ä¸‹ä¸€é¡µæ¸¸æ ‡: {cursor}")

                return result
            else:
                print(f"   âŒ API è¿”å›é”™è¯¯ç : {code}")
                print(f"   é”™è¯¯æ¶ˆæ¯: {message}")
                print(f"   å®Œæ•´å“åº”å‰500å­—ç¬¦: {json.dumps(result, ensure_ascii=False, indent=2)[:500]}")
                return result
        else:
            print(f"   âŒ HTTP è¯·æ±‚å¤±è´¥")
            print(f"   é”™è¯¯ä¿¡æ¯: {response.text[:200]}")
            return {
                "error": f"HTTP {response.status_code}",
                "message": response.text
            }
            
    except requests.exceptions.Timeout:
        print(f"   âŒ è¯·æ±‚è¶…æ—¶")
        return {"error": "timeout"}
    except requests.exceptions.RequestException as e:
        print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
        return {"error": str(e)}
    except Exception as e:
        print(f"   âŒ æœªçŸ¥é”™è¯¯: {str(e)}")
        return {"error": str(e)}


def fetch_multiple_pages(api_key: str, keyword: str, page_count: int = 3, count_per_page: int = 20, output_dir: str = None) -> list:
    """
    è·å–å¤šé¡µæ˜Ÿå›¾ KOL æœç´¢ç»“æœ
    
    Args:
        api_key: API å¯†é’¥
        keyword: æœç´¢å…³é”®è¯
        page_count: è¦è·å–çš„é¡µæ•°
        count_per_page: æ¯é¡µæ•°é‡
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        list: æ‰€æœ‰ KOL æ•°æ®åˆ—è¡¨
    """
    all_kols = []
    
    print(f"\n{'='*60}")
    print(f"ğŸ” å¼€å§‹æœç´¢æ˜Ÿå›¾ KOL: {keyword}")
    print(f"   ç›®æ ‡é¡µæ•°: {page_count}")
    print(f"   æ¯é¡µæ•°é‡: {count_per_page}")
    print(f"{'='*60}")
    
    for page in range(1, page_count + 1):
        print(f"\n[ç¬¬ {page}/{page_count} é¡µ]")
        print("-" * 60)
        
        # è°ƒç”¨æ˜Ÿå›¾ KOL æœç´¢ APIï¼ˆä½¿ç”¨ page å‚æ•°ï¼Œä¿å­˜åŸå§‹è¯·æ±‚å“åº”ï¼‰
        result = search_kol_v1(api_key, keyword, page=page, count=count_per_page, save_raw=True, output_dir=output_dir)
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸï¼ˆæ˜Ÿå›¾ API è¿”å› code=200 è¡¨ç¤ºæˆåŠŸï¼‰
        if 'error' in result or result.get('code') != 200:
            print(f"âš ï¸ ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼Œåœæ­¢æœç´¢")
            break
        
        # æå–ä½œè€…åˆ—è¡¨
        data = result.get('data', {})
        authors = data.get('authors', [])
        pagination = data.get('pagination', {})
        has_more = pagination.get('has_more', False)

        if not authors:
            print(f"âš ï¸ ç¬¬ {page} é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢æœç´¢")
            break

        # æ·»åŠ åˆ°æ€»åˆ—è¡¨
        all_kols.extend(authors)

        # æ˜¾ç¤ºæœ¬é¡µä½œè€…ä¿¡æ¯
        print(f"\næœ¬é¡µä½œè€…é¢„è§ˆ:")
        for i, author in enumerate(authors[:3], 1):
            attr_data = author.get('attribute_datas', {})
            nickname = attr_data.get('nick_name', 'N/A')
            follower_count = int(attr_data.get('follower', '0'))
            star_score = float(attr_data.get('star_index', '0'))
            fans_level = attr_data.get('grade', 'N/A')
            print(f"   {i}. {nickname} - ç²‰ä¸: {follower_count:,} - æ˜Ÿå›¾è¯„åˆ†: {star_score:.1f} - ç­‰çº§: {fans_level}")

        if len(authors) > 3:
            print(f"   ... è¿˜æœ‰ {len(authors) - 3} ä¸ªä½œè€…")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
        if not has_more:
            print(f"\nâœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼ˆå…± {page} é¡µï¼‰")
            break
        
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        if page < page_count:
            print(f"\nâ³ ç­‰å¾… 1 ç§’åç»§ç»­...")
            time.sleep(1)
    
    print(f"\n{'='*60}")
    print(f"âœ… æœç´¢å®Œæˆï¼å…±è·å– {len(all_kols)} ä¸ª KOL")
    print(f"{'='*60}")
    
    return all_kols


def analyze_kol_distribution(kols: list) -> dict:
    """
    åˆ†æ KOL è¾¾äººçš„ç²‰ä¸æ•°åˆ†å¸ƒ
    
    å®šä¹‰ï¼š
    - å¤´éƒ¨è¾¾äºº: ç²‰ä¸æ•° >= 100ä¸‡
    - è…°éƒ¨è¾¾äºº: 10ä¸‡ <= ç²‰ä¸æ•° < 100ä¸‡
    - å°¾éƒ¨è¾¾äºº: 1ä¸‡ <= ç²‰ä¸æ•° < 10ä¸‡
    - ç´ äºº: ç²‰ä¸æ•° < 1ä¸‡
    
    Args:
        kols: KOL åˆ—è¡¨
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š å¼€å§‹åˆ†æ KOL è¾¾äººåˆ†å¸ƒ")
    print(f"{'='*60}")
    
    # åˆ†ç±»ç»Ÿè®¡
    categories = {
        'å¤´éƒ¨è¾¾äºº (>=100ä¸‡)': [],
        'è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)': [],
        'å°¾éƒ¨è¾¾äºº (1ä¸‡~10ä¸‡)': [],
        'ç´ äºº (<1ä¸‡)': []
    }
    
    # ç²‰ä¸æ•°åŒºé—´ç»Ÿè®¡ï¼ˆæ›´ç»†è‡´çš„åŒºé—´ï¼‰
    follower_ranges = {
        '1ä¸‡ä»¥ä¸‹': 0,
        '1-5ä¸‡': 0,
        '5-10ä¸‡': 0,
        '10-20ä¸‡': 0,
        '20-50ä¸‡': 0,
        '50-100ä¸‡': 0,
        '100-200ä¸‡': 0,
        '200-500ä¸‡': 0,
        '500ä¸‡ä»¥ä¸Š': 0
    }
    
    # éå†ä½œè€…è¿›è¡Œåˆ†ç±»
    for author in kols:
        attr_data = author.get('attribute_datas', {})
        follower_count = int(attr_data.get('follower', '0'))
        
        # åˆ†ç±»
        if follower_count >= 1_000_000:
            categories['å¤´éƒ¨è¾¾äºº (>=100ä¸‡)'].append(author)
        elif follower_count >= 100_000:
            categories['è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)'].append(author)
        elif follower_count >= 10_000:
            categories['å°¾éƒ¨è¾¾äºº (1ä¸‡~10ä¸‡)'].append(author)
        else:
            categories['ç´ äºº (<1ä¸‡)'].append(author)
        
        # ç»†åˆ†åŒºé—´ç»Ÿè®¡
        if follower_count < 10_000:
            follower_ranges['1ä¸‡ä»¥ä¸‹'] += 1
        elif follower_count < 50_000:
            follower_ranges['1-5ä¸‡'] += 1
        elif follower_count < 100_000:
            follower_ranges['5-10ä¸‡'] += 1
        elif follower_count < 200_000:
            follower_ranges['10-20ä¸‡'] += 1
        elif follower_count < 500_000:
            follower_ranges['20-50ä¸‡'] += 1
        elif follower_count < 1_000_000:
            follower_ranges['50-100ä¸‡'] += 1
        elif follower_count < 2_000_000:
            follower_ranges['100-200ä¸‡'] += 1
        elif follower_count < 5_000_000:
            follower_ranges['200-500ä¸‡'] += 1
        else:
            follower_ranges['500ä¸‡ä»¥ä¸Š'] += 1
    
    # æ‰“å°åˆ†æç»“æœ
    print(f"\næ€» KOL æ•°: {len(kols)}")
    print(f"\nKOL åˆ†ç±»ç»Ÿè®¡:")
    print("-" * 60)
    
    for category, kol_list in categories.items():
        count = len(kol_list)
        percentage = (count / len(kols) * 100) if kols else 0
        print(f"  {category}: {count} äºº ({percentage:.1f}%)")
    
    print(f"\nç²‰ä¸æ•°åŒºé—´åˆ†å¸ƒ:")
    print("-" * 60)
    
    for range_name, count in follower_ranges.items():
        percentage = (count / len(kols) * 100) if kols else 0
        bar = 'â–ˆ' * int(percentage / 2)  # å¯è§†åŒ–æ¡å½¢å›¾
        print(f"  {range_name:12s}: {count:3d} äºº ({percentage:5.1f}%) {bar}")
    
    # è…°éƒ¨è¾¾äººè¯¦ç»†åˆ†æ
    print(f"\n{'='*60}")
    print(f"ğŸ¯ è…°éƒ¨è¾¾äººè¯¦ç»†åˆ†æ (ç²‰ä¸æ•° 10ä¸‡~100ä¸‡)")
    print(f"{'='*60}")
    
    waist_kols = categories['è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)']
    print(f"è…°éƒ¨è¾¾äººæ€»æ•°: {len(waist_kols)}")
    
    if waist_kols:
        # æŒ‰ç²‰ä¸æ•°æ’åº
        waist_kols_sorted = sorted(waist_kols, key=lambda x: int(x.get('attribute_datas', {}).get('follower', '0')), reverse=True)

        # ç»Ÿè®¡
        follower_counts = [int(kol.get('attribute_datas', {}).get('follower', '0')) for kol in waist_kols]
        avg_followers = sum(follower_counts) / len(follower_counts)
        max_followers = max(follower_counts)
        min_followers = min(follower_counts)

        print(f"\nç²‰ä¸æ•°ç»Ÿè®¡:")
        print(f"  å¹³å‡ç²‰ä¸æ•°: {avg_followers:,.0f}")
        print(f"  æœ€é«˜ç²‰ä¸æ•°: {max_followers:,}")
        print(f"  æœ€ä½ç²‰ä¸æ•°: {min_followers:,}")

        print(f"\nè…°éƒ¨è¾¾äºº TOP 10:")
        print("-" * 60)

        for i, kol in enumerate(waist_kols_sorted[:10], 1):
            attr_data = kol.get('attribute_datas', {})
            nickname = attr_data.get('nick_name', 'N/A')
            follower_count = int(attr_data.get('follower', '0'))
            # è§£ælast_10_itemsæ¥è·å–ä½œå“æ•°é‡ï¼ˆè¿‘ä¼¼å€¼ï¼‰
            last_10_items = attr_data.get('last_10_items', '[]')
            try:
                items = json.loads(last_10_items) if last_10_items else []
                aweme_count = len(items)
            except:
                aweme_count = 0
            star_score = float(attr_data.get('star_index', '0'))
            fans_level = attr_data.get('grade', 'N/A')

            print(f"  {i:2d}. {nickname}")
            print(f"      ç²‰ä¸: {follower_count:,} | ä½œå“: {aweme_count} | æ˜Ÿå›¾è¯„åˆ†: {star_score:.1f} | ç­‰çº§: {fans_level}")

            # æ£€æŸ¥æ˜¯å¦æœ‰å•†ä¸šæŠ¥ä»·ä¿¡æ¯ï¼ˆåœ¨æ–°æ•°æ®ç»“æ„ä¸­å¯èƒ½ä¸å­˜åœ¨ï¼‰
            # è¿™é‡Œæš‚æ—¶è·³è¿‡ä»·æ ¼ä¿¡æ¯ï¼Œå› ä¸ºæ–°APIç»“æ„ä¸­å¯èƒ½æ²¡æœ‰è¿™ä¸ªå­—æ®µ
    
    # æ„å»ºè¿”å›ç»“æœ
    analysis_result = {
        'summary': {
            'total_kols': len(kols),
            'head_kols': len(categories['å¤´éƒ¨è¾¾äºº (>=100ä¸‡)']),
            'waist_kols': len(categories['è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)']),
            'tail_kols': len(categories['å°¾éƒ¨è¾¾äºº (1ä¸‡~10ä¸‡)']),
            'normal_users': len(categories['ç´ äºº (<1ä¸‡)'])
        },
        'follower_ranges': follower_ranges,
        'waist_kol_details': {
            'count': len(waist_kols),
            'avg_followers': avg_followers if waist_kols else 0,
            'max_followers': max_followers if waist_kols else 0,
            'min_followers': min_followers if waist_kols else 0,
            'top_10': waist_kols_sorted[:10] if waist_kols else []
        },
        'categories': {
            category: kol_list
            for category, kol_list in categories.items()
        }
    }
    
    return analysis_result


def save_results(all_kols: list, analysis: dict, output_dir: str, keyword: str):
    """
    ä¿å­˜æœç´¢ç»“æœå’Œåˆ†æç»“æœåˆ°æ–‡ä»¶
    
    Args:
        all_kols: æ‰€æœ‰ KOL æ•°æ®
        analysis: åˆ†æç»“æœ
        output_dir: è¾“å‡ºç›®å½•
        keyword: æœç´¢å…³é”®è¯
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # 1. ä¿å­˜åŸå§‹æœç´¢ç»“æœï¼ˆ3é¡µå®Œæ•´æ•°æ®ï¼‰
    raw_data_file = os.path.join(output_dir, f'xingtu_search_results_3pages_{timestamp}.json')
    with open(raw_data_file, 'w', encoding='utf-8') as f:
        json.dump({
            'search_metadata': {
                'keyword': keyword,
                'search_date': datetime.now().isoformat(),
                'total_kols': len(all_kols),
                'api_interface': 'search_kol_v1',
                'api_source': 'æŠ–éŸ³æ˜Ÿå›¾'
            },
            'kols': all_kols
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ åŸå§‹æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {raw_data_file}")
    
    # 2. ä¿å­˜åˆ†æç»“æœ
    analysis_file = os.path.join(output_dir, f'xingtu_waist_kol_analysis_{timestamp}.json')
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump({
            'analysis_metadata': {
                'keyword': keyword,
                'analysis_date': datetime.now().isoformat(),
                'total_kols': len(all_kols),
                'api_source': 'æŠ–éŸ³æ˜Ÿå›¾'
            },
            'analysis': analysis
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_file}")
    
    # 3. ä¿å­˜è…°éƒ¨è¾¾äººå•ç‹¬åˆ—è¡¨ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
    waist_kols = analysis['categories']['è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)']
    waist_kol_file = os.path.join(output_dir, f'xingtu_waist_kols_only_{timestamp}.json')
    with open(waist_kol_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'keyword': keyword,
                'date': datetime.now().isoformat(),
                'count': len(waist_kols),
                'api_source': 'æŠ–éŸ³æ˜Ÿå›¾'
            },
            'waist_kols': waist_kols
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ è…°éƒ¨è¾¾äººåˆ—è¡¨å·²ä¿å­˜åˆ°: {waist_kol_file}")
    
    # 4. ç”Ÿæˆç®€æŠ¥æ–‡ä»¶ï¼ˆMarkdown æ ¼å¼ï¼‰
    report_file = os.path.join(output_dir, f'xingtu_analysis_report_{timestamp}.md')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"# æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**æœç´¢å…³é”®è¯**: {keyword}\n")
        f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ•°æ®æ¥æº**: æŠ–éŸ³æ˜Ÿå›¾ KOL æœç´¢ API (search_kol_v1)\n\n")
        
        f.write(f"## æ€»ä½“ç»Ÿè®¡\n\n")
        f.write(f"- æ€» KOL æ•°: {analysis['summary']['total_kols']}\n")
        f.write(f"- å¤´éƒ¨è¾¾äºº (>=100ä¸‡): {analysis['summary']['head_kols']} äºº\n")
        f.write(f"- **è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡): {analysis['summary']['waist_kols']} äºº**\n")
        f.write(f"- å°¾éƒ¨è¾¾äºº (1ä¸‡~10ä¸‡): {analysis['summary']['tail_kols']} äºº\n")
        f.write(f"- ç´ äºº (<1ä¸‡): {analysis['summary']['normal_users']} äºº\n\n")
        
        f.write(f"## ç²‰ä¸æ•°åŒºé—´åˆ†å¸ƒ\n\n")
        f.write(f"| åŒºé—´ | æ•°é‡ | å æ¯” |\n")
        f.write(f"|------|------|------|\n")
        total = analysis['summary']['total_kols']
        for range_name, count in analysis['follower_ranges'].items():
            percentage = (count / total * 100) if total > 0 else 0
            f.write(f"| {range_name} | {count} | {percentage:.1f}% |\n")
        
        f.write(f"\n## è…°éƒ¨è¾¾äººè¯¦ç»†ä¿¡æ¯\n\n")
        waist_details = analysis['waist_kol_details']
        f.write(f"- **æ€»æ•°**: {waist_details['count']} äºº\n")
        f.write(f"- **å¹³å‡ç²‰ä¸æ•°**: {waist_details['avg_followers']:,.0f}\n")
        f.write(f"- **ç²‰ä¸æ•°èŒƒå›´**: {waist_details['min_followers']:,} ~ {waist_details['max_followers']:,}\n\n")
        
        if waist_details['top_10']:
            f.write(f"### è…°éƒ¨è¾¾äºº TOP 10\n\n")
            f.write(f"| æ’å | æ˜µç§° | ç²‰ä¸æ•° | ä½œå“æ•° | æ˜Ÿå›¾è¯„åˆ† | ç²‰ä¸ç­‰çº§ |\n")
            f.write(f"|------|------|--------|--------|----------|----------|\n")
            
            for i, kol in enumerate(waist_details['top_10'], 1):
                attr_data = kol.get('attribute_datas', {})
                nickname = attr_data.get('nick_name', 'N/A')
                follower_count = int(attr_data.get('follower', '0'))
                # è§£ælast_10_itemsæ¥è·å–ä½œå“æ•°é‡
                last_10_items = attr_data.get('last_10_items', '[]')
                try:
                    items = json.loads(last_10_items) if last_10_items else []
                    aweme_count = len(items)
                except:
                    aweme_count = 0
                star_score = float(attr_data.get('star_index', '0'))
                fans_level = attr_data.get('grade', 'N/A')

                f.write(f"| {i} | {nickname} | {follower_count:,} | {aweme_count} | {star_score:.1f} | {fans_level} |\n")
    
    print(f"ğŸ’¾ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    print(f"\n{'='*60}")
    print(f"âœ… æ‰€æœ‰æ–‡ä»¶ä¿å­˜å®Œæˆï¼")
    print(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°ï¼šæœç´¢æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººå¹¶åˆ†æ"""
    
    print("=" * 60)
    print("æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢ä¸åˆ†æå·¥å…·")
    print("API æ¥å£: search_kol_v1")
    print("=" * 60)
    
    # 1. åŠ è½½ API Key
    print("\n1ï¸âƒ£ åŠ è½½ API é…ç½®...")
    try:
        api_key = load_api_key()
        print(f"âœ… API Key å·²åŠ è½½: {api_key[:10]}...{api_key[-10:]}")
    except ValueError as e:
        print(f"âŒ {e}")
        return
    
    # 2. æœç´¢æŠ¤è‚¤è¾¾äººï¼ˆè·å– 3 é¡µæ•°æ®ï¼‰
    print("\n2ï¸âƒ£ å¼€å§‹æœç´¢...")
    keyword = "æŠ¤è‚¤"  # ä½¿ç”¨ç®€çŸ­å…³é”®è¯ï¼Œæ˜Ÿå›¾æœç´¢å¯èƒ½ä¸æ”¯æŒå¤ªé•¿çš„å…³é”®è¯
    page_count = 3
    count_per_page = 20  # æ˜Ÿå›¾ API å»ºè®®æ¯é¡µ 20 æ¡
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    script_dir = Path(__file__).parent.parent  # backend/test/kol/xingtu-searchkol/
    output_dir = script_dir / "output"
    
    all_kols = fetch_multiple_pages(api_key, keyword, page_count, count_per_page, str(output_dir))
    
    if not all_kols:
        print("âŒ æœªè·å–åˆ°ä»»ä½• KOL æ•°æ®")
        return
    
    # 3. åˆ†æè¾¾äººåˆ†å¸ƒ
    print("\n3ï¸âƒ£ åˆ†æè¾¾äººåˆ†å¸ƒ...")
    analysis = analyze_kol_distribution(all_kols)
    
    # 4. ä¿å­˜ç»“æœ
    print("\n4ï¸âƒ£ ä¿å­˜ç»“æœ...")
    save_results(all_kols, analysis, str(output_dir), keyword)
    
    print("\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"\nğŸ“Œ å…³é”®å‘ç°:")
    print(f"   æœç´¢å…³é”®è¯: {keyword}")
    print(f"   æ€» KOL æ•°: {len(all_kols)}")
    print(f"   è…°éƒ¨è¾¾äººæ•°: {analysis['summary']['waist_kols']} äºº")
    if len(all_kols) > 0:
        print(f"   è…°éƒ¨è¾¾äººå æ¯”: {(analysis['summary']['waist_kols'] / len(all_kols) * 100):.1f}%")


if __name__ == "__main__":
    main()

