#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢åˆ†æä¸å¤šå…³é”®è¯æµ‹è¯•è„šæœ¬

åŠŸèƒ½ï¼š
1. åˆ†æå‰1-20é¡µçš„æ•°æ®å˜åŒ–è¶‹åŠ¿
2. æœç´¢ç‰¹å®šè¾¾äºº"æŠ€æœ¯å‘˜å°æ˜Ÿæ˜ŸğŸŒŸ"
3. åˆ†æä¸ºä»€ä¹ˆæŸäº›è¾¾äººæ²¡æœ‰å‡ºç°åœ¨æœç´¢ç»“æœä¸­
4. æµ‹è¯•ä¸åŒå…³é”®è¯ç»„åˆçš„æœç´¢æ•ˆæœ
5. ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š

æ¥å£æ–‡æ¡£: https://api.tikhub.io/#/Douyin-Xingtu-API/search_kol_v1_api_v1_douyin_xingtu_search_kol_v1_get
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, List, Any, Tuple
from collections import Counter
import statistics


def load_api_key():
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key"""
    backend_dir = Path(__file__).parent.parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®")
    
    return api_key


def load_page_data(page: int, output_dir: str) -> Dict[str, Any]:
    """åŠ è½½æŒ‡å®šé¡µé¢çš„åŸå§‹æ•°æ®"""
    detail_dir = os.path.join(output_dir, 'detail')
    
    # æŸ¥æ‰¾è¯¥é¡µé¢çš„æ–‡ä»¶ï¼ˆå¯èƒ½æœ‰å¤šä¸ªæ—¶é—´æˆ³ç‰ˆæœ¬ï¼Œå–æœ€æ–°çš„ï¼‰
    import glob
    pattern = os.path.join(detail_dir, f'raw_page_{page}_*.json')
    files = glob.glob(pattern)
    
    if not files:
        return None
    
    # å–æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(files, key=os.path.getmtime)
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def analyze_pages_trend(start_page: int, end_page: int, output_dir: str) -> Dict[str, Any]:
    """
    åˆ†æé¡µé¢æ•°æ®çš„å˜åŒ–è¶‹åŠ¿
    
    Args:
        start_page: èµ·å§‹é¡µç 
        end_page: ç»“æŸé¡µç 
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“Š åˆ†æç¬¬ {start_page}-{end_page} é¡µæ•°æ®å˜åŒ–è¶‹åŠ¿")
    print(f"{'='*80}")
    
    page_stats = []
    all_kols = []
    
    for page in range(start_page, end_page + 1):
        page_data = load_page_data(page, output_dir)
        
        if not page_data:
            print(f"âš ï¸ ç¬¬ {page} é¡µæ•°æ®æœªæ‰¾åˆ°")
            continue
        
        # æå–æ•°æ®
        data = page_data.get('data', {})
        authors = data.get('authors', [])
        
        # ç»Ÿè®¡æœ¬é¡µæ•°æ®
        followers = []
        star_scores = []
        interaction_rates = []
        prices = []
        
        for author in authors:
            attr_data = author.get('attribute_datas', {})
            
            follower = int(attr_data.get('follower', 0))
            followers.append(follower)
            
            star_score = float(attr_data.get('star_index', 0))
            if star_score > 0:
                star_scores.append(star_score)
            
            price = int(attr_data.get('price_20_60', 0))
            if price > 0:
                prices.append(price)
            
            # è®¡ç®—äº’åŠ¨ç‡
            last_10_items_str = attr_data.get('last_10_items', '[]')
            try:
                last_10_items = json.loads(last_10_items_str) if last_10_items_str else []
                total_vv = sum(int(item.get('vv', 0)) for item in last_10_items)
                total_interaction = sum(
                    int(item.get('like_cnt', 0)) + 
                    int(item.get('comment_cnt', 0)) + 
                    int(item.get('share_cnt', 0)) 
                    for item in last_10_items
                )
                if total_vv > 0:
                    interaction_rate = (total_interaction / total_vv) * 100
                    interaction_rates.append(interaction_rate)
            except:
                pass
        
        page_stat = {
            'page': page,
            'kol_count': len(authors),
            'avg_followers': sum(followers) // len(followers) if followers else 0,
            'max_followers': max(followers) if followers else 0,
            'min_followers': min(followers) if followers else 0,
            'avg_star_score': sum(star_scores) / len(star_scores) if star_scores else 0,
            'avg_interaction_rate': sum(interaction_rates) / len(interaction_rates) if interaction_rates else 0,
            'avg_price': sum(prices) // len(prices) if prices else 0,
            'has_star_score_count': len(star_scores),
            'has_price_count': len(prices)
        }
        
        page_stats.append(page_stat)
        all_kols.extend(authors)
        
        print(f"ç¬¬ {page:2d} é¡µ: {len(authors)} ä¸ªè¾¾äºº, å¹³å‡ç²‰ä¸ {page_stat['avg_followers']:,}, å¹³å‡è¯„åˆ† {page_stat['avg_star_score']:.2f}")
    
    return {
        'page_stats': page_stats,
        'total_kols': len(all_kols),
        'pages_analyzed': len(page_stats)
    }


def search_specific_kol(api_key: str, kol_name: str, output_dir: str) -> Dict[str, Any]:
    """
    æœç´¢ç‰¹å®šè¾¾äºº
    
    Args:
        api_key: APIå¯†é’¥
        kol_name: è¾¾äººåç§°
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        dict: æœç´¢ç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"ğŸ” æœç´¢ç‰¹å®šè¾¾äºº: {kol_name}")
    print(f"{'='*80}")
    
    url = "https://api.tikhub.io/api/v1/douyin/xingtu/search_kol_v1"
    
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    params = {
        'keyword': kol_name,
        'page': 1,
        'count': 20,
        'sort_type': 1,
        'platformSource': '_1'
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            
            # ä¿å­˜æœç´¢ç»“æœ
            detail_dir = os.path.join(output_dir, 'detail')
            os.makedirs(detail_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'search_{kol_name.replace("/", "_")}_{timestamp}.json'
            filepath = os.path.join(detail_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜: {filename}")
            
            # åˆ†æç»“æœ
            code = result.get('code', -1)
            if code == 200:
                data = result.get('data', {})
                authors = data.get('authors', [])
                
                print(f"âœ… æ‰¾åˆ° {len(authors)} ä¸ªè¾¾äºº")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡è¾¾äºº
                found = False
                for author in authors:
                    attr_data = author.get('attribute_datas', {})
                    nick_name = attr_data.get('nick_name', '')
                    if kol_name in nick_name or nick_name in kol_name:
                        found = True
                        print(f"\nâœ… æ‰¾åˆ°ç›®æ ‡è¾¾äºº:")
                        print(f"   æ˜µç§°: {nick_name}")
                        print(f"   è¾¾äººID: {author.get('star_id', '')}")
                        print(f"   ç²‰ä¸æ•°: {int(attr_data.get('follower', 0)):,}")
                        print(f"   æ˜Ÿå›¾è¯„åˆ†: {float(attr_data.get('star_index', 0)):.2f}")
                        break
                
                if not found:
                    print(f"\nâš ï¸ æœªæ‰¾åˆ°ç›®æ ‡è¾¾äºº '{kol_name}'")
                    print(f"è¿”å›çš„è¾¾äººåˆ—è¡¨:")
                    for i, author in enumerate(authors[:5], 1):
                        attr_data = author.get('attribute_datas', {})
                        print(f"   {i}. {attr_data.get('nick_name', '')} - ç²‰ä¸ {int(attr_data.get('follower', 0)):,}")
                
                return result
            else:
                print(f"âŒ API è¿”å›é”™è¯¯ç : {code}")
                return result
        else:
            print(f"âŒ HTTP è¯·æ±‚å¤±è´¥: {response.status_code}")
            return {"error": f"HTTP {response.status_code}"}
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
        return {"error": str(e)}


def test_keyword_combinations(api_key: str, output_dir: str) -> List[Dict[str, Any]]:
    """
    æµ‹è¯•ä¸åŒå…³é”®è¯ç»„åˆ
    
    Args:
        api_key: APIå¯†é’¥
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        list: æ‰€æœ‰æµ‹è¯•ç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"ğŸ§ª æµ‹è¯•ä¸åŒå…³é”®è¯ç»„åˆ")
    print(f"{'='*80}")
    
    keywords = [
        "æŠ¤è‚¤",
        "æŠ¤è‚¤è¾¾äºº",
        "æŠ¤è‚¤åšä¸»",
        "ç¾å¦†æŠ¤è‚¤",
        "ç§‘å­¦æŠ¤è‚¤"
    ]
    
    results = []
    
    for i, keyword in enumerate(keywords, 1):
        print(f"\n[{i}/{len(keywords)}] æµ‹è¯•å…³é”®è¯: {keyword}")
        print("-" * 80)
        
        url = "https://api.tikhub.io/api/v1/douyin/xingtu/search_kol_v1"
        
        headers = {
            'accept': 'application/json',
            'Authorization': f'Bearer {api_key}'
        }
        
        params = {
            'keyword': keyword,
            'page': 1,
            'count': 20,
            'sort_type': 1,
            'platformSource': '_1'
        }
        
        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                
                # ä¿å­˜ç»“æœ
                detail_dir = os.path.join(output_dir, 'detail')
                os.makedirs(detail_dir, exist_ok=True)
                
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f'keyword_test_{keyword}_{timestamp}.json'
                filepath = os.path.join(detail_dir, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                
                # åˆ†æç»“æœ
                code = result.get('code', -1)
                if code == 200:
                    data = result.get('data', {})
                    authors = data.get('authors', [])
                    
                    # ç»Ÿè®¡æ•°æ®
                    followers = [int(author.get('attribute_datas', {}).get('follower', 0)) for author in authors]
                    avg_followers = sum(followers) // len(followers) if followers else 0
                    
                    star_scores = [
                        float(author.get('attribute_datas', {}).get('star_index', 0)) 
                        for author in authors 
                        if float(author.get('attribute_datas', {}).get('star_index', 0)) > 0
                    ]
                    avg_star_score = sum(star_scores) / len(star_scores) if star_scores else 0
                    
                    print(f"âœ… è¿”å› {len(authors)} ä¸ªè¾¾äºº")
                    print(f"   å¹³å‡ç²‰ä¸æ•°: {avg_followers:,}")
                    print(f"   å¹³å‡æ˜Ÿå›¾è¯„åˆ†: {avg_star_score:.2f}")
                    print(f"   ç²‰ä¸æ•°èŒƒå›´: {min(followers):,} - {max(followers):,}")
                    
                    # æ˜¾ç¤ºå‰3ä¸ªè¾¾äºº
                    print(f"\n   å‰3ä¸ªè¾¾äºº:")
                    for j, author in enumerate(authors[:3], 1):
                        attr_data = author.get('attribute_datas', {})
                        print(f"   {j}. {attr_data.get('nick_name', '')} - ç²‰ä¸ {int(attr_data.get('follower', 0)):,}")
                    
                    results.append({
                        'keyword': keyword,
                        'success': True,
                        'kol_count': len(authors),
                        'avg_followers': avg_followers,
                        'avg_star_score': avg_star_score,
                        'authors': authors,
                        'filename': filename
                    })
                else:
                    print(f"âŒ API è¿”å›é”™è¯¯ç : {code}")
                    results.append({
                        'keyword': keyword,
                        'success': False,
                        'error': f"APIé”™è¯¯ç  {code}"
                    })
            else:
                print(f"âŒ HTTP è¯·æ±‚å¤±è´¥: {response.status_code}")
                results.append({
                    'keyword': keyword,
                    'success': False,
                    'error': f"HTTP {response.status_code}"
                })
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
            results.append({
                'keyword': keyword,
                'success': False,
                'error': str(e)
            })
        
        # æ·»åŠ å»¶è¿Ÿ
        if i < len(keywords):
            print(f"\nâ³ ç­‰å¾… 1 ç§’...")
            time.sleep(1)
    
    return results


def generate_analysis_report(
    pages_trend: Dict[str, Any],
    keyword_test_results: List[Dict[str, Any]],
    output_dir: str
):
    """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # 1. åˆ†æå‰1-20é¡µæ•°æ®è¶‹åŠ¿
    page_stats = pages_trend['page_stats']
    
    # è®¡ç®—è¶‹åŠ¿
    followers_trend = [stat['avg_followers'] for stat in page_stats]
    star_scores_trend = [stat['avg_star_score'] for stat in page_stats]
    prices_trend = [stat['avg_price'] for stat in page_stats]
    
    report_content = f"""# æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢æ·±åº¦åˆ†ææŠ¥å‘Š

## ğŸ“Š æŠ¥å‘Šæ¦‚è§ˆ

- **åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **åˆ†æèŒƒå›´**: ç¬¬1-20é¡µæ•°æ®
- **æ€»è¾¾äººæ•°**: {pages_trend['total_kols']}
- **æµ‹è¯•å…³é”®è¯**: {len(keyword_test_results)} ä¸ª

---

## ä¸€ã€å‰1-20é¡µæ•°æ®å˜åŒ–è¶‹åŠ¿åˆ†æ

### 1.1 æ•´ä½“è¶‹åŠ¿æ€»ç»“

"""
    
    # ç²‰ä¸æ•°è¶‹åŠ¿
    if len(followers_trend) > 1:
        first_half_avg = sum(followers_trend[:10]) / len(followers_trend[:10])
        second_half_avg = sum(followers_trend[10:]) / len(followers_trend[10:])
        change_rate = ((second_half_avg - first_half_avg) / first_half_avg * 100) if first_half_avg > 0 else 0
        
        report_content += f"""
**ç²‰ä¸æ•°å˜åŒ–**:
- å‰10é¡µå¹³å‡: {int(first_half_avg):,}
- å10é¡µå¹³å‡: {int(second_half_avg):,}
- å˜åŒ–ç‡: {change_rate:+.1f}%
- è¶‹åŠ¿: {'ğŸ“‰ ä¸‹é™' if change_rate < -5 else 'ğŸ“ˆ ä¸Šå‡' if change_rate > 5 else 'â¡ï¸ ç¨³å®š'}

"""
    
    # æ˜Ÿå›¾è¯„åˆ†è¶‹åŠ¿
    valid_star_scores = [s for s in star_scores_trend if s > 0]
    if len(valid_star_scores) > 1:
        avg_star_score = sum(valid_star_scores) / len(valid_star_scores)
        report_content += f"""
**æ˜Ÿå›¾è¯„åˆ†**:
- å¹³å‡è¯„åˆ†: {avg_star_score:.2f}
- æœ€é«˜è¯„åˆ†: {max(valid_star_scores):.2f}
- æœ€ä½è¯„åˆ†: {min(valid_star_scores):.2f}
- æ ‡å‡†å·®: {statistics.stdev(valid_star_scores):.2f}

"""
    
    # æŠ¥ä»·è¶‹åŠ¿
    valid_prices = [p for p in prices_trend if p > 0]
    if len(valid_prices) > 1:
        avg_price = sum(valid_prices) / len(valid_prices)
        report_content += f"""
**å•†ä¸šæŠ¥ä»·**:
- å¹³å‡æŠ¥ä»·: {int(avg_price):,} å…ƒ
- æœ€é«˜æŠ¥ä»·: {max(valid_prices):,} å…ƒ
- æœ€ä½æŠ¥ä»·: {min(valid_prices):,} å…ƒ

"""
    
    report_content += f"""
### 1.2 é€é¡µè¯¦ç»†æ•°æ®

| é¡µç  | è¾¾äººæ•° | å¹³å‡ç²‰ä¸æ•° | ç²‰ä¸æ•°èŒƒå›´ | å¹³å‡æ˜Ÿå›¾è¯„åˆ† | å¹³å‡æŠ¥ä»· |
|------|--------|-----------|-----------|-------------|---------|
"""
    
    for stat in page_stats:
        report_content += f"| {stat['page']} | {stat['kol_count']} | {stat['avg_followers']:,} | {stat['min_followers']:,} - {stat['max_followers']:,} | {stat['avg_star_score']:.2f} | {stat['avg_price']:,} å…ƒ |\n"
    
    report_content += f"""
### 1.3 æ•°æ®è´¨é‡åˆ†æ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ€»è¾¾äººæ•° | {pages_trend['total_kols']} |
| æœ‰æ˜Ÿå›¾è¯„åˆ†çš„é¡µé¢æ•° | {sum(1 for stat in page_stats if stat['avg_star_score'] > 0)} / {len(page_stats)} |
| æœ‰æŠ¥ä»·çš„é¡µé¢æ•° | {sum(1 for stat in page_stats if stat['avg_price'] > 0)} / {len(page_stats)} |
| å¹³å‡æ¯é¡µè¾¾äººæ•° | {pages_trend['total_kols'] / len(page_stats):.1f} |

### 1.4 å…³é”®å‘ç°

"""
    
    # åˆ†æå…³é”®å‘ç°
    if change_rate < -10:
        report_content += f"1. **ç²‰ä¸æ•°æ˜¾è‘—ä¸‹é™**: ä»ç¬¬1é¡µåˆ°ç¬¬20é¡µï¼Œå¹³å‡ç²‰ä¸æ•°ä¸‹é™äº† {abs(change_rate):.1f}%ï¼Œè¯´æ˜æ˜Ÿå›¾æœç´¢ç»“æœæŒ‰ç²‰ä¸æ•°æˆ–ç»¼åˆæ’åºé€’å‡\n"
    elif change_rate > 10:
        report_content += f"1. **ç²‰ä¸æ•°ä¸Šå‡**: å¹³å‡ç²‰ä¸æ•°ä¸Šå‡äº† {change_rate:.1f}%ï¼Œæ’åºå¯èƒ½ä¸æ˜¯ä¸¥æ ¼æŒ‰ç²‰ä¸æ•°é€’å‡\n"
    else:
        report_content += f"1. **ç²‰ä¸æ•°ç›¸å¯¹ç¨³å®š**: å‰å10é¡µç²‰ä¸æ•°å˜åŒ–ä¸å¤§ï¼Œè¯´æ˜æœç´¢ç»“æœé›†ä¸­åœ¨æŸä¸ªç²‰ä¸åŒºé—´\n"
    
    # ç²‰ä¸åŒºé—´åˆ†æ
    max_follower = max(stat['max_followers'] for stat in page_stats)
    min_follower = min(stat['min_followers'] for stat in page_stats)
    report_content += f"2. **ç²‰ä¸åŒºé—´**: {min_follower:,} - {max_follower:,}ï¼Œè·¨åº¦ {max_follower / min_follower:.1f} å€\n"
    
    # æ˜Ÿå›¾è¯„åˆ†è¦†ç›–ç‡
    has_score_count = sum(stat['has_star_score_count'] for stat in page_stats)
    total_kols = pages_trend['total_kols']
    score_coverage = (has_score_count / total_kols * 100) if total_kols > 0 else 0
    report_content += f"3. **æ˜Ÿå›¾è¯„åˆ†è¦†ç›–ç‡**: {score_coverage:.1f}%ï¼Œä»…éƒ¨åˆ†è¾¾äººæœ‰æ˜Ÿå›¾è¯„åˆ†\n"
    
    # æŠ¥ä»·è¦†ç›–ç‡
    has_price_count = sum(stat['has_price_count'] for stat in page_stats)
    price_coverage = (has_price_count / total_kols * 100) if total_kols > 0 else 0
    report_content += f"4. **æŠ¥ä»·è¦†ç›–ç‡**: {price_coverage:.1f}%ï¼Œçº¦ä¸€åŠè¾¾äººæœ‰æ˜ç¡®æŠ¥ä»·\n"
    
    report_content += f"""
---

## äºŒã€ç‰¹å®šè¾¾äººæœç´¢åˆ†æ

### 2.1 æœç´¢ç›®æ ‡

**è¾¾äººåç§°**: æŠ€æœ¯å‘˜å°æ˜Ÿæ˜ŸğŸŒŸ
- ç²‰ä¸æ•°: 93.5ä¸‡
- è·èµæ•°: 944.9ä¸‡
- å…³æ³¨æ•°: 433
- ç‰¹ç‚¹: IPå±åœ°åŒ—äº¬ï¼ŒæŠ¤è‚¤å†…å®¹åˆ›ä½œè€…

### 2.2 æœç´¢ç»“æœ

"""
    
    # æ£€æŸ¥æ˜¯å¦åœ¨ç°æœ‰æ•°æ®ä¸­
    report_content += f"""
#### åœ¨"æŠ¤è‚¤"å…³é”®è¯ç»“æœä¸­

âŒ **æœªæ‰¾åˆ°** - åœ¨å‰28é¡µï¼ˆ560ä¸ªè¾¾äººï¼‰æœç´¢ç»“æœä¸­æœªæ‰¾åˆ°è¯¥è¾¾äºº

"""
    
    report_content += f"""
### 2.3 æœªæ‰¾åˆ°åŸå› åˆ†æ

å¯èƒ½çš„åŸå› ï¼š

1. **å…³é”®è¯åŒ¹é…é—®é¢˜**
   - è¾¾äººæ˜µç§°"æŠ€æœ¯å‘˜å°æ˜Ÿæ˜ŸğŸŒŸ"ä¸åŒ…å«"æŠ¤è‚¤"å…³é”®å­—
   - æ˜Ÿå›¾æœç´¢ä¸»è¦åŒ¹é…æ˜µç§°ã€ç®€ä»‹ã€æ ‡ç­¾ä¸­çš„å…³é”®è¯
   - å³ä½¿å†…å®¹æ˜¯æŠ¤è‚¤ç›¸å…³ï¼Œæ˜µç§°ä¸åŒ¹é…ä¹Ÿå¯èƒ½æ’åé å

2. **æ’åºç®—æ³•å½±å“**
   - æ˜Ÿå›¾é»˜è®¤ä½¿ç”¨ç»¼åˆæ’åºï¼ˆsort_type=1ï¼‰
   - ç»¼åˆæ’åºè€ƒè™‘ï¼šç²‰ä¸æ•°ã€äº’åŠ¨ç‡ã€æ˜Ÿå›¾è¯„åˆ†ã€å•†ä¸šåˆä½œå†å²ç­‰
   - è¯¥è¾¾äººå¯èƒ½åœ¨ç»¼åˆè¯„åˆ†ä¸­æ’åè¾ƒå

3. **æ˜Ÿå›¾è®¤è¯çŠ¶æ€**
   - è¯¥è¾¾äººå¯èƒ½æœªå…¥é©»æ˜Ÿå›¾å¹³å°
   - æˆ–å…¥é©»ä½†æœªå®Œæˆè®¤è¯
   - æœªè®¤è¯è¾¾äººå¯èƒ½ä¸ä¼šå‡ºç°åœ¨æœç´¢ç»“æœä¸­

4. **å†…å®¹æ ‡ç­¾åˆ†ç±»**
   - è¾¾äººçš„å†…å®¹æ ‡ç­¾å¯èƒ½ä¸å®Œå…¨åŒ¹é…"æŠ¤è‚¤"
   - å¯èƒ½è¢«å½’ç±»ä¸º"ç¾å¦†"ã€"ç§‘æ™®"ç­‰å…¶ä»–ç±»åˆ«

5. **æœç´¢ç»“æœåˆ†é¡µé™åˆ¶**
   - æˆ‘ä»¬åªæœç´¢äº†28é¡µï¼ˆ560ä¸ªè¾¾äººï¼‰
   - æ˜Ÿå›¾å¯èƒ½æœ‰æ•°åƒä¸ªæŠ¤è‚¤ç›¸å…³è¾¾äºº
   - è¯¥è¾¾äººå¯èƒ½åœ¨æ›´åé¢çš„é¡µæ•°

---

## ä¸‰ã€å¤šå…³é”®è¯æœç´¢å¯¹æ¯”åˆ†æ

### 3.1 æµ‹è¯•å…³é”®è¯åˆ—è¡¨

"""
    
    # å…³é”®è¯æµ‹è¯•ç»“æœ
    for i, result in enumerate(keyword_test_results, 1):
        if result['success']:
            report_content += f"""
#### {i}. å…³é”®è¯: {result['keyword']}

- **è¿”å›è¾¾äººæ•°**: {result['kol_count']}
- **å¹³å‡ç²‰ä¸æ•°**: {result['avg_followers']:,}
- **å¹³å‡æ˜Ÿå›¾è¯„åˆ†**: {result['avg_star_score']:.2f}
- **æ•°æ®æ–‡ä»¶**: `{result['filename']}`

"""
        else:
            report_content += f"""
#### {i}. å…³é”®è¯: {result['keyword']}

âŒ **æœç´¢å¤±è´¥**: {result.get('error', 'æœªçŸ¥é”™è¯¯')}

"""
    
    report_content += f"""
### 3.2 å…³é”®è¯æ•ˆæœå¯¹æ¯”

| å…³é”®è¯ | è¾¾äººæ•° | å¹³å‡ç²‰ä¸æ•° | å¹³å‡æ˜Ÿå›¾è¯„åˆ† | çŠ¶æ€ |
|--------|--------|-----------|-------------|------|
"""
    
    for result in keyword_test_results:
        if result['success']:
            report_content += f"| {result['keyword']} | {result['kol_count']} | {result['avg_followers']:,} | {result['avg_star_score']:.2f} | âœ… æˆåŠŸ |\n"
        else:
            report_content += f"| {result['keyword']} | - | - | - | âŒ å¤±è´¥ |\n"
    
    report_content += f"""
### 3.3 å…³é”®è¯æœç´¢è§„å¾‹æ€»ç»“

"""
    
    # åˆ†æä¸åŒå…³é”®è¯çš„æ•ˆæœ
    successful_results = [r for r in keyword_test_results if r['success']]
    
    if len(successful_results) >= 2:
        # æŒ‰å¹³å‡ç²‰ä¸æ•°æ’åº
        sorted_by_followers = sorted(successful_results, key=lambda x: x['avg_followers'], reverse=True)
        report_content += f"""
**æŒ‰å¹³å‡ç²‰ä¸æ•°æ’åº**:
1. **{sorted_by_followers[0]['keyword']}**: å¹³å‡ç²‰ä¸æ•°æœ€é«˜ï¼ˆ{sorted_by_followers[0]['avg_followers']:,}ï¼‰
2. **{sorted_by_followers[-1]['keyword']}**: å¹³å‡ç²‰ä¸æ•°æœ€ä½ï¼ˆ{sorted_by_followers[-1]['avg_followers']:,}ï¼‰

"""
        
        # æŒ‰æ˜Ÿå›¾è¯„åˆ†æ’åº
        sorted_by_score = sorted(successful_results, key=lambda x: x['avg_star_score'], reverse=True)
        report_content += f"""
**æŒ‰å¹³å‡æ˜Ÿå›¾è¯„åˆ†æ’åº**:
1. **{sorted_by_score[0]['keyword']}**: å¹³å‡è¯„åˆ†æœ€é«˜ï¼ˆ{sorted_by_score[0]['avg_star_score']:.2f}ï¼‰
2. **{sorted_by_score[-1]['keyword']}**: å¹³å‡è¯„åˆ†æœ€ä½ï¼ˆ{sorted_by_score[-1]['avg_star_score']:.2f}ï¼‰

"""
    
    report_content += f"""
---

## å››ã€å¦‚ä½•æ‰¾åˆ°ç›®æ ‡è¾¾äºº

### 4.1 æœç´¢ç­–ç•¥å»ºè®®

#### ç­–ç•¥1: ç²¾ç¡®æ˜µç§°æœç´¢
- **æ–¹æ³•**: ç›´æ¥æœç´¢è¾¾äººæ˜µç§°"æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ"
- **ä¼˜ç‚¹**: æœ€ç²¾ç¡®ï¼Œå¦‚æœè¾¾äººåœ¨æ˜Ÿå›¾åˆ™èƒ½ç›´æ¥æ‰¾åˆ°
- **é€‚ç”¨**: å·²çŸ¥è¾¾äººæ˜µç§°çš„æƒ…å†µ

#### ç­–ç•¥2: å¤šå…³é”®è¯ç»„åˆ
- **æ–¹æ³•**: ä½¿ç”¨"æŠ¤è‚¤+è¾¾äºº"ã€"ç¾å¦†+æŠ¤è‚¤"ç­‰ç»„åˆè¯
- **ä¼˜ç‚¹**: å¯èƒ½åŒ¹é…åˆ°æ›´å¤šç›¸å…³è¾¾äºº
- **é€‚ç”¨**: ä¸ç¡®å®šè¾¾äººå…·ä½“åˆ†ç±»æ—¶

#### ç­–ç•¥3: è°ƒæ•´æ’åºæ–¹å¼
- **æ–¹æ³•**: å°è¯•ä¸åŒçš„ sort_type
  - 1 = ç»¼åˆæ’åºï¼ˆé»˜è®¤ï¼‰
  - 2 = ç²‰ä¸æ•°ä»é«˜åˆ°ä½
  - 3 = ç²‰ä¸æ•°ä»ä½åˆ°é«˜
- **ä¼˜ç‚¹**: å¯ä»¥çœ‹åˆ°ä¸åŒç»´åº¦çš„è¾¾äººæ’å
- **é€‚ç”¨**: éœ€è¦ç‰¹å®šç²‰ä¸åŒºé—´çš„è¾¾äºº

#### ç­–ç•¥4: æ‰©å¤§æœç´¢èŒƒå›´
- **æ–¹æ³•**: æœç´¢æ›´å¤šé¡µæ•°ï¼ˆ50é¡µã€100é¡µï¼‰
- **ä¼˜ç‚¹**: è¦†ç›–æ›´å…¨é¢
- **ç¼ºç‚¹**: è€—æ—¶è¾ƒé•¿ï¼ŒAPIè°ƒç”¨æ¬¡æ•°å¤š

#### ç­–ç•¥5: ä½¿ç”¨æ ‡ç­¾ç­›é€‰
- **æ–¹æ³•**: ç»“åˆå†…å®¹æ ‡ç­¾ï¼ˆå¦‚"æŠ¤è‚¤ä¿å…»"ã€"æˆåˆ†æŠ¤è‚¤"ï¼‰
- **ä¼˜ç‚¹**: æ›´ç²¾å‡†çš„å†…å®¹åŒ¹é…
- **é€‚ç”¨**: éœ€è¦ç‰¹å®šç»†åˆ†é¢†åŸŸçš„è¾¾äºº

### 4.2 é’ˆå¯¹"æŠ€æœ¯å‘˜å°æ˜Ÿæ˜ŸğŸŒŸ"çš„æœç´¢å»ºè®®

1. **ç›´æ¥æ˜µç§°æœç´¢** â­â­â­â­â­
   - æœç´¢å…³é”®è¯: "æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ"
   - å¦‚æœè¾¾äººåœ¨æ˜Ÿå›¾ï¼Œæ­¤æ–¹æ³•æœ€æœ‰æ•ˆ

2. **ç§‘æ™®ç±»å…³é”®è¯** â­â­â­â­
   - æœç´¢å…³é”®è¯: "ç§‘å­¦æŠ¤è‚¤"ã€"æŠ¤è‚¤ç§‘æ™®"
   - æ ¹æ®è¾¾äººå®šä½ï¼Œå¯èƒ½æ›´ç¬¦åˆ

3. **æˆåˆ†ç±»å…³é”®è¯** â­â­â­â­
   - æœç´¢å…³é”®è¯: "æˆåˆ†æŠ¤è‚¤"ã€"æŠ¤è‚¤æˆåˆ†"
   - æŠ€æœ¯å‘è¾¾äººå¯èƒ½è¢«å½’ç±»äºæ­¤

4. **ç²‰ä¸æ•°ç­›é€‰** â­â­â­
   - ç­›é€‰ 50-100ä¸‡ ç²‰ä¸åŒºé—´
   - é…åˆå…¶ä»–å…³é”®è¯ä½¿ç”¨

### 4.3 æœç´¢æ¥å£æœ€ä½³å®è·µ

```python
# 1. ç²¾ç¡®æ˜µç§°æœç´¢
params = {{
    'keyword': 'æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ',
    'page': 1,
    'count': 20,
    'sort_type': 1,  # ç»¼åˆæ’åº
    'platformSource': '_1'
}}

# 2. æŒ‰ç²‰ä¸æ•°æ’åº
params = {{
    'keyword': 'æŠ¤è‚¤',
    'page': 1,
    'count': 20,
    'sort_type': 2,  # ç²‰ä¸æ•°ä»é«˜åˆ°ä½
    'platformSource': '_1'
}}

# 3. ç»„åˆå…³é”®è¯æœç´¢
keywords = ['ç§‘å­¦æŠ¤è‚¤', 'æˆåˆ†æŠ¤è‚¤', 'æŠ¤è‚¤æŠ€æœ¯']
for keyword in keywords:
    # æœç´¢æ¯ä¸ªå…³é”®è¯
    pass
```

---

## äº”ã€æ•°æ®å·®å¼‚åˆ†æ

### 5.1 "æŠ¤è‚¤" vs å…¶ä»–å…³é”®è¯

"""
    
    # æ¯”è¾ƒä¸åŒå…³é”®è¯çš„æ•°æ®å·®å¼‚
    if len(successful_results) >= 2:
        base_result = next((r for r in successful_results if r['keyword'] == 'æŠ¤è‚¤'), successful_results[0])
        
        report_content += f"""
ä»¥"æŠ¤è‚¤"ä¸ºåŸºå‡†ï¼Œå¯¹æ¯”å…¶ä»–å…³é”®è¯çš„å·®å¼‚ï¼š

| å…³é”®è¯ | è¾¾äººæ•°å·®å¼‚ | å¹³å‡ç²‰ä¸æ•°å·®å¼‚ | å¹³å‡è¯„åˆ†å·®å¼‚ |
|--------|-----------|--------------|-------------|
"""
        
        for result in successful_results:
            if result['keyword'] != base_result['keyword']:
                kol_diff = result['kol_count'] - base_result['kol_count']
                follower_diff_pct = ((result['avg_followers'] - base_result['avg_followers']) / base_result['avg_followers'] * 100) if base_result['avg_followers'] > 0 else 0
                score_diff = result['avg_star_score'] - base_result['avg_star_score']
                
                report_content += f"| {result['keyword']} | {kol_diff:+d} | {follower_diff_pct:+.1f}% | {score_diff:+.2f} |\n"
    
    report_content += f"""
### 5.2 å…³é”®å‘ç°

"""
    
    # åˆ†æå‘ç°
    if len(successful_results) >= 2:
        # æ‰¾å‡ºè¿”å›è¾¾äººæ•°æœ€å¤šå’Œæœ€å°‘çš„å…³é”®è¯
        max_kol_result = max(successful_results, key=lambda x: x['kol_count'])
        min_kol_result = min(successful_results, key=lambda x: x['kol_count'])
        
        report_content += f"""
1. **æœç´¢ç»“æœæ•°é‡**
   - æœ€å¤š: "{max_kol_result['keyword']}" è¿”å› {max_kol_result['kol_count']} ä¸ªè¾¾äºº
   - æœ€å°‘: "{min_kol_result['keyword']}" è¿”å› {min_kol_result['kol_count']} ä¸ªè¾¾äºº
   - å·®å¼‚: {max_kol_result['kol_count'] - min_kol_result['kol_count']} ä¸ª

"""
        
        # æ‰¾å‡ºå¹³å‡ç²‰ä¸æ•°æœ€é«˜å’Œæœ€ä½çš„å…³é”®è¯
        max_follower_result = max(successful_results, key=lambda x: x['avg_followers'])
        min_follower_result = min(successful_results, key=lambda x: x['avg_followers'])
        
        report_content += f"""
2. **å¹³å‡ç²‰ä¸æ•°**
   - æœ€é«˜: "{max_follower_result['keyword']}" å¹³å‡ç²‰ä¸ {max_follower_result['avg_followers']:,}
   - æœ€ä½: "{min_follower_result['keyword']}" å¹³å‡ç²‰ä¸ {min_follower_result['avg_followers']:,}
   - è¯´æ˜: ä¸åŒå…³é”®è¯ä¼šåŒ¹é…åˆ°ä¸åŒé‡çº§çš„è¾¾äºº

"""
    
    report_content += f"""
3. **æœç´¢ç­–ç•¥å»ºè®®**
   - å•ä¸€å…³é”®è¯ï¼ˆå¦‚"æŠ¤è‚¤"ï¼‰è¦†ç›–èŒƒå›´å¹¿ï¼Œä½†ä¸å¤Ÿç²¾å‡†
   - ç»„åˆå…³é”®è¯ï¼ˆå¦‚"æŠ¤è‚¤è¾¾äºº"ï¼‰æ›´ç²¾å‡†ï¼Œä½†å¯èƒ½é—æ¼éƒ¨åˆ†è¾¾äºº
   - å»ºè®®ä½¿ç”¨å¤šä¸ªå…³é”®è¯ç»„åˆæœç´¢ï¼Œç„¶ååˆå¹¶å»é‡

---

## å…­ã€ç»“è®ºä¸å»ºè®®

### 6.1 æ ¸å¿ƒç»“è®º

1. **å‰1-20é¡µæ•°æ®è¶‹åŠ¿**
   - ç²‰ä¸æ•°å‘ˆ{'ä¸‹é™' if change_rate < -5 else 'ä¸Šå‡' if change_rate > 5 else 'ç¨³å®š'}è¶‹åŠ¿
   - æ˜Ÿå›¾è¯„åˆ†è¦†ç›–ç‡çº¦ {score_coverage:.1f}%
   - å•†ä¸šæŠ¥ä»·è¦†ç›–ç‡çº¦ {price_coverage:.1f}%

2. **ä¸ºä»€ä¹ˆæ‰¾ä¸åˆ°"æŠ€æœ¯å‘˜å°æ˜Ÿæ˜ŸğŸŒŸ"**
   - æ˜µç§°ä¸åŒ…å«"æŠ¤è‚¤"å…³é”®è¯
   - å¯èƒ½æœªå…¥é©»æ˜Ÿå›¾æˆ–è®¤è¯çŠ¶æ€ä¸åŒ
   - æœç´¢æ’åºç®—æ³•å¯¼è‡´æ’åé å
   - éœ€è¦æ›´ç²¾ç¡®çš„æœç´¢æ–¹å¼

3. **å…³é”®è¯é€‰æ‹©çš„å½±å“**
   - ä¸åŒå…³é”®è¯è¿”å›çš„è¾¾äººé›†åˆæœ‰æ˜æ˜¾å·®å¼‚
   - ç»„åˆè¯æ›´ç²¾å‡†ï¼Œå•ä¸€è¯è¦†ç›–æ›´å¹¿
   - å»ºè®®ä½¿ç”¨å¤šå…³é”®è¯ç­–ç•¥

### 6.2 å®æ–½å»ºè®®

#### çŸ­æœŸå»ºè®®ï¼ˆç«‹å³å¯æ‰§è¡Œï¼‰

1. **ç²¾ç¡®æœç´¢ç›®æ ‡è¾¾äºº**
   - ä½¿ç”¨è¾¾äººæ˜µç§°"æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ"ç›´æ¥æœç´¢
   - å¦‚æœæ˜Ÿå›¾æœç´¢æ— ç»“æœï¼Œè€ƒè™‘è¾¾äººæœªå…¥é©»æ˜Ÿå›¾

2. **æ‰©å±•å…³é”®è¯åº“**
   - å»ºç«‹ 10-20 ä¸ªæŠ¤è‚¤ç›¸å…³å…³é”®è¯
   - å®šæœŸç”¨ä¸åŒå…³é”®è¯æœç´¢å¹¶åˆå¹¶ç»“æœ
   - å»ºè®®å…³é”®è¯: "ç§‘å­¦æŠ¤è‚¤"ã€"æˆåˆ†æŠ¤è‚¤"ã€"æŠ¤è‚¤æŠ€æœ¯"ã€"æŠ¤è‚¤ç§‘æ™®"

3. **è°ƒæ•´æœç´¢å‚æ•°**
   - å°è¯•ä¸åŒçš„ sort_typeï¼ˆ1/2/3ï¼‰
   - æ‰©å¤§æœç´¢é¡µæ•°èŒƒå›´ï¼ˆè‡³å°‘50é¡µï¼‰
   - è®°å½•ä¸åŒå‚æ•°ç»„åˆçš„æ•ˆæœ

#### ä¸­æœŸå»ºè®®ï¼ˆ1-2å‘¨å†…ï¼‰

1. **å»ºç«‹è¾¾äººæ•°æ®åº“**
   - åˆå¹¶å¤šæ¬¡æœç´¢ç»“æœ
   - æŒ‰ç²‰ä¸æ•°ã€æ˜Ÿå›¾è¯„åˆ†ã€äº’åŠ¨ç‡åˆ†ç±»
   - å®šæœŸæ›´æ–°è¾¾äººæ•°æ®

2. **ä¼˜åŒ–ç­›é€‰ç­–ç•¥**
   - æ ¹æ®ä¸šåŠ¡éœ€æ±‚è®¾å®šç­›é€‰æ¡ä»¶
   - å»ºç«‹è¾¾äººè¯„åˆ†æ¨¡å‹
   - è‡ªåŠ¨åŒ–ç­›é€‰æµç¨‹

3. **ç›‘æ§æ•°æ®å˜åŒ–**
   - å®šæœŸé‡æ–°æœç´¢å…³é”®è¾¾äºº
   - è·Ÿè¸ªç²‰ä¸æ•°ã€äº’åŠ¨ç‡å˜åŒ–
   - è¯†åˆ«æ½œåŠ›è¾¾äºº

#### é•¿æœŸå»ºè®®ï¼ˆæŒç»­ä¼˜åŒ–ï¼‰

1. **å¤šå¹³å°æ•°æ®å¯¹æ¯”**
   - ç»“åˆæŠ–éŸ³å®˜æ–¹æ•°æ®
   - å‚è€ƒç¬¬ä¸‰æ–¹æ•°æ®å¹³å°
   - äº¤å‰éªŒè¯è¾¾äººä¿¡æ¯

2. **å»ºç«‹æ ‡ç­¾ä½“ç³»**
   - ç»†åˆ†æŠ¤è‚¤è¾¾äººç±»å‹
   - å»ºç«‹è‡ªå®šä¹‰æ ‡ç­¾
   - ä¼˜åŒ–æ¨èç®—æ³•

3. **è‡ªåŠ¨åŒ–å·¥ä½œæµ**
   - å®šæ—¶è‡ªåŠ¨æœç´¢
   - è‡ªåŠ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š
   - å¼‚å¸¸æ•°æ®é¢„è­¦

### 6.3 æ³¨æ„äº‹é¡¹

âš ï¸ **é‡è¦æé†’**:

1. æ˜Ÿå›¾APIæœç´¢ç»“æœå¯èƒ½ä¸åŒ…å«æ‰€æœ‰æŠ–éŸ³è¾¾äºº
2. éƒ¨åˆ†è¾¾äººå¯èƒ½æœªå…¥é©»æ˜Ÿå›¾å¹³å°
3. æœç´¢ç»“æœä¼šéšæ—¶é—´åŠ¨æ€å˜åŒ–
4. å»ºè®®ç»“åˆå¤šç§æ•°æ®æºç»¼åˆåˆ¤æ–­

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æ•°æ®æœ‰æ•ˆæœŸ**: 24å°æ—¶ï¼ˆAPIç¼“å­˜ï¼‰  
**è”ç³»æ–¹å¼**: è§é¡¹ç›®æ–‡æ¡£
"""
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(output_dir, f'æ·±åº¦åˆ†ææŠ¥å‘Š_{timestamp}.md')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"\nğŸ’¾ æ·±åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return report_file


def main():
    """ä¸»å‡½æ•°"""
    
    print("=" * 80)
    print("æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢æ·±åº¦åˆ†æå·¥å…·")
    print("=" * 80)
    
    # 1. åŠ è½½ API Key
    print("\n1ï¸âƒ£ åŠ è½½ API é…ç½®...")
    try:
        api_key = load_api_key()
        print(f"âœ… API Key å·²åŠ è½½")
    except ValueError as e:
        print(f"âŒ {e}")
        return
    
    # 2. è®¾ç½®è¾“å‡ºç›®å½•
    script_dir = Path(__file__).parent.parent
    output_dir = script_dir / "output"
    
    # 3. åˆ†æå‰1-20é¡µæ•°æ®è¶‹åŠ¿
    print("\n2ï¸âƒ£ åˆ†æå‰1-20é¡µæ•°æ®è¶‹åŠ¿...")
    pages_trend = analyze_pages_trend(1, 20, str(output_dir))
    
    # 4. æœç´¢ç‰¹å®šè¾¾äºº"æŠ€æœ¯å‘˜å°æ˜Ÿæ˜ŸğŸŒŸ"
    print("\n3ï¸âƒ£ æœç´¢ç‰¹å®šè¾¾äºº...")
    search_specific_kol(api_key, "æŠ€æœ¯å‘˜å°æ˜Ÿæ˜Ÿ", str(output_dir))
    
    # 5. æµ‹è¯•ä¸åŒå…³é”®è¯ç»„åˆ
    print("\n4ï¸âƒ£ æµ‹è¯•ä¸åŒå…³é”®è¯ç»„åˆ...")
    keyword_test_results = test_keyword_combinations(api_key, str(output_dir))
    
    # 6. ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
    print("\n5ï¸âƒ£ ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...")
    generate_analysis_report(pages_trend, keyword_test_results, str(output_dir))
    
    print(f"\n{'='*80}")
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()

