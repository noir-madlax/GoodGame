#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åˆ†æè¾¾äººè§†é¢‘èƒ½åŠ›è¯„ä¼°æŒ‡æ ‡

åŸºäºæŠ–éŸ³/å°çº¢ä¹¦ç­‰å¹³å°è¾¾äººè¯„ä¼°çš„å¸¸è§æŒ‡æ ‡ï¼š
1. åŸºç¡€æŒ‡æ ‡ï¼šç²‰ä¸æ•°ã€ä½œå“æ•°ã€å¹³å‡æ’­æ”¾é‡
2. äº’åŠ¨æŒ‡æ ‡ï¼šç‚¹èµã€è¯„è®ºã€åˆ†äº«ã€æ”¶è—
3. æ•ˆç‡æŒ‡æ ‡ï¼šèµè¯„æ¯”ã€äº’åŠ¨ç‡ã€å®Œæ’­ç‡
4. å†…å®¹è´¨é‡ï¼šè§†é¢‘æ—¶é•¿åˆ†å¸ƒã€å†…å®¹æ ‡ç­¾
5. å•†ä¸šåŒ–æŒ‡æ ‡ï¼šå¸¦è´§è§†é¢‘å æ¯”ã€è½¬åŒ–ç‡
"""

import json
import os
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Any, Tuple
import statistics

def analyze_kol_capability_metrics():
    """åˆ†æè¾¾äººè§†é¢‘èƒ½åŠ›çš„å„é¡¹è¯„ä¼°æŒ‡æ ‡"""

    output_dir = Path(__file__).parent / "output"
    data_file = Path(__file__).parent.parent / "kol-video-fetcher" / "output" / "final_video_details.json"

    if not data_file.exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return

    print("ğŸ¯ å¼€å§‹åˆ†æè¾¾äººè§†é¢‘èƒ½åŠ›æŒ‡æ ‡...")
    print("=" * 70)

    # 1. åŠ è½½æ•°æ®
    with open(data_file, 'r', encoding='utf-8') as f:
        videos = json.load(f)

    print(f"ğŸ“Š æ€»è§†é¢‘æ•°: {len(videos)}")
    print()

    # 2. æŒ‰ä½œè€…åˆ†ç»„è§†é¢‘
    author_videos = defaultdict(list)
    for video in videos:
        author = video.get('author', {})
        if isinstance(author, dict):
            uid = author.get('uid', 'unknown')
            nickname = author.get('nickname', 'unknown')
            author_key = f"{uid}_{nickname}"
        else:
            author_key = str(author)

        author_videos[author_key].append(video)

    print(f"ğŸ‘¥ å”¯ä¸€ä½œè€…æ•°: {len(author_videos)}")
    print()

    # 3. è®¡ç®—æ¯ä¸ªä½œè€…çš„åŸºç¡€ç»Ÿè®¡
    author_stats = {}

    for author_key, video_list in author_videos.items():
        if '_' in author_key:
            uid, nickname = author_key.split('_', 1)
        else:
            uid, nickname = 'unknown', author_key

        # åŸºç¡€ç»Ÿè®¡
        total_videos = len(video_list)

        # ç»Ÿè®¡æ•°æ®èšåˆ
        total_digg = sum(v.get('statistics', {}).get('digg_count', 0) for v in video_list)
        total_comment = sum(v.get('statistics', {}).get('comment_count', 0) for v in video_list)
        total_share = sum(v.get('statistics', {}).get('share_count', 0) for v in video_list)
        total_collect = sum(v.get('statistics', {}).get('collect_count', 0) for v in video_list)
        total_play = sum(v.get('statistics', {}).get('play_count', 0) for v in video_list)

        # å¹³å‡æŒ‡æ ‡
        avg_digg = total_digg / total_videos if total_videos > 0 else 0
        avg_comment = total_comment / total_videos if total_videos > 0 else 0
        avg_share = total_share / total_videos if total_videos > 0 else 0
        avg_collect = total_collect / total_videos if total_videos > 0 else 0
        avg_play = total_play / total_videos if total_videos > 0 else 0

        # èµè¯„æ¯”
        like_comment_ratio = total_digg / total_comment if total_comment > 0 else total_digg

        # äº’åŠ¨ç‡ (è¯„è®º+åˆ†äº«+æ”¶è—)/ç‚¹èµ
        total_interactions = total_comment + total_share + total_collect
        interaction_rate = total_interactions / total_digg if total_digg > 0 else 0

        # è§†é¢‘æ—¶é•¿åˆ†æ
        durations = []
        for v in video_list:
            raw_data = v.get('raw_video_data', {})
            duration = raw_data.get('duration')
            if duration:
                durations.append(duration / 1000)  # ç§’

        avg_duration = statistics.mean(durations) if durations else 0

        # å†…å®¹ç‰¹å¾
        has_hashtag = sum(1 for v in video_list if '#' in v.get('desc', ''))
        has_mention = sum(1 for v in video_list if '@' in v.get('desc', ''))

        author_stats[author_key] = {
            'uid': uid,
            'nickname': nickname,
            'video_count': total_videos,

            # åŸºç¡€æŒ‡æ ‡
            'total_digg': total_digg,
            'total_comment': total_comment,
            'total_share': total_share,
            'total_collect': total_collect,
            'total_play': total_play,

            # å¹³å‡æŒ‡æ ‡
            'avg_digg': avg_digg,
            'avg_comment': avg_comment,
            'avg_share': avg_share,
            'avg_collect': avg_collect,
            'avg_play': avg_play,

            # æ•ˆç‡æŒ‡æ ‡
            'like_comment_ratio': like_comment_ratio,
            'interaction_rate': interaction_rate,

            # å†…å®¹æŒ‡æ ‡
            'avg_duration': avg_duration,
            'hashtag_ratio': has_hashtag / total_videos if total_videos > 0 else 0,
            'mention_ratio': has_mention / total_videos if total_videos > 0 else 0
        }

    # 4. åˆ†ææŒ‡æ ‡åˆ†å¸ƒ
    print("ğŸ“Š è¾¾äººèƒ½åŠ›æŒ‡æ ‡åˆ†å¸ƒåˆ†æ:")
    print("-" * 50)

    # ä½œå“æ•°åˆ†å¸ƒ
    video_counts = [stats['video_count'] for stats in author_stats.values()]
    print("ä½œå“æ•°åˆ†å¸ƒ:")
    count_ranges = {
        '1ä¸ªè§†é¢‘': len([c for c in video_counts if c == 1]),
        '2ä¸ªè§†é¢‘': len([c for c in video_counts if c == 2]),
        '3ä¸ªè§†é¢‘': len([c for c in video_counts if c == 3])
    }
    for range_name, count in count_ranges.items():
        pct = (count / len(video_counts)) * 100
        print("10")

    print()

    # å¹³å‡ç‚¹èµæ•°åˆ†å¸ƒ
    avg_diggs = [stats['avg_digg'] for stats in author_stats.values()]
    if avg_diggs:
        print("å¹³å‡ç‚¹èµæ•°åˆ†å¸ƒ:")
        digg_ranges = {
            '< 100': len([d for d in avg_diggs if d < 100]),
            '100-500': len([d for d in avg_diggs if 100 <= d < 500]),
            '500-1000': len([d for d in avg_diggs if 500 <= d < 1000]),
            '1000-5000': len([d for d in avg_diggs if 1000 <= d < 5000]),
            '> 5000': len([d for d in avg_diggs if d >= 5000])
        }
        for range_name, count in digg_ranges.items():
            pct = (count / len(avg_diggs)) * 100
            print("10")
    print()

    # èµè¯„æ¯”åˆ†å¸ƒ
    like_comment_ratios = [stats['like_comment_ratio'] for stats in author_stats.values()]
    if like_comment_ratios:
        print("èµè¯„æ¯”åˆ†å¸ƒ:")
        ratio_ranges = {
            '< 10': len([r for r in like_comment_ratios if r < 10]),
            '10-50': len([r for r in like_comment_ratios if 10 <= r < 50]),
            '50-100': len([r for r in like_comment_ratios if 50 <= r < 100]),
            '100-500': len([r for r in like_comment_ratios if 100 <= r < 500]),
            '> 500': len([r for r in like_comment_ratios if r >= 500])
        }
        for range_name, count in ratio_ranges.items():
            pct = (count / len(like_comment_ratios)) * 100
            print("10")
    print()

    # äº’åŠ¨ç‡åˆ†å¸ƒ
    interaction_rates = [stats['interaction_rate'] for stats in author_stats.values()]
    if interaction_rates:
        print("äº’åŠ¨ç‡åˆ†å¸ƒ:")
        interact_ranges = {
            '< 0.01': len([r for r in interaction_rates if r < 0.01]),
            '0.01-0.05': len([r for r in interaction_rates if 0.01 <= r < 0.05]),
            '0.05-0.1': len([r for r in interaction_rates if 0.05 <= r < 0.1]),
            '0.1-0.2': len([r for r in interaction_rates if 0.1 <= r < 0.2]),
            '> 0.2': len([r for r in interaction_rates if r >= 0.2])
        }
        for range_name, count in interact_ranges.items():
            pct = (count / len(interaction_rates)) * 100
            print("10")
    print()

    # 5. è¯†åˆ«é«˜èƒ½åŠ›è¾¾äºº
    print("ğŸ† é«˜èƒ½åŠ›è¾¾äººè¯†åˆ«:")
    print("-" * 50)

    # æŒ‰ä¸åŒæŒ‡æ ‡æ’åº
    top_by_digg = sorted(author_stats.items(), key=lambda x: x[1]['avg_digg'], reverse=True)[:10]
    top_by_ratio = sorted(author_stats.items(), key=lambda x: x[1]['like_comment_ratio'], reverse=True)[:10]
    top_by_interaction = sorted(author_stats.items(), key=lambda x: x[1]['interaction_rate'], reverse=True)[:10]

    print("å¹³å‡ç‚¹èµæ•° TOP 10:")
    for i, (author_key, stats) in enumerate(top_by_digg, 1):
        print("5")

    print("\nèµè¯„æ¯” TOP 10:")
    for i, (author_key, stats) in enumerate(top_by_ratio, 1):
        print("5")

    print("\näº’åŠ¨ç‡ TOP 10:")
    for i, (author_key, stats) in enumerate(top_by_interaction, 1):
        print("5")

    print()

    # 6. è¯„ä¼°å¸¦è´§èƒ½åŠ›æŒ‡æ ‡è¯´æ˜
    print("ğŸ’¡ è¾¾äººå¸¦è´§èƒ½åŠ›è¯„ä¼°æŒ‡æ ‡è¯´æ˜:")
    print("-" * 50)
    print("1. åŸºç¡€æŒ‡æ ‡:")
    print("   â€¢ ç²‰ä¸é‡çº§: 10ä¸‡-100ä¸‡ä¸ºè…°éƒ¨è¾¾äºº")
    print("   â€¢ ä½œå“æ•°é‡: ç¨³å®šçš„å†…å®¹è¾“å‡ºèƒ½åŠ›")
    print("   â€¢ å¹³å‡æ’­æ”¾é‡: å†…å®¹ä¼ æ’­å¹¿åº¦")
    print()
    print("2. äº’åŠ¨æ•ˆç‡æŒ‡æ ‡:")
    print("   â€¢ èµè¯„æ¯”: ç‚¹èµ/è¯„è®º, >50ä¸ºä¼˜è´¨ï¼Œ>100ä¸ºä¼˜ç§€")
    print("   â€¢ äº’åŠ¨ç‡: (è¯„è®º+åˆ†äº«+æ”¶è—)/ç‚¹èµ, >0.05ä¸ºæ´»è·ƒ")
    print("   â€¢ åˆ†äº«æ”¶è—æ¯”: å†…å®¹ä¼ æ’­æ„æ„¿")
    print()
    print("3. å†…å®¹è´¨é‡æŒ‡æ ‡:")
    print("   â€¢ è§†é¢‘æ—¶é•¿: æŠ¤è‚¤å†…å®¹é€šå¸¸15-60ç§’æœ€ä½³")
    print("   â€¢ è¯é¢˜æ ‡ç­¾ä½¿ç”¨ç‡: å†…å®¹è¥é”€èƒ½åŠ›")
    print("   â€¢ @æåŠç‡: å“ç‰Œåˆä½œæ„æ„¿")
    print()
    print("4. å•†ä¸šåŒ–æ½œåŠ›:")
    print("   â€¢ é«˜èµè¯„æ¯” + é«˜äº’åŠ¨ç‡ = å†…å®¹è´¨é‡å¥½")
    print("   â€¢ é«˜å¹³å‡ç‚¹èµ = ç²‰ä¸åŸºç¡€æ‰å®")
    print("   â€¢ ç¨³å®šçš„å†…å®¹è¾“å‡º = å•†ä¸šåˆä½œå¯é æ€§")
    print()

    # 7. ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
    capability_analysis = {
        'summary': {
            'total_authors': len(author_stats),
            'total_videos': len(videos),
            'avg_videos_per_author': len(videos) / len(author_stats),
            'avg_like_comment_ratio': statistics.mean(like_comment_ratios) if like_comment_ratios else 0,
            'avg_interaction_rate': statistics.mean(interaction_rates) if interaction_rates else 0
        },
        'author_stats': author_stats,
        'top_performers': {
            'by_avg_digg': top_by_digg,
            'by_like_comment_ratio': top_by_ratio,
            'by_interaction_rate': top_by_interaction
        },
        'distribution_analysis': {
            'video_counts': count_ranges,
            'digg_ranges': digg_ranges if 'digg_ranges' in locals() else {},
            'ratio_ranges': ratio_ranges if 'ratio_ranges' in locals() else {},
            'interaction_ranges': interact_ranges if 'interact_ranges' in locals() else {}
        }
    }

    report_file = output_dir / "kol_capability_analysis.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(capability_analysis, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ è¯¦ç»†èƒ½åŠ›åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print("=" * 70)
    print("âœ… è¾¾äººèƒ½åŠ›åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    analyze_kol_capability_metrics()

