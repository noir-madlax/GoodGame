#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¾¾äººè§†é¢‘é‡åŒ–æŒ‡æ ‡åˆ†ææ¡†æ¶

ä¸“ä¸šåˆ†ææŠ–éŸ³/å°çº¢ä¹¦ç­‰å¹³å°è¾¾äººè§†é¢‘çš„é‡åŒ–æŒ‡æ ‡ï¼Œ
åŸºäºè¡Œä¸šæ ‡å‡†å’ŒTikTokå®˜æ–¹æ•°æ®åˆ†ææ–¹æ³•ã€‚
"""

import json
import os
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Any, Tuple
import statistics
import math

def calculate_quantitative_metrics():
    """è®¡ç®—ä¸“ä¸šçš„è¾¾äººè§†é¢‘é‡åŒ–æŒ‡æ ‡"""

    output_dir = Path(__file__).parent / "output"
    data_file = Path(__file__).parent.parent / "kol-video-fetcher" / "output" / "final_video_details.json"

    if not data_file.exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return

    print("ğŸ“Š å¼€å§‹è®¡ç®—è¾¾äººè§†é¢‘é‡åŒ–æŒ‡æ ‡...")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    with open(data_file, 'r', encoding='utf-8') as f:
        videos = json.load(f)

    print(f"ğŸ“Š åˆ†æ {len(videos)} ä¸ªè§†é¢‘æ•°æ®")
    print()

    # 2. æŒ‰ä½œè€…åˆ†ç»„
    author_videos = defaultdict(list)
    for video in videos:
        author = video.get('author', {})
        if isinstance(author, dict):
            uid = author.get('uid', 'unknown')
            nickname = author.get('nickname', 'unknown')
            author_key = f"{uid}_{nickname}"
        else:
            author_key = str(author)
        author_videos[author_key].append(video)

    print(f"ğŸ‘¥ æ¶‰åŠ {len(author_videos)} ä½è¾¾äºº")
    print()

    # 3. è®¡ç®—æ¯ä¸ªè¾¾äººçš„é‡åŒ–æŒ‡æ ‡
    kol_metrics = {}

    for author_key, video_list in author_videos.items():
        if '_' in author_key:
            uid, nickname = author_key.split('_', 1)
        else:
            uid, nickname = 'unknown', author_key

        # åŸºç¡€ç»Ÿè®¡æ•°æ®
        stats = calculate_author_statistics(video_list)

        # è®¡ç®—ä¸“ä¸šé‡åŒ–æŒ‡æ ‡
        metrics = calculate_professional_metrics(stats)

        kol_metrics[author_key] = {
            'basic_info': {
                'uid': uid,
                'nickname': nickname,
                'video_count': len(video_list)
            },
            'statistics': stats,
            'quantitative_metrics': metrics
        }

    # 4. è¾“å‡ºé‡åŒ–æŒ‡æ ‡æ¡†æ¶
    print("ğŸ¯ è¾¾äººè§†é¢‘é‡åŒ–åˆ†ææŒ‡æ ‡æ¡†æ¶")
    print("=" * 80)

    print("ğŸ“ˆ ä¸€çº§æŒ‡æ ‡ï¼ˆæ ¸å¿ƒè¯„ä¼°ï¼‰:")
    print("-" * 60)

    # å±•ç¤ºå‡ ä¸ªç¤ºä¾‹è¾¾äººçš„æŒ‡æ ‡
    sample_kols = list(kol_metrics.keys())[:3]

    for author_key in sample_kols:
        kol = kol_metrics[author_key]
        metrics = kol['quantitative_metrics']
        basic = kol['basic_info']

        print(f"\nğŸ‘¤ {basic['nickname'][:20]} ({basic['video_count']}ä¸ªè§†é¢‘)")
        print(".2f")
        print(".1f")
        print(".1f")
        print(".1f")
        print(".3f")
        print(".1f")
    print()

    # 5. æŒ‡æ ‡åˆ†å¸ƒåˆ†æ
    print("ğŸ“Š å…¨é‡æ•°æ®æŒ‡æ ‡åˆ†å¸ƒåˆ†æ:")
    print("-" * 60)

    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡å€¼
    all_like_comment_ratios = []
    all_interaction_rates = []
    all_engagement_rates = []
    all_content_quality_scores = []
    all_virality_scores = []

    for kol in kol_metrics.values():
        m = kol['quantitative_metrics']
        all_like_comment_ratios.append(m['like_comment_ratio'])
        all_interaction_rates.append(m['interaction_rate'])
        all_engagement_rates.append(m['engagement_rate'])
        all_content_quality_scores.append(m['content_quality_score'])
        all_virality_scores.append(m['virality_score'])

    # èµè¯„æ¯”åˆ†å¸ƒ
    print("èµè¯„æ¯”åˆ†å¸ƒ:")
    ratio_ranges = analyze_distribution(all_like_comment_ratios,
                                      [0, 10, 30, 50, 100, 200, float('inf')],
                                      ['<10', '10-30', '30-50', '50-100', '100-200', '>200'])
    for range_name, count in ratio_ranges.items():
        pct = (count / len(all_like_comment_ratios)) * 100
        print("10")

    # å†…å®¹è´¨é‡åˆ†æ•°åˆ†å¸ƒ
    print("\nå†…å®¹è´¨é‡åˆ†æ•°åˆ†å¸ƒ:")
    quality_ranges = analyze_distribution(all_content_quality_scores,
                                        [0, 0.3, 0.5, 0.7, 0.9, 1.0],
                                        ['<0.3', '0.3-0.5', '0.5-0.7', '0.7-0.9', '0.9-1.0'])
    for range_name, count in quality_ranges.items():
        pct = (count / len(all_content_quality_scores)) * 100
        print("10")

    # 6. ä¸“ä¸šåˆ†ææŒ‡æ ‡è¯´æ˜
    print("\nğŸ’¡ ä¸“ä¸šé‡åŒ–æŒ‡æ ‡è¯¦è§£:")
    print("-" * 60)

    metrics_explanation = {
        "1. èµè¯„æ¯” (Like-Comment Ratio)": {
            "è®¡ç®—å…¬å¼": "èµè¯„æ¯” = æ€»ç‚¹èµæ•° / æ€»è¯„è®ºæ•°",
            "æ„ä¹‰": "è¡¡é‡ç²‰ä¸å¯¹å†…å®¹çš„è®¤åŒåº¦ï¼Œé¿å…åˆ·èµåˆ·è¯„",
            "ä¼˜ç§€æ ‡å‡†": ">50 (é«˜è´¨é‡å†…å®¹), >100 (ç²¾å“å†…å®¹)",
            "è¡Œä¸šå‚è€ƒ": "å¤´éƒ¨è¾¾äººé€šå¸¸>100, è…°éƒ¨è¾¾äºº>30"
        },

        "2. äº’åŠ¨ç‡ (Interaction Rate)": {
            "è®¡ç®—å…¬å¼": "äº’åŠ¨ç‡ = (è¯„è®º+åˆ†äº«+æ”¶è—) / ç‚¹èµæ•°",
            "æ„ä¹‰": "è¡¡é‡ç²‰ä¸å‚ä¸åº¦ï¼Œå†…å®¹ä¼ æ’­æ„æ„¿",
            "ä¼˜ç§€æ ‡å‡†": ">0.08 (8%), è¡¨ç¤ºå¼ºäº’åŠ¨",
            "è¡Œä¸šå‚è€ƒ": "ä¼˜è´¨å†…å®¹é€šå¸¸>0.05, ç—…æ¯’å†…å®¹>0.15"
        },

        "3. ç»¼åˆå‚ä¸åº¦ (Engagement Rate)": {
            "è®¡ç®—å…¬å¼": "ç»¼åˆå‚ä¸åº¦ = (è¯„è®º+åˆ†äº«+æ”¶è—+è½¬å‘) / ç‚¹èµæ•°",
            "æ„ä¹‰": "å…¨æ–¹ä½è¡¡é‡å†…å®¹äº’åŠ¨æ°´å¹³",
            "ä¼˜ç§€æ ‡å‡†": ">0.10 (10%), å¼ºäº’åŠ¨å†…å®¹",
            "è¡Œä¸šå‚è€ƒ": "å“ç‰Œåˆä½œè¾¾äººé€šå¸¸>0.08"
        },

        "4. å†…å®¹è´¨é‡åˆ†æ•° (Content Quality Score)": {
            "è®¡ç®—å…¬å¼": "å†…å®¹è´¨é‡åˆ†æ•° = (èµè¯„æ¯”/100 * 0.4) + (äº’åŠ¨ç‡/0.1 * 0.3) + (åˆ†äº«ç‡/0.02 * 0.3)",
            "æ„ä¹‰": "ç»¼åˆè¯„ä¼°å†…å®¹è´¨é‡çš„é‡åŒ–åˆ†æ•°",
            "ä¼˜ç§€æ ‡å‡†": ">0.7 (ä¼˜è´¨å†…å®¹)",
            "è¡Œä¸šå‚è€ƒ": "0.8+ ä¸ºå¤´éƒ¨å†…å®¹è´¨é‡"
        },

        "5. ä¼ æ’­åŠ›åˆ†æ•° (Virality Score)": {
            "è®¡ç®—å…¬å¼": "ä¼ æ’­åŠ›åˆ†æ•° = (åˆ†äº«æ•°/ç‚¹èµæ•°) * (æ”¶è—æ•°/ç‚¹èµæ•°) * 100",
            "æ„ä¹‰": "è¡¡é‡å†…å®¹çš„è‡ªä¼ æ’­èƒ½åŠ›",
            "ä¼˜ç§€æ ‡å‡†": ">2.0 (å¼ºä¼ æ’­)",
            "è¡Œä¸šå‚è€ƒ": "ç—…æ¯’å†…å®¹é€šå¸¸>5.0"
        },

        "6. å¹³å‡ç‚¹èµæ•° (Average Digg Count)": {
            "è®¡ç®—å…¬å¼": "å¹³å‡ç‚¹èµæ•° = æ€»ç‚¹èµæ•° / è§†é¢‘æ•°é‡",
            "æ„ä¹‰": "è¡¡é‡è¾¾äººåŸºç¡€ç²‰ä¸è´¨é‡",
            "ä¼˜ç§€æ ‡å‡†": ">1000 (è…°éƒ¨è¾¾äºº), >5000 (å¤´éƒ¨è¾¾äºº)",
            "è¡Œä¸šå‚è€ƒ": "æŠ¤è‚¤é¢†åŸŸè…°éƒ¨è¾¾äººé€šå¸¸1000-5000"
        },

        "7. åˆ†äº«å æ¯” (Share Ratio)": {
            "è®¡ç®—å…¬å¼": "åˆ†äº«å æ¯” = åˆ†äº«æ•° / ç‚¹èµæ•°",
            "æ„ä¹‰": "è¡¡é‡å†…å®¹çš„åˆ†äº«æ„æ„¿ï¼Œå£ç¢‘ä¼ æ’­èƒ½åŠ›",
            "ä¼˜ç§€æ ‡å‡†": ">0.02 (2%), æ˜“äºåˆ†äº«",
            "è¡Œä¸šå‚è€ƒ": "å®ç”¨å†…å®¹åˆ†äº«ç‡æ›´é«˜"
        },

        "8. æ”¶è—å æ¯” (Collect Ratio)": {
            "è®¡ç®—å…¬å¼": "æ”¶è—å æ¯” = æ”¶è—æ•° / ç‚¹èµæ•°",
            "æ„ä¹‰": "è¡¡é‡å†…å®¹çš„æ”¶è—ä»·å€¼ï¼Œå®ç”¨æ€§",
            "ä¼˜ç§€æ ‡å‡†": ">0.05 (5%), é«˜ä»·å€¼å†…å®¹",
            "è¡Œä¸šå‚è€ƒ": "æ•™ç¨‹ç±»å†…å®¹æ”¶è—ç‡æ›´é«˜"
        }
    }

    for metric_name, details in metrics_explanation.items():
        print(f"\n{metric_name}")
        print(f"  å…¬å¼: {details['è®¡ç®—å…¬å¼']}")
        print(f"  æ„ä¹‰: {details['æ„ä¹‰']}")
        print(f"  æ ‡å‡†: {details['ä¼˜ç§€æ ‡å‡†']}")
        print(f"  å‚è€ƒ: {details['è¡Œä¸šå‚è€ƒ']}")

    # 7. ä¿å­˜é‡åŒ–åˆ†æç»“æœ
    quantitative_analysis = {
        'summary': {
            'total_kols': len(kol_metrics),
            'total_videos': len(videos),
            'avg_videos_per_kol': len(videos) / len(kol_metrics),
            'metrics_distribution': {
                'like_comment_ratio': analyze_distribution(all_like_comment_ratios),
                'interaction_rate': analyze_distribution(all_interaction_rates),
                'engagement_rate': analyze_distribution(all_engagement_rates),
                'content_quality_score': analyze_distribution(all_content_quality_scores),
                'virality_score': analyze_distribution(all_virality_scores)
            }
        },
        'kol_quantitative_metrics': kol_metrics,
        'metrics_framework': metrics_explanation
    }

    report_file = output_dir / "quantitative_metrics_analysis.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(quantitative_analysis, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ é‡åŒ–æŒ‡æ ‡åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print("=" * 80)
    print("âœ… é‡åŒ–æŒ‡æ ‡åˆ†æå®Œæˆ!")

def calculate_author_statistics(video_list: List[Dict]) -> Dict[str, Any]:
    """è®¡ç®—è¾¾äººçš„åŸºç¡€ç»Ÿè®¡æ•°æ®"""
    total_digg = sum(v.get('statistics', {}).get('digg_count', 0) for v in video_list)
    total_comment = sum(v.get('statistics', {}).get('comment_count', 0) for v in video_list)
    total_share = sum(v.get('statistics', {}).get('share_count', 0) for v in video_list)
    total_collect = sum(v.get('statistics', {}).get('collect_count', 0) for v in video_list)
    total_forward = sum(v.get('statistics', {}).get('forward_count', 0) for v in video_list)

    video_count = len(video_list)

    return {
        'total_digg': total_digg,
        'total_comment': total_comment,
        'total_share': total_share,
        'total_collect': total_collect,
        'total_forward': total_forward,
        'video_count': video_count,
        'avg_digg': total_digg / video_count if video_count > 0 else 0,
        'avg_comment': total_comment / video_count if video_count > 0 else 0,
        'avg_share': total_share / video_count if video_count > 0 else 0,
        'avg_collect': total_collect / video_count if video_count > 0 else 0
    }

def calculate_professional_metrics(stats: Dict[str, Any]) -> Dict[str, Any]:
    """è®¡ç®—ä¸“ä¸šçš„é‡åŒ–æŒ‡æ ‡"""

    # åŸºç¡€æ•°æ®
    total_digg = stats['total_digg']
    total_comment = stats['total_comment']
    total_share = stats['total_share']
    total_collect = stats['total_collect']
    total_forward = stats['total_forward']

    # é¿å…é™¤é›¶é”™è¯¯
    safe_digg = max(total_digg, 1)
    safe_comment = max(total_comment, 1)

    # 1. èµè¯„æ¯” (Like-Comment Ratio)
    like_comment_ratio = total_digg / safe_comment

    # 2. äº’åŠ¨ç‡ (Interaction Rate) - è¯„è®º+åˆ†äº«+æ”¶è—
    interaction_rate = (total_comment + total_share + total_collect) / safe_digg

    # 3. ç»¼åˆå‚ä¸åº¦ (Engagement Rate) - åŒ…å«è½¬å‘
    engagement_rate = (total_comment + total_share + total_collect + total_forward) / safe_digg

    # 4. åˆ†äº«å æ¯” (Share Ratio)
    share_ratio = total_share / safe_digg

    # 5. æ”¶è—å æ¯” (Collect Ratio)
    collect_ratio = total_collect / safe_digg

    # 6. å†…å®¹è´¨é‡åˆ†æ•° (Content Quality Score)
    # ç»¼åˆèµè¯„æ¯”ã€äº’åŠ¨ç‡ã€åˆ†äº«ç‡ä¸‰ä¸ªç»´åº¦
    quality_score = (
        min(like_comment_ratio / 100, 1.0) * 0.4 +  # èµè¯„æ¯”è´¡çŒ®40%
        min(interaction_rate / 0.1, 1.0) * 0.3 +    # äº’åŠ¨ç‡è´¡çŒ®30%
        min(share_ratio / 0.02, 1.0) * 0.3          # åˆ†äº«ç‡è´¡çŒ®30%
    )

    # 7. ä¼ æ’­åŠ›åˆ†æ•° (Virality Score)
    # åˆ†äº«æ„æ„¿ Ã— æ”¶è—æ„æ„¿
    virality_score = (share_ratio * collect_ratio) * 100

    # 8. å¹³å‡ç‚¹èµæ•°
    avg_digg = stats['avg_digg']

    return {
        'like_comment_ratio': like_comment_ratio,
        'interaction_rate': interaction_rate,
        'engagement_rate': engagement_rate,
        'share_ratio': share_ratio,
        'collect_ratio': collect_ratio,
        'content_quality_score': quality_score,
        'virality_score': virality_score,
        'avg_digg': avg_digg,

        # è´¨é‡ç­‰çº§ (åŸºäºç»¼åˆè¯„åˆ†)
        'quality_level': get_quality_level(quality_score),
        'virality_level': get_virality_level(virality_score)
    }

def get_quality_level(score: float) -> str:
    """æ ¹æ®å†…å®¹è´¨é‡åˆ†æ•°è¿”å›ç­‰çº§"""
    if score >= 0.8:
        return "Sçº§ (å¤´éƒ¨å†…å®¹)"
    elif score >= 0.7:
        return "Açº§ (ä¼˜è´¨å†…å®¹)"
    elif score >= 0.5:
        return "Bçº§ (è‰¯å¥½å†…å®¹)"
    elif score >= 0.3:
        return "Cçº§ (ä¸€èˆ¬å†…å®¹)"
    else:
        return "Dçº§ (éœ€æ”¹è¿›)"

def get_virality_level(score: float) -> str:
    """æ ¹æ®ä¼ æ’­åŠ›åˆ†æ•°è¿”å›ç­‰çº§"""
    if score >= 10.0:
        return "ç—…æ¯’çº§ (æå¼ºä¼ æ’­)"
    elif score >= 5.0:
        return "ä¼˜ç§€çº§ (å¼ºä¼ æ’­)"
    elif score >= 2.0:
        return "è‰¯å¥½çº§ (ä¸­ä¼ æ’­)"
    elif score >= 0.5:
        return "ä¸€èˆ¬çº§ (å¼±ä¼ æ’­)"
    else:
        return "ä½ä¼ æ’­"

def analyze_distribution(values: List[float], bins: List[float] = None,
                        labels: List[str] = None) -> Dict[str, int]:
    """åˆ†ææ•°å€¼åˆ†å¸ƒ"""
    if bins is None:
        # é»˜è®¤åˆ†ä½æ•°åˆ†æ
        if len(values) >= 10:
            values.sort()
            bins = [
                0,
                values[int(len(values) * 0.25)],  # Q1
                values[int(len(values) * 0.5)],   # Q2
                values[int(len(values) * 0.75)],  # Q3
                float('inf')
            ]
            labels = ['Q1ä»¥ä¸‹', 'Q1-Q2', 'Q2-Q3', 'Q3ä»¥ä¸Š']
        else:
            return {}

    if labels is None:
        labels = [f'{bins[i]:.1f}-{bins[i+1]:.1f}' for i in range(len(bins)-1)]

    distribution = defaultdict(int)
    for value in values:
        for i, bin_edge in enumerate(bins[:-1]):
            if value <= bins[i+1]:
                distribution[labels[i]] += 1
                break

    return dict(distribution)

if __name__ == "__main__":
    calculate_quantitative_metrics()

