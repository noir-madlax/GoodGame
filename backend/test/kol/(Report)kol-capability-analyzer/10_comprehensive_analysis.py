#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„è§†é¢‘æ•°æ®é‡åŒ–åˆ†æ - åŒ…å«æ‰€æœ‰æ ¸å¿ƒæŒ‡æ ‡

åŠŸèƒ½ï¼š
1. æ•°æ®æ¦‚å†µç»Ÿè®¡ï¼ˆKOLæ•°é‡ã€è§†é¢‘æ•°é‡ã€è§†é¢‘ç±»å‹åˆ†å¸ƒã€å¹³å‡æŒ‡æ ‡ï¼‰
2. æ ¸å¿ƒé‡åŒ–æŒ‡æ ‡åˆ†å¸ƒï¼ˆæ’­æ”¾é‡ã€ç‚¹èµã€è¯„è®ºã€åˆ†äº«ã€è½¬å‘ã€æ”¶è—ã€èµè¯„æ¯”ç­‰ï¼‰
3. çƒ­é—¨è§†é¢‘çƒ­åº¦è¯„ä¼°æŒ‡æ ‡ï¼ˆå®Œæ’­ç‡æ›¿ä»£æŒ‡æ ‡ã€äº’åŠ¨ç‡ã€ä¼ æ’­åŠ›ç­‰ï¼‰
"""

import json
import os
from pathlib import Path
from collections import defaultdict, Counter
import statistics

def comprehensive_analysis():
    base_dir = Path(__file__).parent.parent / "kol-video-fetcher" / "output"
    ids_file = Path(__file__).parent.parent / "kol-video-fetcher" / "output" / "kol_video_ids.json"
    details_file = base_dir / "final_video_details.json"
    
    if not ids_file.exists() or not details_file.exists():
        print("âŒ ç¼ºå°‘æ•°æ®æ–‡ä»¶")
        return

    # 1. åŠ è½½æ•°æ®
    with open(ids_file, 'r', encoding='utf-8') as f:
        kol_structure = json.load(f)
        
    with open(details_file, 'r', encoding='utf-8') as f:
        video_details_list = json.load(f)
    
    video_map = {v['aweme_id']: v for v in video_details_list if v.get('aweme_id')}
    
    # 2. æ•´åˆæ•°æ®
    merged_data = []
    kol_count = 0
    video_type_count = Counter()
    
    for kol in kol_structure:
        kol_name = kol.get('kol_name', 'Unknown')
        video_details_db = kol.get('video_details', {})
        has_video = False
        
        for v_type, details_db in video_details_db.items():
            if not details_db: continue
            
            vid = details_db.get('item_id')
            if not vid: continue
            
            api_details = video_map.get(vid)
            if not api_details: continue
            
            has_video = True
            video_type_count[v_type] += 1
            
            # æ’­æ”¾é‡
            play_count = details_db.get('vv', 0)
            
            # äº’åŠ¨æ•°æ®
            stats = api_details.get('statistics', {})
            digg = stats.get('digg_count', 0)
            comment = stats.get('comment_count', 0)
            share = stats.get('share_count', 0)
            collect = stats.get('collect_count', 0)
            forward = stats.get('forward_count', 0)
            
            # è§†é¢‘æ—¶é•¿ (æ¯«ç§’)
            raw_data = api_details.get('raw_video_data', {})
            duration_ms = raw_data.get('duration', 0)
            duration_sec = duration_ms / 1000 if duration_ms else 0
            
            # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
            safe_digg = max(digg, 1)
            safe_play = max(play_count, 1)
            
            # èµè¯„æ¯”
            lc_ratio = digg / max(comment, 1)
            
            # äº’åŠ¨ç‡ (åŸºäºç‚¹èµ)
            interaction_rate = (comment + share + collect) / safe_digg
            
            # ç»¼åˆå‚ä¸åº¦
            engagement_rate = (comment + share + collect + forward) / safe_digg
            
            # åˆ†äº«å æ¯”
            share_ratio = share / safe_digg
            
            # æ”¶è—å æ¯”
            collect_ratio = collect / safe_digg
            
            # è½¬å‘å æ¯”
            forward_ratio = forward / safe_digg
            
            # çƒ­åº¦æŒ‡æ•° (åŸºäºæ’­æ”¾é‡çš„äº’åŠ¨è½¬åŒ–)
            # çƒ­åº¦æŒ‡æ•° = (æ€»äº’åŠ¨ / æ’­æ”¾é‡) * 1000
            total_interaction = digg + comment + share + collect + forward
            heat_index = (total_interaction / safe_play) * 1000
            
            # ä¼ æ’­åŠ›æŒ‡æ•° (åˆ†äº«+è½¬å‘ç›¸å¯¹äºç‚¹èµ)
            virality_score = (share + forward) / safe_digg * 100
            
            # å®Œæ’­ç‡æ›¿ä»£æŒ‡æ ‡: æ’­èµæ¯” (æ’­æ”¾é‡/ç‚¹èµæ•°)
            # é€šå¸¸ï¼Œå®Œæ’­ç‡é«˜çš„è§†é¢‘ï¼Œç‚¹èµè½¬åŒ–ç‡ä¹Ÿé«˜ï¼Œå³æ’­èµæ¯”ä½ï¼ˆ10-30ä¹‹é—´è¾ƒå¥½ï¼‰
            play_like_ratio = play_count / safe_digg
            
            merged_data.append({
                'kol_name': kol_name,
                'video_type': v_type,
                'play_count': play_count,
                'digg': digg,
                'comment': comment,
                'share': share,
                'collect': collect,
                'forward': forward,
                'duration_sec': duration_sec,
                'lc_ratio': lc_ratio,
                'interaction_rate': interaction_rate,
                'engagement_rate': engagement_rate,
                'share_ratio': share_ratio,
                'collect_ratio': collect_ratio,
                'forward_ratio': forward_ratio,
                'heat_index': heat_index,
                'virality_score': virality_score,
                'play_like_ratio': play_like_ratio
            })
            
        if has_video:
            kol_count += 1
            
    total_videos = len(merged_data)
    
    print("=" * 80)
    print("æŠ¤è‚¤å‚ç±»è¾¾äººè§†é¢‘æ•°æ®æ·±åº¦é‡åŒ–åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    
    # ===== 1. æ•°æ®æ¦‚å†µ =====
    print("\n### 1. æ•°æ®æ¦‚å†µ")
    print(f"\n**æ ·æœ¬è§„æ¨¡**:")
    print(f"- KOL æ•°é‡: {kol_count} ä½")
    print(f"- è§†é¢‘æ ·æœ¬æ€»æ•°: {total_videos} æ¡")
    print(f"- å¹³å‡æ¯ä½ KOL è§†é¢‘æ•°: {total_videos/kol_count:.2f} æ¡")
    
    print(f"\n**è§†é¢‘ç±»å‹åˆ†å¸ƒ**:")
    type_mapping = {
        'masterpiece': 'çˆ†æ¬¾è§†é¢‘ (Tag 3)',
        'hot': 'çƒ­é—¨è§†é¢‘ (Tag 5)',
        'newest': 'æœ€æ–°è§†é¢‘ (Tag 6)'
    }
    for v_type in ['masterpiece', 'hot', 'newest']:
        count = video_type_count.get(v_type, 0)
        pct = count / total_videos * 100
        print(f"- {type_mapping[v_type]}: {count} æ¡ ({pct:.1f}%)")
    
    print(f"\n**æ ¸å¿ƒæ•°æ®æŒ‡æ ‡ï¼ˆå¹³å‡å€¼ï¼‰**:")
    avg_play = statistics.mean([d['play_count'] for d in merged_data])
    avg_digg = statistics.mean([d['digg'] for d in merged_data])
    avg_comment = statistics.mean([d['comment'] for d in merged_data])
    avg_share = statistics.mean([d['share'] for d in merged_data])
    avg_collect = statistics.mean([d['collect'] for d in merged_data])
    avg_forward = statistics.mean([d['forward'] for d in merged_data])
    avg_duration = statistics.mean([d['duration_sec'] for d in merged_data if d['duration_sec'] > 0])
    
    print(f"- å¹³å‡æ’­æ”¾é‡: {avg_play:,.0f}")
    print(f"- å¹³å‡ç‚¹èµæ•°: {avg_digg:,.0f}")
    print(f"- å¹³å‡è¯„è®ºæ•°: {avg_comment:,.0f}")
    print(f"- å¹³å‡åˆ†äº«æ•°: {avg_share:,.0f}")
    print(f"- å¹³å‡æ”¶è—æ•°: {avg_collect:,.0f}")
    print(f"- å¹³å‡è½¬å‘æ•°: {avg_forward:,.0f}")
    print(f"- å¹³å‡è§†é¢‘æ—¶é•¿: {avg_duration:.1f} ç§’")
    
    # ===== 2. æ ¸å¿ƒé‡åŒ–æŒ‡æ ‡åˆ†å¸ƒ =====
    print("\n### 2. æ ¸å¿ƒé‡åŒ–æŒ‡æ ‡åˆ†å¸ƒ")
    
    def print_dist(name, key, bins, labels):
        values = [d[key] for d in merged_data]
        counts = [0] * len(labels)
        for v in values:
            for i, b in enumerate(bins[:-1]):
                if bins[i] <= v < bins[i+1]:
                    counts[i] += 1
                    break
                elif i == len(bins) - 2 and v >= bins[i+1]:
                    counts[i] += 1
                    break
        
        print(f"\n**{name}**:")
        for i, label in enumerate(labels):
            pct = counts[i] / total_videos * 100
            print(f"- {label}: {counts[i]} æ¡ ({pct:.1f}%)")
            
    # æ’­æ”¾é‡
    print_dist("æ’­æ”¾é‡åˆ†å¸ƒ", "play_count", 
               [0, 10000, 100000, 500000, 1000000, float('inf')],
               ['< 1ä¸‡', '1ä¸‡ - 10ä¸‡', '10ä¸‡ - 50ä¸‡', '50ä¸‡ - 100ä¸‡', '> 100ä¸‡'])
               
    # ç‚¹èµæ•°
    print_dist("ç‚¹èµæ•°åˆ†å¸ƒ", "digg",
               [0, 100, 1000, 10000, 100000, float('inf')],
               ['< 100', '100 - 1åƒ', '1åƒ - 1ä¸‡', '1ä¸‡ - 10ä¸‡', '> 10ä¸‡'])
               
    # è¯„è®ºæ•°
    print_dist("è¯„è®ºæ•°åˆ†å¸ƒ", "comment",
               [0, 10, 100, 500, 1000, float('inf')],
               ['< 10', '10 - 100', '100 - 500', '500 - 1åƒ', '> 1åƒ'])
               
    # åˆ†äº«æ•°
    print_dist("åˆ†äº«æ•°åˆ†å¸ƒ", "share",
               [0, 1, 10, 50, 100, float('inf')],
               ['0', '1 - 10', '10 - 50', '50 - 100', '> 100'])
               
    # è½¬å‘æ•°
    print_dist("è½¬å‘æ•°åˆ†å¸ƒ", "forward",
               [0, 1, 10, 50, 100, float('inf')],
               ['0', '1 - 10', '10 - 50', '50 - 100', '> 100'])
               
    # æ”¶è—æ•°
    print_dist("æ”¶è—æ•°åˆ†å¸ƒ", "collect",
               [0, 10, 100, 1000, 10000, float('inf')],
               ['< 10', '10 - 100', '100 - 1åƒ', '1åƒ - 1ä¸‡', '> 1ä¸‡'])
               
    # èµè¯„æ¯”
    print_dist("èµè¯„æ¯”åˆ†å¸ƒ", "lc_ratio",
               [0, 10, 30, 50, 100, float('inf')],
               ['< 10', '10 - 30', '30 - 50', '50 - 100', '> 100'])
               
    # æ”¶è—å æ¯”
    print_dist("æ”¶è—å æ¯”åˆ†å¸ƒ (æ”¶è—/ç‚¹èµ)", "collect_ratio",
               [0, 0.01, 0.05, 0.1, float('inf')],
               ['< 1%', '1% - 5%', '5% - 10%', '> 10%'])
    
    # ===== 3. è§†é¢‘çƒ­åº¦è¯„ä¼°æŒ‡æ ‡åˆ†å¸ƒ =====
    print("\n### 3. è§†é¢‘çƒ­åº¦è¯„ä¼°æŒ‡æ ‡åˆ†å¸ƒ")
    
    # çƒ­åº¦æŒ‡æ•° (åŸºäºæ’­æ”¾é‡çš„äº’åŠ¨è½¬åŒ–ç‡)
    print_dist("çƒ­åº¦æŒ‡æ•°åˆ†å¸ƒ (äº’åŠ¨/æ’­æ”¾é‡*1000)", "heat_index",
               [0, 1, 5, 10, 50, float('inf')],
               ['< 1 (å†·é—¨)', '1 - 5 (ä¸€èˆ¬)', '5 - 10 (çƒ­é—¨)', '10 - 50 (çˆ†æ¬¾)', '> 50 (è¶…çº§çˆ†æ¬¾)'])
    
    # ä¼ æ’­åŠ›æŒ‡æ•°
    print_dist("ä¼ æ’­åŠ›æŒ‡æ•°åˆ†å¸ƒ ((åˆ†äº«+è½¬å‘)/ç‚¹èµ*100)", "virality_score",
               [0, 1, 5, 10, 20, float('inf')],
               ['< 1% (å¼±ä¼ æ’­)', '1% - 5% (ä¸€èˆ¬)', '5% - 10% (å¼ºä¼ æ’­)', '10% - 20% (ç—…æ¯’ä¼ æ’­)', '> 20% (è¶…å¼ºç—…æ¯’)'])
    
    # æ’­èµæ¯” (å®Œæ’­ç‡æ›¿ä»£æŒ‡æ ‡)
    print_dist("æ’­èµæ¯”åˆ†å¸ƒ (æ’­æ”¾/ç‚¹èµ) - å®Œæ’­ç‡æ›¿ä»£æŒ‡æ ‡", "play_like_ratio",
               [0, 10, 30, 100, 500, float('inf')],
               ['< 10 (æé«˜è½¬åŒ–)', '10 - 30 (ä¼˜ç§€)', '30 - 100 (è‰¯å¥½)', '100 - 500 (ä¸€èˆ¬)', '> 500 (è¾ƒå·®)'])
    
    # ===== 4. çƒ­é—¨è§†é¢‘æ’è¡Œ =====
    print("\n### 4. çƒ­é—¨è§†é¢‘é‡åŒ–åˆ†æ (Top 10 ç»¼åˆçƒ­åº¦)")
    print("\nç»¼åˆçƒ­åº¦ = æ’­æ”¾é‡ Ã— çƒ­åº¦æŒ‡æ•°ï¼Œåæ˜ ç»å¯¹å½±å“åŠ›")
    
    # è®¡ç®—ç»¼åˆçƒ­åº¦
    for d in merged_data:
        d['comprehensive_heat'] = d['play_count'] * d['heat_index']
    
    sorted_by_heat = sorted(merged_data, key=lambda x: x['comprehensive_heat'], reverse=True)[:10]
    
    print("\n| æ’å | è¾¾äºº | ç±»å‹ | æ’­æ”¾é‡ | ç‚¹èµ | è¯„è®º | æ”¶è— | èµè¯„æ¯” | çƒ­åº¦æŒ‡æ•° | ç»¼åˆçƒ­åº¦ |")
    print("|---|---|---|---|---|---|---|---|---|---|")
    for i, v in enumerate(sorted_by_heat, 1):
        print(f"| {i} | {v['kol_name'][:15]} | {v['video_type']} | {v['play_count']:,} | {v['digg']:,} | {v['comment']:,} | {v['collect']:,} | {v['lc_ratio']:.1f} | {v['heat_index']:.2f} | {v['comprehensive_heat']:,.0f} |")
    
    # ===== 5. é«˜ä»·å€¼å†…å®¹æ’è¡Œ =====
    print("\n### 5. é«˜ä»·å€¼å†…å®¹åˆ†æ (Top 10 æ”¶è—å æ¯”)")
    print("\nç­›é€‰æ¡ä»¶: æ’­æ”¾é‡ > 1ä¸‡")
    
    high_value = [v for v in merged_data if v['play_count'] > 10000]
    sorted_by_collect = sorted(high_value, key=lambda x: x['collect_ratio'], reverse=True)[:10]
    
    print("\n| æ’å | è¾¾äºº | ç±»å‹ | æ’­æ”¾é‡ | æ”¶è—æ•° | ç‚¹èµæ•° | æ”¶è—å æ¯” | èµè¯„æ¯” |")
    print("|---|---|---|---|---|---|---|---|")
    for i, v in enumerate(sorted_by_collect, 1):
        print(f"| {i} | {v['kol_name'][:15]} | {v['video_type']} | {v['play_count']:,} | {v['collect']:,} | {v['digg']:,} | {v['collect_ratio']*100:.1f}% | {v['lc_ratio']:.1f} |")
    
    # ===== 6. æŒ‡æ ‡è¯´æ˜ =====
    print("\n### 6. æ ¸å¿ƒé‡åŒ–æŒ‡æ ‡è¯´æ˜")
    
    metrics_doc = {
        "åŸºç¡€äº’åŠ¨æŒ‡æ ‡": [
            ("digg_count", "ç‚¹èµæ•°", "ç”¨æˆ·å¯¹å†…å®¹çš„åŸºæœ¬è®¤å¯"),
            ("comment_count", "è¯„è®ºæ•°", "ç”¨æˆ·çš„æ·±åº¦å‚ä¸æ„æ„¿"),
            ("share_count", "åˆ†äº«æ•°", "å†…å®¹çš„å£ç¢‘ä¼ æ’­èƒ½åŠ›"),
            ("collect_count", "æ”¶è—æ•°", "å†…å®¹çš„å®ç”¨ä»·å€¼å’Œæ”¶è—æ„æ„¿"),
            ("forward_count", "è½¬å‘æ•°", "å†…å®¹çš„äºŒæ¬¡ä¼ æ’­èƒ½åŠ›"),
        ],
        "è¡ç”Ÿæ•ˆç‡æŒ‡æ ‡": [
            ("like_comment_ratio", "èµè¯„æ¯”", "ç‚¹èµÃ·è¯„è®ºï¼Œ>50 ä¸ºé«˜è´¨é‡å†…å®¹"),
            ("interaction_rate", "äº’åŠ¨ç‡", "(è¯„è®º+åˆ†äº«+æ”¶è—)Ã·ç‚¹èµï¼Œ>0.08 ä¸ºå¼ºäº’åŠ¨"),
            ("engagement_rate", "ç»¼åˆå‚ä¸åº¦", "(è¯„è®º+åˆ†äº«+æ”¶è—+è½¬å‘)Ã·ç‚¹èµ"),
            ("share_ratio", "åˆ†äº«å æ¯”", "åˆ†äº«Ã·ç‚¹èµï¼Œ>0.02 ä¸ºæ˜“ä¼ æ’­"),
            ("collect_ratio", "æ”¶è—å æ¯”", "æ”¶è—Ã·ç‚¹èµï¼Œ>0.05 ä¸ºé«˜ä»·å€¼"),
        ],
        "çƒ­åº¦è¯„ä¼°æŒ‡æ ‡": [
            ("heat_index", "çƒ­åº¦æŒ‡æ•°", "(æ€»äº’åŠ¨Ã·æ’­æ”¾é‡)Ã—1000ï¼Œ>10 ä¸ºçˆ†æ¬¾"),
            ("virality_score", "ä¼ æ’­åŠ›æŒ‡æ•°", "(åˆ†äº«+è½¬å‘)Ã·ç‚¹èµÃ—100ï¼Œ>5% ä¸ºå¼ºä¼ æ’­"),
            ("play_like_ratio", "æ’­èµæ¯”", "æ’­æ”¾Ã·ç‚¹èµï¼Œ10-30 ä¸ºä¼˜ç§€ï¼ˆå®Œæ’­ç‡æ›¿ä»£ï¼‰"),
        ]
    }
    
    for category, metrics in metrics_doc.items():
        print(f"\n**{category}**:")
        for field, name, desc in metrics:
            print(f"- **{name}** (`{field}`): {desc}")
    
    # ä¿å­˜æ•°æ®
    report_data = {
        'summary': {
            'kol_count': kol_count,
            'total_videos': total_videos,
            'avg_videos_per_kol': total_videos / kol_count,
            'video_type_distribution': dict(video_type_count),
            'avg_metrics': {
                'play_count': avg_play,
                'digg': avg_digg,
                'comment': avg_comment,
                'share': avg_share,
                'collect': avg_collect,
                'forward': avg_forward,
                'duration_sec': avg_duration
            }
        },
        'top_videos_by_heat': sorted_by_heat,
        'high_value_videos': sorted_by_collect,
        'all_videos': merged_data
    }
    
    output_file = base_dir / "comprehensive_analysis_report.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
        
    print(f"\n\nğŸ’¾ è¯¦ç»†æ•°æ®å·²ä¿å­˜: {output_file}")

if __name__ == "__main__":
    comprehensive_analysis()

