#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆå¯¼å…¥è„šæœ¬ï¼šåˆ†æ‰¹å¯¼å…¥æ‰€æœ‰72ä½KOLç”¨æˆ·æ•°æ®åˆ° gg_douyin_user_search è¡¨
"""

import json
import os
from pathlib import Path
import requests


def load_all_kol_data():
    """åŠ è½½æ‰€æœ‰KOLæ•°æ®"""
    detail_dir = Path(__file__).parent.parent / 'detail'
    kol_data = []

    for json_file in detail_dir.glob('kol_check_*.json'):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            user_info = data.get('user_info', {})
            api_response = data.get('api_response', {})

            # æ„é€ æ•°æ®åº“è®°å½•
            record = {
                'uid': user_info.get('uid'),
                'sec_uid': user_info.get('sec_uid'),
                'nickname': user_info.get('nickname', '').replace("'", "''"),  # è½¬ä¹‰å•å¼•å·
                'follower_count': user_info.get('follower_count', 0),
                'raw_data': json.dumps(api_response).replace("'", "''"),  # è½¬ä¹‰å•å¼•å·
                'search_keyword': 'çš®è‚¤å¥½ ä¸“å®¶',
                'search_date': '2025-11-24'
            }

            kol_data.append(record)

        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {json_file.name} å¤±è´¥: {e}")
            continue

    return kol_data


def execute_sql_batch(sql_query, batch_num):
    """é€šè¿‡MCPæ‰§è¡ŒSQLæ‰¹æ¬¡"""
    print(f"ğŸ”„ æ‰§è¡Œç¬¬ {batch_num} æ‰¹...")

    # è¿™é‡Œæˆ‘ä»¬éœ€è¦é€šè¿‡HTTPè¯·æ±‚è°ƒç”¨MCP
    # ä½†ç”±äºæˆ‘ä»¬æ— æ³•ç›´æ¥è°ƒç”¨MCPï¼Œæˆ‘ä»¬å°†SQLä¿å­˜åˆ°æ–‡ä»¶
    output_file = f"batch_{batch_num}.sql"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(sql_query)

    print(f"âœ… SQLå·²ä¿å­˜åˆ° {output_file}")
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ æŠ–éŸ³ç”¨æˆ·æœç´¢æ•°æ®æ‰¹é‡å¯¼å…¥å·¥å…·")
    print("=" * 80)

    # åŠ è½½æ‰€æœ‰æ•°æ®
    print("ğŸ“Š åŠ è½½KOLæ•°æ®...")
    kol_data = load_all_kol_data()
    print(f"âœ… å…±åŠ è½½ {len(kol_data)} æ¡è®°å½•")

    # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹5æ¡è®°å½•
    batch_size = 5
    total_batches = (len(kol_data) + batch_size - 1) // batch_size

    print(f"ğŸ“¦ åˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡è®°å½•")

    for i in range(0, len(kol_data), batch_size):
        batch = kol_data[i:i + batch_size]
        batch_num = (i // batch_size) + 1

        print(f"\nğŸ”„ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ ({i+1}-{min(i+batch_size, len(kol_data))})")

        # ç”ŸæˆINSERTè¯­å¥
        values = []
        for record in batch:
            value = f"('{record['uid']}', '{record['sec_uid']}', '{record['nickname']}', {record['follower_count']}, '{record['raw_data']}', '{record['search_keyword']}', '{record['search_date']}')"
            values.append(value)

        sql = f"""INSERT INTO gg_douyin_user_search (uid, sec_uid, nickname, follower_count, raw_data, search_keyword, search_date) VALUES
{','.join(values)}
ON CONFLICT (uid) DO UPDATE SET
  sec_uid = EXCLUDED.sec_uid,
  nickname = EXCLUDED.nickname,
  follower_count = EXCLUDED.follower_count,
  raw_data = EXCLUDED.raw_data,
  search_keyword = EXCLUDED.search_keyword,
  search_date = EXCLUDED.search_date,
  updated_at = now();"""

        # æ‰§è¡ŒSQL
        if execute_sql_batch(sql, batch_num):
            print(f"âœ… ç¬¬ {batch_num} æ‰¹å¤„ç†å®Œæˆ")
        else:
            print(f"âŒ ç¬¬ {batch_num} æ‰¹å¤„ç†å¤±è´¥")
            break

    print("\n" + "=" * 80)
    print("ğŸ“ˆ å¯¼å…¥å®Œæˆ")
    print(f"æ€»è®°å½•æ•°: {len(kol_data)}")
    print(f"æ€»æ‰¹æ¬¡æ•°: {total_batches}")
    print("è¯·æ‰‹åŠ¨æ‰§è¡Œç”Ÿæˆçš„SQLæ–‡ä»¶")
    print("=" * 80)


if __name__ == '__main__':
    main()
