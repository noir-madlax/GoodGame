#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸´æ—¶è„šæœ¬ï¼šå¯¼å…¥72ä½KOLç”¨æˆ·åˆ° gg_douyin_user_search è¡¨

ä½¿ç”¨æ–¹æ³•:
    python import_user_search.py

ä¾èµ–:
    pip install python-dotenv supabase
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv
from supabase import create_client, Client

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(Path(__file__).parent.parent.parent.parent.parent / '.env')

# Supabaseé…ç½®
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    print("é”™è¯¯: æœªæ‰¾åˆ°SUPABASE_URLæˆ–SUPABASE_KEYç¯å¢ƒå˜é‡")
    sys.exit(1)

# åˆ›å»ºSupabaseå®¢æˆ·ç«¯
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


def safe_int(value: Any, default: int = 0) -> int:
    """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
    if value is None or value == '':
        return default
    try:
        return int(float(str(value)))
    except (ValueError, TypeError):
        return default


def parse_user_search_data(kol_file: Path) -> Optional[Dict]:
    """
    ä»KOLæ£€æŸ¥æ–‡ä»¶ä¸­è§£æç”¨æˆ·æœç´¢æ•°æ®

    Args:
        kol_file: KOLæ£€æŸ¥JSONæ–‡ä»¶è·¯å¾„

    Returns:
        è§£æåçš„ç”¨æˆ·æ•°æ®å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None
    """
    try:
        with open(kol_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        user_info = data.get('user_info', {})
        api_response = data.get('api_response', {})

        # æå–api_responseä¸­çš„dataéƒ¨åˆ†ä½œä¸ºraw_data
        api_data = api_response.get('data', {})

        return {
            'uid': user_info.get('uid'),
            'sec_uid': user_info.get('sec_uid'),
            'nickname': user_info.get('nickname'),
            'follower_count': safe_int(user_info.get('follower_count')),
            'raw_data': api_response,  # ä¿å­˜å®Œæ•´çš„APIå“åº”
            'search_keyword': 'çš®è‚¤å¥½ ä¸“å®¶',  # ä»æ–‡ä»¶åæ¨æ–­çš„æœç´¢å…³é”®è¯
            'search_date': datetime.now().date().isoformat(),  # å½“å‰æ—¥æœŸ
        }

    except Exception as e:
        print(f"âŒ è§£ææ–‡ä»¶ {kol_file.name} å¤±è´¥: {str(e)}")
        return None


def import_user_search_batch(user_data_list: List[Dict]) -> Dict[str, int]:
    """
    æ‰¹é‡å¯¼å…¥ç”¨æˆ·æœç´¢æ•°æ®

    Args:
        user_data_list: ç”¨æˆ·æ•°æ®åˆ—è¡¨

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    stats = {
        'total': len(user_data_list),
        'success': 0,
        'error': 0,
        'duplicates': 0
    }

    for idx, user_data in enumerate(user_data_list, 1):
        uid = user_data.get('uid')
        nickname = user_data.get('nickname', 'æœªçŸ¥')

        print(f"  [{idx}/{len(user_data_list)}] å¯¼å…¥ç”¨æˆ·: {nickname} (UID: {uid})")

        try:
            # ä½¿ç”¨upsertç¡®ä¿ä¸é‡å¤æ’å…¥
            result = supabase.table('gg_douyin_user_search').upsert(
                user_data,
                on_conflict='uid'
            ).execute()

            stats['success'] += 1
            print(f"    âœ… å·²å¯¼å…¥")

        except Exception as e:
            error_msg = str(e)
            if 'duplicate key value' in error_msg.lower():
                stats['duplicates'] += 1
                print(f"    âš ï¸ é‡å¤æ•°æ®ï¼Œå·²è·³è¿‡")
            else:
                stats['error'] += 1
                print(f"    âŒ å¯¼å…¥å¤±è´¥: {error_msg}")

    return stats


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ æŠ–éŸ³ç”¨æˆ·æœç´¢æ•°æ®å¯¼å…¥å·¥å…·")
    print("=" * 80)

    # æ•°æ®ç›®å½•
    detail_dir = Path(__file__).parent.parent / 'detail'

    if not detail_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {detail_dir}")
        return

    # è·å–æ‰€æœ‰KOLæ£€æŸ¥æ–‡ä»¶
    kol_files = list(detail_dir.glob('kol_check_*.json'))

    if not kol_files:
        print("âŒ æœªæ‰¾åˆ°KOLæ£€æŸ¥æ–‡ä»¶")
        return

    print(f"\nğŸ“Š æ‰¾åˆ° {len(kol_files)} ä¸ªKOLæ£€æŸ¥æ–‡ä»¶")

    # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹20ä¸ªç”¨æˆ·
    batch_size = 20
    total_stats = {
        'total': 0,
        'success': 0,
        'error': 0,
        'duplicates': 0
    }

    for i in range(0, len(kol_files), batch_size):
        batch_files = kol_files[i:i + batch_size]
        batch_data = []

        print(f"\nğŸ”„ å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ (æ–‡ä»¶ {i+1}-{min(i+batch_size, len(kol_files))})")

        # è§£æè¿™ä¸€æ‰¹çš„æ•°æ®
        for kol_file in batch_files:
            user_data = parse_user_search_data(kol_file)
            if user_data:
                batch_data.append(user_data)

        if batch_data:
            # å¯¼å…¥è¿™ä¸€æ‰¹æ•°æ®
            batch_stats = import_user_search_batch(batch_data)

            # ç´¯åŠ ç»Ÿè®¡
            for key in total_stats:
                total_stats[key] += batch_stats[key]

            print(f"  ğŸ“Š æœ¬æ‰¹ç»Ÿè®¡: æˆåŠŸ {batch_stats['success']}, å¤±è´¥ {batch_stats['error']}, é‡å¤ {batch_stats['duplicates']}")
        else:
            print("  âš ï¸ æœ¬æ‰¹æ— æœ‰æ•ˆæ•°æ®")

    # æ‰“å°æ±‡æ€»ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“ˆ å¯¼å…¥ç»Ÿè®¡æ±‡æ€»")
    print("=" * 80)
    print(f"æ€»æ–‡ä»¶æ•°: {len(kol_files)}")
    print(f"è§£ææˆåŠŸ: {total_stats['total']}")
    print(f"âœ… å¯¼å…¥æˆåŠŸ: {total_stats['success']}")
    print(f"âŒ å¯¼å…¥å¤±è´¥: {total_stats['error']}")
    print(f"âš ï¸ é‡å¤æ•°æ®: {total_stats['duplicates']}")
    print("=" * 80)


if __name__ == '__main__':
    main()
