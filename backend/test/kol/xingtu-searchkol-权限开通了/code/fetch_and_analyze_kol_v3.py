#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢ä¸ç»“æ„åŒ–åˆ†æè„šæœ¬ V3

åŠŸèƒ½ï¼š
1. è°ƒç”¨ TikHub API çš„æ˜Ÿå›¾ search_kol_v1 æ¥å£æœç´¢"æŠ¤è‚¤è¾¾äºº"
2. æ”¯æŒè‡ªå®šä¹‰é¡µæ•°å’Œèµ·å§‹é¡µï¼Œè·³è¿‡å·²ä¸‹è½½çš„é¡µé¢
3. ç»“æ„åŒ–è§£æå…³é”®ä¸šåŠ¡æ•°æ®
4. åˆå¹¶æ‰€æœ‰é¡µé¢æ•°æ®å¹¶ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š

æ¥å£æ–‡æ¡£: https://api.tikhub.io/#/Douyin-Xingtu-API/search_kol_v1_api_v1_douyin_xingtu_search_kol_v1_get
"""

import os
import json
import requests
import time
import glob
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, List, Any, Tuple
from collections import Counter


def load_api_key():
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key"""
    backend_dir = Path(__file__).parent.parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®")
    
    return api_key


def get_existing_pages(output_dir: str) -> List[int]:
    """
    æ£€æŸ¥å·²å­˜åœ¨çš„é¡µé¢æ–‡ä»¶
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        list: å·²å­˜åœ¨çš„é¡µç åˆ—è¡¨
    """
    detail_dir = os.path.join(output_dir, 'detail')
    if not os.path.exists(detail_dir):
        return []
    
    # æŸ¥æ‰¾æ‰€æœ‰ raw_page_*.json æ–‡ä»¶
    pattern = os.path.join(detail_dir, 'raw_page_*.json')
    existing_files = glob.glob(pattern)
    
    existing_pages = []
    for filepath in existing_files:
        filename = os.path.basename(filepath)
        # æå–é¡µç ï¼šraw_page_1_20251118_113605.json -> 1
        parts = filename.split('_')
        if len(parts) >= 3 and parts[0] == 'raw' and parts[1] == 'page':
            try:
                page_num = int(parts[2])
                existing_pages.append(page_num)
            except ValueError:
                continue
    
    return sorted(existing_pages)


def fetch_kol_page(api_key: str, keyword: str, page: int, count: int = 20) -> Dict[str, Any]:
    """è°ƒç”¨ TikHub API è·å–ä¸€é¡µ KOL æ•°æ®"""
    url = "https://api.tikhub.io/api/v1/douyin/xingtu/search_kol_v1"
    
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    params = {
        'keyword': keyword,
        'page': page,
        'count': count,
        'sort_type': 1,
        'platformSource': '_1'
    }
    
    print(f"\nğŸ“¡ å‘é€è¯·æ±‚: ç¬¬ {page} é¡µ...")
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            code = result.get('code', -1)
            
            if code == 200:
                data = result.get('data', {})
                authors = data.get('authors', [])
                print(f"   âœ… æˆåŠŸè·å– {len(authors)} ä¸ªè¾¾äºº")
                return result
            else:
                print(f"   âŒ API è¿”å›é”™è¯¯ç : {code}")
                return result
        else:
            print(f"   âŒ HTTP è¯·æ±‚å¤±è´¥: {response.status_code}")
            return {"error": f"HTTP {response.status_code}", "message": response.text}
            
    except Exception as e:
        print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
        return {"error": str(e)}


def parse_kol_data(author: Dict[str, Any]) -> Dict[str, Any]:
    """è§£æå•ä¸ªè¾¾äººçš„å…³é”®ä¸šåŠ¡æ•°æ®"""
    attr_data = author.get('attribute_datas', {})
    
    # è§£ææ ‡ç­¾
    tags_relation_str = attr_data.get('tags_relation', '{}')
    try:
        tags_relation = json.loads(tags_relation_str) if tags_relation_str else {}
    except:
        tags_relation = {}
    
    # è§£ææœ€è¿‘10ä¸ªä½œå“
    last_10_items_str = attr_data.get('last_10_items', '[]')
    try:
        last_10_items = json.loads(last_10_items_str) if last_10_items_str else []
    except:
        last_10_items = []
    
    # è®¡ç®—ä½œå“å¹³å‡æ•°æ®
    total_vv = 0
    total_like = 0
    total_comment = 0
    total_share = 0
    item_count = len(last_10_items)
    
    for item in last_10_items:
        total_vv += int(item.get('vv', 0))
        total_like += int(item.get('like_cnt', 0))
        total_comment += int(item.get('comment_cnt', 0))
        total_share += int(item.get('share_cnt', 0))
    
    avg_vv = total_vv // item_count if item_count > 0 else 0
    avg_like = total_like // item_count if item_count > 0 else 0
    avg_comment = total_comment // item_count if item_count > 0 else 0
    avg_share = total_share // item_count if item_count > 0 else 0
    
    # æ„å»ºå…³é”®ä¸šåŠ¡æ•°æ®
    kol_data = {
        'è¾¾äººID': author.get('star_id', ''),
        'æ˜µç§°': attr_data.get('nick_name', ''),
        'å¤´åƒ': attr_data.get('avatar_uri', ''),
        'æ€§åˆ«': 'å¥³' if attr_data.get('gender', '') == '2' else 'ç”·' if attr_data.get('gender', '') == '1' else 'æœªçŸ¥',
        'åœ°åŒº': f"{attr_data.get('province', '')} {attr_data.get('city', '')}".strip(),
        'ç²‰ä¸æ•°': int(attr_data.get('follower', 0)),
        '15å¤©ç²‰ä¸å¢é‡': int(attr_data.get('fans_increment_within_15d', 0)),
        '30å¤©ç²‰ä¸å¢é‡': attr_data.get('fans_increment_within_30d', '0'),
        '15å¤©ç²‰ä¸å¢é•¿ç‡': float(attr_data.get('fans_increment_rate_within_15d', 0)),
        'æ˜Ÿå›¾è¯„åˆ†': float(attr_data.get('star_index', 0)),
        'ç²‰ä¸ç­‰çº§': attr_data.get('grade', '0'),
        'è¾¾äººç±»å‹': 'ä¸ªäºº' if attr_data.get('author_type', '') == '1' else 'æœºæ„' if attr_data.get('author_type', '') == '2' else 'æœªçŸ¥',
        'è´¦å·çŠ¶æ€': 'æ­£å¸¸' if attr_data.get('author_status', '') == '1' else 'å¼‚å¸¸',
        'å†…å®¹æ ‡ç­¾': tags_relation,
        'è¿‘æœŸä½œå“æ•°': item_count,
        'å¹³å‡æ’­æ”¾é‡': avg_vv,
        'å¹³å‡ç‚¹èµæ•°': avg_like,
        'å¹³å‡è¯„è®ºæ•°': avg_comment,
        'å¹³å‡åˆ†äº«æ•°': avg_share,
        'äº’åŠ¨ç‡': round((avg_like + avg_comment + avg_share) / avg_vv * 100, 2) if avg_vv > 0 else 0,
        'ç”µå•†ç­‰çº§': attr_data.get('author_ecom_level', ''),
        'ç”µå•†å¯ç”¨': attr_data.get('e_commerce_enable', '0') == '1',
        '30å¤©å¸¦è´§è§†é¢‘æ•°': int(attr_data.get('ecom_video_product_num_30d', 0)),
        '30å¤©å¸¦è´§GMVåŒºé—´': attr_data.get('ecom_gmv_30d_range', ''),
        '30å¤©å¹³å‡å®¢å•ä»·åŒºé—´': attr_data.get('ecom_avg_order_value_30d_range', ''),
        '1-20ç§’è§†é¢‘æŠ¥ä»·': int(attr_data.get('price_1_20', 0)),
        '20-60ç§’è§†é¢‘æŠ¥ä»·': int(attr_data.get('price_20_60', 0)),
        '60ç§’ä»¥ä¸Šè§†é¢‘æŠ¥ä»·': int(attr_data.get('price_60', 0)),
        'é¢„ä¼°æ’­æ”¾é‡': int(attr_data.get('expected_play_num', 0)),
        'é¢„ä¼°è‡ªç„¶æ’­æ”¾é‡': int(attr_data.get('expected_natural_play_num', 0)),
        'æ˜¯å¦é»‘é©¬è¾¾äºº': attr_data.get('is_black_horse_author', 'false') == 'true',
        'æ˜¯å¦ä¼˜è´¨è¾¾äºº': attr_data.get('is_excellenct_author', '0') == '1',
        'æ˜¯å¦çŸ­å‰§è¾¾äºº': attr_data.get('is_short_drama', '0') == '1',
        'æ˜¯å¦æ”¯æŒå…±åˆ›': attr_data.get('is_cocreate_author', 'false') == 'true',
    }
    
    return kol_data


def save_raw_response(response_data: Dict[str, Any], page: int, output_dir: str):
    """ä¿å­˜åŸå§‹ API å“åº”åˆ° detail ç›®å½•"""
    detail_dir = os.path.join(output_dir, 'detail')
    os.makedirs(detail_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'raw_page_{page}_{timestamp}.json'
    filepath = os.path.join(detail_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(response_data, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ’¾ å·²ä¿å­˜: {filename}")


def load_all_pages_data(output_dir: str) -> Tuple[List[Dict], List[int]]:
    """
    åŠ è½½æ‰€æœ‰å·²ä¿å­˜çš„é¡µé¢æ•°æ®
    
    Returns:
        tuple: (æ‰€æœ‰é¡µé¢çš„åŸå§‹æ•°æ®åˆ—è¡¨, é¡µç åˆ—è¡¨)
    """
    detail_dir = os.path.join(output_dir, 'detail')
    pattern = os.path.join(detail_dir, 'raw_page_*.json')
    existing_files = glob.glob(pattern)
    
    # æŒ‰é¡µç æ’åº
    page_data_map = {}
    for filepath in existing_files:
        filename = os.path.basename(filepath)
        parts = filename.split('_')
        if len(parts) >= 3:
            try:
                page_num = int(parts[2])
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    page_data_map[page_num] = data
            except (ValueError, json.JSONDecodeError):
                continue
    
    # æŒ‰é¡µç æ’åº
    sorted_pages = sorted(page_data_map.keys())
    all_pages_data = [page_data_map[page] for page in sorted_pages]
    
    return all_pages_data, sorted_pages


def generate_detailed_analysis(all_parsed_kols: List[Dict], total_pages: int, output_dir: str):
    """ç”Ÿæˆè¯¦ç»†çš„æ•°æ®åˆ†ææŠ¥å‘Š"""
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š å¼€å§‹ç”Ÿæˆè¯¦ç»†æ•°æ®åˆ†æ")
    print(f"{'='*80}")
    
    total_count = len(all_parsed_kols)
    
    # ========== 1. ç²‰ä¸æ•°åˆ†æ ==========
    follower_counts = [kol['ç²‰ä¸æ•°'] for kol in all_parsed_kols]
    avg_followers = sum(follower_counts) // len(follower_counts) if follower_counts else 0
    max_followers = max(follower_counts) if follower_counts else 0
    min_followers = min(follower_counts) if follower_counts else 0
    median_followers = sorted(follower_counts)[len(follower_counts)//2] if follower_counts else 0
    
    # ç²‰ä¸æ•°åŒºé—´åˆ†å¸ƒ
    follower_ranges = {
        '0-1ä¸‡': 0, '1-5ä¸‡': 0, '5-10ä¸‡': 0, '10-20ä¸‡': 0,
        '20-50ä¸‡': 0, '50-100ä¸‡': 0, '100-200ä¸‡': 0, '200-500ä¸‡': 0,
        '500-1000ä¸‡': 0, '1000ä¸‡+': 0
    }
    
    for count in follower_counts:
        if count < 10000:
            follower_ranges['0-1ä¸‡'] += 1
        elif count < 50000:
            follower_ranges['1-5ä¸‡'] += 1
        elif count < 100000:
            follower_ranges['5-10ä¸‡'] += 1
        elif count < 200000:
            follower_ranges['10-20ä¸‡'] += 1
        elif count < 500000:
            follower_ranges['20-50ä¸‡'] += 1
        elif count < 1000000:
            follower_ranges['50-100ä¸‡'] += 1
        elif count < 2000000:
            follower_ranges['100-200ä¸‡'] += 1
        elif count < 5000000:
            follower_ranges['200-500ä¸‡'] += 1
        elif count < 10000000:
            follower_ranges['500-1000ä¸‡'] += 1
        else:
            follower_ranges['1000ä¸‡+'] += 1
    
    # ========== 2. æ˜Ÿå›¾è¯„åˆ†åˆ†æ ==========
    star_scores = [kol['æ˜Ÿå›¾è¯„åˆ†'] for kol in all_parsed_kols if kol['æ˜Ÿå›¾è¯„åˆ†'] > 0]
    avg_star_score = sum(star_scores) / len(star_scores) if star_scores else 0
    max_star_score = max(star_scores) if star_scores else 0
    min_star_score = min(star_scores) if star_scores else 0
    
    # æ˜Ÿå›¾è¯„åˆ†åŒºé—´åˆ†å¸ƒ
    star_ranges = {
        '0-40åˆ†': 0, '40-50åˆ†': 0, '50-60åˆ†': 0,
        '60-70åˆ†': 0, '70-80åˆ†': 0, '80-90åˆ†': 0, '90-100åˆ†': 0
    }
    
    for score in star_scores:
        if score < 40:
            star_ranges['0-40åˆ†'] += 1
        elif score < 50:
            star_ranges['40-50åˆ†'] += 1
        elif score < 60:
            star_ranges['50-60åˆ†'] += 1
        elif score < 70:
            star_ranges['60-70åˆ†'] += 1
        elif score < 80:
            star_ranges['70-80åˆ†'] += 1
        elif score < 90:
            star_ranges['80-90åˆ†'] += 1
        else:
            star_ranges['90-100åˆ†'] += 1
    
    # ========== 3. äº’åŠ¨ç‡åˆ†æ ==========
    interaction_rates = [kol['äº’åŠ¨ç‡'] for kol in all_parsed_kols if kol['äº’åŠ¨ç‡'] > 0]
    avg_interaction_rate = sum(interaction_rates) / len(interaction_rates) if interaction_rates else 0
    max_interaction_rate = max(interaction_rates) if interaction_rates else 0
    min_interaction_rate = min(interaction_rates) if interaction_rates else 0
    
    # äº’åŠ¨ç‡åŒºé—´åˆ†å¸ƒ
    interaction_ranges = {
        '0-1%': 0, '1-3%': 0, '3-5%': 0, '5-10%': 0, '10%+': 0
    }
    
    for rate in interaction_rates:
        if rate < 1:
            interaction_ranges['0-1%'] += 1
        elif rate < 3:
            interaction_ranges['1-3%'] += 1
        elif rate < 5:
            interaction_ranges['3-5%'] += 1
        elif rate < 10:
            interaction_ranges['5-10%'] += 1
        else:
            interaction_ranges['10%+'] += 1
    
    # ========== 4. æŠ¥ä»·åˆ†æ ==========
    prices_20_60 = [kol['20-60ç§’è§†é¢‘æŠ¥ä»·'] for kol in all_parsed_kols if kol['20-60ç§’è§†é¢‘æŠ¥ä»·'] > 0]
    avg_price = sum(prices_20_60) // len(prices_20_60) if prices_20_60 else 0
    max_price = max(prices_20_60) if prices_20_60 else 0
    min_price = min(prices_20_60) if prices_20_60 else 0
    
    # æŠ¥ä»·åŒºé—´åˆ†å¸ƒ
    price_ranges = {
        '0-1ä¸‡': 0, '1-3ä¸‡': 0, '3-5ä¸‡': 0, '5-10ä¸‡': 0,
        '10-20ä¸‡': 0, '20-50ä¸‡': 0, '50ä¸‡+': 0
    }
    
    for price in prices_20_60:
        if price < 10000:
            price_ranges['0-1ä¸‡'] += 1
        elif price < 30000:
            price_ranges['1-3ä¸‡'] += 1
        elif price < 50000:
            price_ranges['3-5ä¸‡'] += 1
        elif price < 100000:
            price_ranges['5-10ä¸‡'] += 1
        elif price < 200000:
            price_ranges['10-20ä¸‡'] += 1
        elif price < 500000:
            price_ranges['20-50ä¸‡'] += 1
        else:
            price_ranges['50ä¸‡+'] += 1
    
    # ========== 5. å†…å®¹æ ‡ç­¾ç»Ÿè®¡ ==========
    all_tags = []
    for kol in all_parsed_kols:
        tags = kol['å†…å®¹æ ‡ç­¾']
        if isinstance(tags, dict):
            for category, sub_tags in tags.items():
                all_tags.append(category)
                if isinstance(sub_tags, list):
                    all_tags.extend(sub_tags)
    
    tag_counter = Counter(all_tags)
    top_10_tags = tag_counter.most_common(10)
    
    # ========== 6. åœ°åŒºåˆ†å¸ƒ ==========
    provinces = [kol['åœ°åŒº'].split()[0] if kol['åœ°åŒº'] else 'æœªçŸ¥' for kol in all_parsed_kols]
    province_counter = Counter(provinces)
    top_10_provinces = province_counter.most_common(10)
    
    # ========== 7. æ€§åˆ«åˆ†å¸ƒ ==========
    genders = [kol['æ€§åˆ«'] for kol in all_parsed_kols]
    gender_counter = Counter(genders)
    
    # ========== 8. è¾¾äººç±»å‹åˆ†å¸ƒ ==========
    types = [kol['è¾¾äººç±»å‹'] for kol in all_parsed_kols]
    type_counter = Counter(types)
    
    # ========== 9. ç”µå•†æ•°æ®åˆ†æ ==========
    ecom_enabled_count = sum(1 for kol in all_parsed_kols if kol['ç”µå•†å¯ç”¨'])
    ecom_gmv_ranges = [kol['30å¤©å¸¦è´§GMVåŒºé—´'] for kol in all_parsed_kols if kol['30å¤©å¸¦è´§GMVåŒºé—´']]
    gmv_counter = Counter(ecom_gmv_ranges)
    
    # ========== 10. ç‰¹æ®Šæ ‡è®°ç»Ÿè®¡ ==========
    black_horse_count = sum(1 for kol in all_parsed_kols if kol['æ˜¯å¦é»‘é©¬è¾¾äºº'])
    excellent_count = sum(1 for kol in all_parsed_kols if kol['æ˜¯å¦ä¼˜è´¨è¾¾äºº'])
    short_drama_count = sum(1 for kol in all_parsed_kols if kol['æ˜¯å¦çŸ­å‰§è¾¾äºº'])
    cocreate_count = sum(1 for kol in all_parsed_kols if kol['æ˜¯å¦æ”¯æŒå…±åˆ›'])
    
    # ========== ç”Ÿæˆåˆ†ææŠ¥å‘Š ==========
    report_content = f"""# æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæ•°æ®åˆ†ææŠ¥å‘Š

## ğŸ“Š æ•°æ®æ¦‚è§ˆ

- **æœç´¢å…³é”®è¯**: æŠ¤è‚¤ä¿å…»
- **åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ•°æ®é¡µæ•°**: {total_pages} é¡µ
- **æ€»è¾¾äººæ•°**: {total_count} ä¸ª
- **æ•°æ®æ¥æº**: æŠ–éŸ³æ˜Ÿå›¾ KOL æœç´¢ API (search_kol_v1)

---

## ä¸€ã€ç²‰ä¸æ•°æ®åˆ†æ

### 1.1 ç²‰ä¸æ•°ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡ç²‰ä¸æ•° | {avg_followers:,} |
| ä¸­ä½æ•°ç²‰ä¸æ•° | {median_followers:,} |
| æœ€é«˜ç²‰ä¸æ•° | {max_followers:,} |
| æœ€ä½ç²‰ä¸æ•° | {min_followers:,} |
| ç²‰ä¸æ€»æ•° | {sum(follower_counts):,} |

### 1.2 ç²‰ä¸æ•°åŒºé—´åˆ†å¸ƒ

| åŒºé—´ | æ•°é‡ | å æ¯” | å¯è§†åŒ– |
|------|------|------|--------|
"""
    
    for range_name, count in follower_ranges.items():
        percentage = (count / total_count * 100) if total_count > 0 else 0
        bar = 'â–ˆ' * int(percentage / 2)
        report_content += f"| {range_name} | {count} | {percentage:.1f}% | {bar} |\n"
    
    report_content += f"""
### 1.3 ç²‰ä¸å¢é•¿åˆ†æ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡15å¤©ç²‰ä¸å¢é‡ | {sum(kol['15å¤©ç²‰ä¸å¢é‡'] for kol in all_parsed_kols) // total_count:,} |
| å¹³å‡15å¤©å¢é•¿ç‡ | {sum(kol['15å¤©ç²‰ä¸å¢é•¿ç‡'] for kol in all_parsed_kols) / total_count * 100:.2f}% |
| å¢é•¿è¾¾äººæ•° | {sum(1 for kol in all_parsed_kols if kol['15å¤©ç²‰ä¸å¢é‡'] > 0)} |
| ä¸‹é™è¾¾äººæ•° | {sum(1 for kol in all_parsed_kols if kol['15å¤©ç²‰ä¸å¢é‡'] < 0)} |

---

## äºŒã€æ˜Ÿå›¾è¯„åˆ†åˆ†æ

### 2.1 è¯„åˆ†ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡æ˜Ÿå›¾è¯„åˆ† | {avg_star_score:.2f} |
| æœ€é«˜æ˜Ÿå›¾è¯„åˆ† | {max_star_score:.2f} |
| æœ€ä½æ˜Ÿå›¾è¯„åˆ† | {min_star_score:.2f} |
| æœ‰è¯„åˆ†è¾¾äººæ•° | {len(star_scores)} |

### 2.2 è¯„åˆ†åŒºé—´åˆ†å¸ƒ

| åŒºé—´ | æ•°é‡ | å æ¯” | å¯è§†åŒ– |
|------|------|------|--------|
"""
    
    for range_name, count in star_ranges.items():
        percentage = (count / len(star_scores) * 100) if star_scores else 0
        bar = 'â–ˆ' * int(percentage / 2)
        report_content += f"| {range_name} | {count} | {percentage:.1f}% | {bar} |\n"
    
    report_content += f"""
---

## ä¸‰ã€å†…å®¹äº’åŠ¨åˆ†æ

### 3.1 äº’åŠ¨ç‡ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡äº’åŠ¨ç‡ | {avg_interaction_rate:.2f}% |
| æœ€é«˜äº’åŠ¨ç‡ | {max_interaction_rate:.2f}% |
| æœ€ä½äº’åŠ¨ç‡ | {min_interaction_rate:.2f}% |
| æœ‰äº’åŠ¨æ•°æ®è¾¾äººæ•° | {len(interaction_rates)} |

### 3.2 äº’åŠ¨ç‡åŒºé—´åˆ†å¸ƒ

| åŒºé—´ | æ•°é‡ | å æ¯” | å¯è§†åŒ– |
|------|------|------|--------|
"""
    
    for range_name, count in interaction_ranges.items():
        percentage = (count / len(interaction_rates) * 100) if interaction_rates else 0
        bar = 'â–ˆ' * int(percentage / 2)
        report_content += f"| {range_name} | {count} | {percentage:.1f}% | {bar} |\n"
    
    report_content += f"""
### 3.3 ä½œå“æ•°æ®ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡æ’­æ”¾é‡ | {sum(kol['å¹³å‡æ’­æ”¾é‡'] for kol in all_parsed_kols) // total_count:,} |
| å¹³å‡ç‚¹èµæ•° | {sum(kol['å¹³å‡ç‚¹èµæ•°'] for kol in all_parsed_kols) // total_count:,} |
| å¹³å‡è¯„è®ºæ•° | {sum(kol['å¹³å‡è¯„è®ºæ•°'] for kol in all_parsed_kols) // total_count:,} |
| å¹³å‡åˆ†äº«æ•° | {sum(kol['å¹³å‡åˆ†äº«æ•°'] for kol in all_parsed_kols) // total_count:,} |

---

## å››ã€å•†ä¸šæŠ¥ä»·åˆ†æ

### 4.1 æŠ¥ä»·ç»Ÿè®¡ï¼ˆ20-60ç§’è§†é¢‘ï¼‰

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡æŠ¥ä»· | {avg_price:,} å…ƒ |
| æœ€é«˜æŠ¥ä»· | {max_price:,} å…ƒ |
| æœ€ä½æŠ¥ä»· | {min_price:,} å…ƒ |
| æœ‰æŠ¥ä»·è¾¾äººæ•° | {len(prices_20_60)} |
| æ— æŠ¥ä»·è¾¾äººæ•° | {total_count - len(prices_20_60)} |

### 4.2 æŠ¥ä»·åŒºé—´åˆ†å¸ƒ

| åŒºé—´ | æ•°é‡ | å æ¯” | å¯è§†åŒ– |
|------|------|------|--------|
"""
    
    for range_name, count in price_ranges.items():
        percentage = (count / len(prices_20_60) * 100) if prices_20_60 else 0
        bar = 'â–ˆ' * int(percentage / 2)
        report_content += f"| {range_name} | {count} | {percentage:.1f}% | {bar} |\n"
    
    report_content += f"""
---

## äº”ã€å†…å®¹æ ‡ç­¾åˆ†æ

### 5.1 TOP 10 çƒ­é—¨æ ‡ç­¾

| æ’å | æ ‡ç­¾ | å‡ºç°æ¬¡æ•° |
|------|------|----------|
"""
    
    for i, (tag, count) in enumerate(top_10_tags, 1):
        report_content += f"| {i} | {tag} | {count} |\n"
    
    report_content += f"""
---

## å…­ã€åœ°åŒºåˆ†å¸ƒåˆ†æ

### 6.1 TOP 10 çœä»½/åŸå¸‚

| æ’å | åœ°åŒº | è¾¾äººæ•° | å æ¯” |
|------|------|--------|------|
"""
    
    for i, (province, count) in enumerate(top_10_provinces, 1):
        percentage = (count / total_count * 100) if total_count > 0 else 0
        report_content += f"| {i} | {province} | {count} | {percentage:.1f}% |\n"
    
    report_content += f"""
---

## ä¸ƒã€è¾¾äººå±æ€§åˆ†æ

### 7.1 æ€§åˆ«åˆ†å¸ƒ

| æ€§åˆ« | æ•°é‡ | å æ¯” |
|------|------|------|
"""
    
    for gender, count in gender_counter.items():
        percentage = (count / total_count * 100) if total_count > 0 else 0
        report_content += f"| {gender} | {count} | {percentage:.1f}% |\n"
    
    report_content += f"""
### 7.2 è¾¾äººç±»å‹åˆ†å¸ƒ

| ç±»å‹ | æ•°é‡ | å æ¯” |
|------|------|------|
"""
    
    for type_name, count in type_counter.items():
        percentage = (count / total_count * 100) if total_count > 0 else 0
        report_content += f"| {type_name} | {count} | {percentage:.1f}% |\n"
    
    report_content += f"""
---

## å…«ã€ç”µå•†èƒ½åŠ›åˆ†æ

### 8.1 ç”µå•†å¯ç”¨æƒ…å†µ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¼€é€šç”µå•†è¾¾äººæ•° | {ecom_enabled_count} |
| ç”µå•†å¼€é€šç‡ | {(ecom_enabled_count / total_count * 100):.1f}% |
| æœªå¼€é€šç”µå•†è¾¾äººæ•° | {total_count - ecom_enabled_count} |

### 8.2 30å¤©å¸¦è´§GMVåŒºé—´åˆ†å¸ƒ

| GMVåŒºé—´ | æ•°é‡ |
|----------|------|
"""
    
    for gmv_range, count in gmv_counter.most_common():
        report_content += f"| {gmv_range} | {count} |\n"
    
    report_content += f"""
---

## ä¹ã€ç‰¹æ®Šæ ‡è®°ç»Ÿè®¡

| æ ‡è®°ç±»å‹ | æ•°é‡ | å æ¯” |
|----------|------|------|
| é»‘é©¬è¾¾äºº | {black_horse_count} | {(black_horse_count / total_count * 100):.1f}% |
| ä¼˜è´¨è¾¾äºº | {excellent_count} | {(excellent_count / total_count * 100):.1f}% |
| çŸ­å‰§è¾¾äºº | {short_drama_count} | {(short_drama_count / total_count * 100):.1f}% |
| æ”¯æŒå…±åˆ› | {cocreate_count} | {(cocreate_count / total_count * 100):.1f}% |

---

## åã€TOP 20 æŠ¤è‚¤è¾¾äººæ¦œå•

### 10.1 æŒ‰ç²‰ä¸æ•°æ’å

| æ’å | æ˜µç§° | ç²‰ä¸æ•° | æ˜Ÿå›¾è¯„åˆ† | äº’åŠ¨ç‡ | 20-60ç§’æŠ¥ä»· |
|------|------|--------|----------|--------|-------------|
"""
    
    sorted_by_followers = sorted(all_parsed_kols, key=lambda x: x['ç²‰ä¸æ•°'], reverse=True)
    for i, kol in enumerate(sorted_by_followers[:20], 1):
        report_content += f"| {i} | {kol['æ˜µç§°']} | {kol['ç²‰ä¸æ•°']:,} | {kol['æ˜Ÿå›¾è¯„åˆ†']:.2f} | {kol['äº’åŠ¨ç‡']}% | {kol['20-60ç§’è§†é¢‘æŠ¥ä»·']:,} å…ƒ |\n"
    
    report_content += f"""
### 10.2 æŒ‰æ˜Ÿå›¾è¯„åˆ†æ’å

| æ’å | æ˜µç§° | æ˜Ÿå›¾è¯„åˆ† | ç²‰ä¸æ•° | äº’åŠ¨ç‡ | 20-60ç§’æŠ¥ä»· |
|------|------|----------|--------|--------|-------------|
"""
    
    sorted_by_star = sorted(all_parsed_kols, key=lambda x: x['æ˜Ÿå›¾è¯„åˆ†'], reverse=True)
    for i, kol in enumerate(sorted_by_star[:20], 1):
        report_content += f"| {i} | {kol['æ˜µç§°']} | {kol['æ˜Ÿå›¾è¯„åˆ†']:.2f} | {kol['ç²‰ä¸æ•°']:,} | {kol['äº’åŠ¨ç‡']}% | {kol['20-60ç§’è§†é¢‘æŠ¥ä»·']:,} å…ƒ |\n"
    
    report_content += f"""
### 10.3 æŒ‰äº’åŠ¨ç‡æ’å

| æ’å | æ˜µç§° | äº’åŠ¨ç‡ | ç²‰ä¸æ•° | æ˜Ÿå›¾è¯„åˆ† | 20-60ç§’æŠ¥ä»· |
|------|------|--------|--------|----------|-------------|
"""
    
    sorted_by_interaction = sorted(all_parsed_kols, key=lambda x: x['äº’åŠ¨ç‡'], reverse=True)
    for i, kol in enumerate(sorted_by_interaction[:20], 1):
        report_content += f"| {i} | {kol['æ˜µç§°']} | {kol['äº’åŠ¨ç‡']}% | {kol['ç²‰ä¸æ•°']:,} | {kol['æ˜Ÿå›¾è¯„åˆ†']:.2f} | {kol['20-60ç§’è§†é¢‘æŠ¥ä»·']:,} å…ƒ |\n"
    
    report_content += f"""
---

## åä¸€ã€æ€§ä»·æ¯”åˆ†æ

### 11.1 é«˜æ€§ä»·æ¯”è¾¾äººï¼ˆç»¼åˆè¯„åˆ† = æ˜Ÿå›¾è¯„åˆ† Ã— äº’åŠ¨ç‡ / æŠ¥ä»·ï¼‰

| æ’å | æ˜µç§° | ç²‰ä¸æ•° | æ˜Ÿå›¾è¯„åˆ† | äº’åŠ¨ç‡ | 20-60ç§’æŠ¥ä»· | æ€§ä»·æ¯”æŒ‡æ•° |
|------|------|--------|----------|--------|-------------|-----------|
"""
    
    # è®¡ç®—æ€§ä»·æ¯”ï¼ˆæ˜Ÿå›¾è¯„åˆ† Ã— äº’åŠ¨ç‡ / æŠ¥ä»·ï¼ŒæŠ¥ä»·ä¸º0çš„è·³è¿‡ï¼‰
    kols_with_price = [kol for kol in all_parsed_kols if kol['20-60ç§’è§†é¢‘æŠ¥ä»·'] > 0 and kol['æ˜Ÿå›¾è¯„åˆ†'] > 0]
    for kol in kols_with_price:
        kol['æ€§ä»·æ¯”æŒ‡æ•°'] = (kol['æ˜Ÿå›¾è¯„åˆ†'] * kol['äº’åŠ¨ç‡']) / (kol['20-60ç§’è§†é¢‘æŠ¥ä»·'] / 1000)
    
    sorted_by_value = sorted(kols_with_price, key=lambda x: x['æ€§ä»·æ¯”æŒ‡æ•°'], reverse=True)
    for i, kol in enumerate(sorted_by_value[:20], 1):
        report_content += f"| {i} | {kol['æ˜µç§°']} | {kol['ç²‰ä¸æ•°']:,} | {kol['æ˜Ÿå›¾è¯„åˆ†']:.2f} | {kol['äº’åŠ¨ç‡']}% | {kol['20-60ç§’è§†é¢‘æŠ¥ä»·']:,} å…ƒ | {kol['æ€§ä»·æ¯”æŒ‡æ•°']:.4f} |\n"
    
    report_content += f"""
---

## åäºŒã€æ•°æ®æ´å¯Ÿä¸å»ºè®®

### 12.1 å¸‚åœºæ´å¯Ÿ

1. **ç²‰ä¸åˆ†å¸ƒ**: ä¸»è¦é›†ä¸­åœ¨ {max(follower_ranges, key=follower_ranges.get)} åŒºé—´ï¼Œå æ¯” {max(follower_ranges.values()) / total_count * 100:.1f}%
2. **æ˜Ÿå›¾è¯„åˆ†**: å¹³å‡è¯„åˆ† {avg_star_score:.2f}ï¼Œå¤§éƒ¨åˆ†è¾¾äººåœ¨ 50-70 åˆ†åŒºé—´
3. **äº’åŠ¨æƒ…å†µ**: å¹³å‡äº’åŠ¨ç‡ {avg_interaction_rate:.2f}%ï¼Œé«˜äº’åŠ¨ç‡ï¼ˆ>5%ï¼‰è¾¾äººå æ¯” {sum(1 for r in interaction_rates if r > 5) / len(interaction_rates) * 100:.1f}%
4. **å•†ä¸šä»·å€¼**: å¹³å‡æŠ¥ä»· {avg_price:,} å…ƒï¼ŒæŠ¥ä»·å·®å¼‚è¾ƒå¤§åæ˜ å‡ºå¸‚åœºåˆ†å±‚æ˜æ˜¾

### 12.2 é€‰æ‹©å»ºè®®

#### é¢„ç®—å……è¶³ï¼ˆ10ä¸‡+ï¼‰
- å…³æ³¨ï¼šç²‰ä¸ 100ä¸‡+ ä¸”æ˜Ÿå›¾è¯„åˆ† 70+ çš„è¾¾äºº
- ä¼˜åŠ¿ï¼šå“ç‰Œå½±å“åŠ›å¤§ï¼Œæ›å…‰é‡é«˜
- ä»£è¡¨ï¼š{sorted_by_followers[0]['æ˜µç§°']}ã€{sorted_by_followers[1]['æ˜µç§°']} ç­‰

#### é¢„ç®—ä¸­ç­‰ï¼ˆ3-10ä¸‡ï¼‰
- å…³æ³¨ï¼šç²‰ä¸ 20-100ä¸‡ï¼Œäº’åŠ¨ç‡ >3% çš„è¾¾äºº
- ä¼˜åŠ¿ï¼šæ€§ä»·æ¯”é«˜ï¼Œç”¨æˆ·ç²˜æ€§å¥½
- å»ºè®®ï¼šé€‰æ‹©æ˜Ÿå›¾è¯„åˆ† 60+ çš„è¾¾äºº

#### é¢„ç®—æœ‰é™ï¼ˆ3ä¸‡ä»¥ä¸‹ï¼‰
- å…³æ³¨ï¼šç²‰ä¸ 10-50ä¸‡ï¼Œé«˜äº’åŠ¨ç‡ï¼ˆ>5%ï¼‰è¾¾äºº
- ä¼˜åŠ¿ï¼šç²¾å‡†è§¦è¾¾ï¼Œè½¬åŒ–ç‡å¯èƒ½æ›´é«˜
- å»ºè®®ï¼šä¼˜å…ˆé€‰æ‹©å‚ç›´é¢†åŸŸæ·±è€•çš„è¾¾äºº

### 12.3 æŠ•æ”¾ç­–ç•¥

1. **å¤šè¾¾äººç»„åˆ**: å¤´éƒ¨ï¼ˆ1-2ä¸ªï¼‰+ è…°éƒ¨ï¼ˆ3-5ä¸ªï¼‰+ å°¾éƒ¨ï¼ˆ5-10ä¸ªï¼‰
2. **æµ‹è¯•ä¼˜åŒ–**: å…ˆå°è§„æ¨¡æµ‹è¯•ï¼Œæ ¹æ®æ•°æ®ä¼˜åŒ–è¾¾äººé€‰æ‹©
3. **å†…å®¹å…±åˆ›**: ä¼˜å…ˆé€‰æ‹©æ”¯æŒå…±åˆ›çš„è¾¾äººï¼ˆ{cocreate_count} ä¸ªï¼‰
4. **ç”µå•†å¸¦è´§**: å¦‚éœ€å¸¦è´§ï¼Œé€‰æ‹©å·²å¼€é€šç”µå•†çš„è¾¾äººï¼ˆ{ecom_enabled_count} ä¸ªï¼‰

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æ•°æ®æœ‰æ•ˆæœŸ**: 24å°æ—¶ï¼ˆAPIç¼“å­˜ï¼‰  
**æ•°æ®æ¥æº**: æŠ–éŸ³æ˜Ÿå›¾ KOL æœç´¢ API
"""
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(output_dir, f'æ•°æ®åˆ†ææ€»ç»“_{timestamp}.md')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"\nğŸ’¾ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return report_file


def main():
    """ä¸»å‡½æ•°ï¼šè·å–å¤šé¡µæ•°æ®å¹¶ç”Ÿæˆè¯¦ç»†åˆ†æ"""
    
    print("=" * 80)
    print("æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢ä¸ç»“æ„åŒ–åˆ†æå·¥å…· V3")
    print("=" * 80)
    
    # å‚æ•°é…ç½®
    KEYWORD = "æŠ¤è‚¤ä¿å…»"
    TOTAL_PAGES = 28  # æ€»å…±è¦è·å–çš„é¡µæ•°
    COUNT_PER_PAGE = 20
    
    # 1. åŠ è½½ API Key
    print("\n1ï¸âƒ£ åŠ è½½ API é…ç½®...")
    try:
        api_key = load_api_key()
        print(f"âœ… API Key å·²åŠ è½½")
    except ValueError as e:
        print(f"âŒ {e}")
        return
    
    # 2. è®¾ç½®è¾“å‡ºç›®å½•
    script_dir = Path(__file__).parent.parent
    output_dir = script_dir / "output" / "keyword_æŠ¤è‚¤ä¿å…»"
    
    # 3. æ£€æŸ¥å·²å­˜åœ¨çš„é¡µé¢
    print(f"\n2ï¸âƒ£ æ£€æŸ¥å·²ä¸‹è½½çš„é¡µé¢...")
    existing_pages = get_existing_pages(str(output_dir))
    
    if existing_pages:
        print(f"âœ… å‘ç°å·²ä¸‹è½½çš„é¡µé¢: {existing_pages}")
        print(f"   å…± {len(existing_pages)} é¡µ")
    else:
        print(f"   æœªå‘ç°å·²ä¸‹è½½çš„é¡µé¢")
    
    # 4. ç¡®å®šéœ€è¦ä¸‹è½½çš„é¡µé¢
    pages_to_fetch = [p for p in range(1, TOTAL_PAGES + 1) if p not in existing_pages]
    
    if pages_to_fetch:
        print(f"\n3ï¸âƒ£ å¼€å§‹è·å–æ–°é¡µé¢...")
        print(f"   éœ€è¦è·å–: ç¬¬ {pages_to_fetch[0]} - {pages_to_fetch[-1]} é¡µï¼Œå…± {len(pages_to_fetch)} é¡µ")
        
        for page in pages_to_fetch:
            print(f"\n{'='*80}")
            print(f"ç¬¬ {page}/{TOTAL_PAGES} é¡µ")
            print(f"{'='*80}")
            
            # è·å–æ•°æ®
            response_data = fetch_kol_page(api_key, KEYWORD, page, COUNT_PER_PAGE)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if 'error' in response_data or response_data.get('code') != 200:
                print(f"âš ï¸ ç¬¬ {page} é¡µè·å–å¤±è´¥")
                continue
            
            # ä¿å­˜åŸå§‹è¿”å›
            save_raw_response(response_data, page, str(output_dir))
            
            # æ·»åŠ å»¶è¿Ÿ
            if page < pages_to_fetch[-1]:
                print(f"   â³ ç­‰å¾… 1 ç§’...")
                time.sleep(1)
    else:
        print(f"\n3ï¸âƒ£ æ‰€æœ‰é¡µé¢å·²ä¸‹è½½å®Œæˆï¼Œæ— éœ€é‡æ–°è·å–")
    
    # 5. åŠ è½½æ‰€æœ‰é¡µé¢æ•°æ®
    print(f"\n4ï¸âƒ£ åŠ è½½æ‰€æœ‰é¡µé¢æ•°æ®...")
    all_pages_data, loaded_pages = load_all_pages_data(str(output_dir))
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(all_pages_data)} é¡µæ•°æ®")
    print(f"   é¡µç : {loaded_pages}")
    
    # 6. è§£ææ‰€æœ‰è¾¾äººæ•°æ®
    print(f"\n5ï¸âƒ£ è§£æè¾¾äººæ•°æ®...")
    all_parsed_kols = []
    
    for page_data in all_pages_data:
        data = page_data.get('data', {})
        authors = data.get('authors', [])
        
        for author in authors:
            parsed_kol = parse_kol_data(author)
            all_parsed_kols.append(parsed_kol)
    
    print(f"âœ… æˆåŠŸè§£æ {len(all_parsed_kols)} ä¸ªè¾¾äºº")
    
    # 7. æ£€æŸ¥é‡å¤
    all_ids = [kol['è¾¾äººID'] for kol in all_parsed_kols]
    unique_ids = set(all_ids)
    duplicate_count = len(all_ids) - len(unique_ids)
    
    print(f"\n6ï¸âƒ£ æ•°æ®é‡å¤æ£€æŸ¥...")
    print(f"   æ€»è¾¾äººæ•°: {len(all_ids)}")
    print(f"   å”¯ä¸€è¾¾äººæ•°: {len(unique_ids)}")
    print(f"   é‡å¤è¾¾äººæ•°: {duplicate_count}")
    
    if duplicate_count == 0:
        print(f"   âœ… æ— é‡å¤æ•°æ®")
    else:
        print(f"   âš ï¸ å‘ç° {duplicate_count} ä¸ªé‡å¤è¾¾äºº")
    
    # 8. ä¿å­˜å®Œæ•´çš„è§£ææ•°æ®
    print(f"\n7ï¸âƒ£ ä¿å­˜å®Œæ•´æ•°æ®...")
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    detail_dir = os.path.join(output_dir, 'detail')
    parsed_file = os.path.join(detail_dir, f'parsed_kol_data_all_{timestamp}.json')
    
    output_data = {
        'è¯´æ˜': {
            'æ•°æ®æ¥æº': 'æŠ–éŸ³æ˜Ÿå›¾ KOL æœç´¢ API (search_kol_v1)',
            'æœç´¢å…³é”®è¯': KEYWORD,
            'è·å–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'æ•°æ®é¡µæ•°': len(loaded_pages),
            'é¡µç åˆ—è¡¨': loaded_pages,
            'æ€»è¾¾äººæ•°': len(all_parsed_kols),
            'å”¯ä¸€è¾¾äººæ•°': len(unique_ids),
            'é‡å¤è¾¾äººæ•°': duplicate_count
        },
        'è¾¾äººæ•°æ®': all_parsed_kols
    }
    
    with open(parsed_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ å®Œæ•´æ•°æ®å·²ä¿å­˜: {parsed_file}")
    
    # 9. ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
    print(f"\n8ï¸âƒ£ ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...")
    generate_detailed_analysis(all_parsed_kols, len(loaded_pages), str(output_dir))
    
    print(f"\n{'='*80}")
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"   æ€»é¡µæ•°: {len(loaded_pages)}")
    print(f"   æ€»è¾¾äººæ•°: {len(all_parsed_kols)}")
    print(f"   å”¯ä¸€è¾¾äººæ•°: {len(unique_ids)}")


if __name__ == "__main__":
    main()

