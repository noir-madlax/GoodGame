#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢ä¸ç»“æ„åŒ–åˆ†æè„šæœ¬ V2

åŠŸèƒ½ï¼š
1. è°ƒç”¨ TikHub API çš„æ˜Ÿå›¾ search_kol_v1 æ¥å£æœç´¢"æŠ¤è‚¤è¾¾äºº"
2. è·å– 3 é¡µæœç´¢ç»“æœæ•°æ®ï¼Œåˆ†åˆ«ä¿å­˜æ¯é¡µçš„åŸå§‹è¿”å›
3. ç»“æ„åŒ–è§£æå…³é”®ä¸šåŠ¡æ•°æ®
4. æ£€æŸ¥3é¡µæ•°æ®æ˜¯å¦é‡å¤
5. è¾“å‡ºå…³é”®è¾¾äººä¿¡æ¯åˆ°ç‹¬ç«‹ JSON æ–‡ä»¶

æ¥å£æ–‡æ¡£: https://api.tikhub.io/#/Douyin-Xingtu-API/search_kol_v1_api_v1_douyin_xingtu_search_kol_v1_get

æ•°æ®è¯´æ˜ï¼š
- åŸå§‹è¿”å›ä¿å­˜åœ¨ output/detail/ ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åï¼šraw_page_{page}.json
- è§£æåçš„å…³é”®ä¸šåŠ¡æ•°æ®ä¿å­˜åœ¨ output/detail/ ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åï¼šparsed_kol_data.json
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, List, Any


def load_api_key():
    """
    ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key
    
    Returns:
        str: API Key
    
    Raises:
        ValueError: å¦‚æœ API Key æœªè®¾ç½®
    """
    # å®šä½åˆ° backend/.env æ–‡ä»¶
    backend_dir = Path(__file__).parent.parent.parent.parent.parent  # è¿”å›åˆ° backend ç›®å½•
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨ {env_path} æ–‡ä»¶ä¸­é…ç½®")
    
    return api_key


def fetch_kol_page(api_key: str, keyword: str, page: int, count: int = 20) -> Dict[str, Any]:
    """
    è°ƒç”¨ TikHub API çš„æ˜Ÿå›¾ search_kol_v1 æ¥å£è·å–ä¸€é¡µ KOL æ•°æ®
    
    Args:
        api_key: TikHub API å¯†é’¥
        keyword: æœç´¢å…³é”®è¯
        page: é¡µç ï¼Œä» 1 å¼€å§‹
        count: æ¯é¡µè¿”å›æ•°é‡ï¼Œé»˜è®¤ 20
        
    Returns:
        dict: API å“åº”çš„å®Œæ•´ JSON æ•°æ®
    """
    # API ç«¯ç‚¹
    url = "https://api.tikhub.io/api/v1/douyin/xingtu/search_kol_v1"
    
    # è¯·æ±‚å¤´
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    # æŸ¥è¯¢å‚æ•°
    params = {
        'keyword': keyword,
        'page': page,
        'count': count,
        'sort_type': 1,  # ç»¼åˆæ’åº
        'platformSource': '_1'  # æŠ–éŸ³å¹³å°
    }
    
    print(f"\nğŸ“¡ å‘é€è¯·æ±‚: ç¬¬ {page} é¡µ...")
    print(f"   å…³é”®è¯: {keyword}")
    print(f"   æ¯é¡µæ•°é‡: {count}")
    
    try:
        # å‘é€ GET è¯·æ±‚
        response = requests.get(url, headers=headers, params=params, timeout=30)
        
        print(f"   HTTP çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            
            # æ£€æŸ¥ API è¿”å›ç 
            code = result.get('code', -1)
            message = result.get('message_zh', result.get('message', 'Unknown'))
            
            print(f"   API è¿”å›ç : {code}")
            print(f"   è¿”å›æ¶ˆæ¯: {message}")
            
            if code == 200:
                # è·å–ä½œè€…åˆ—è¡¨
                data = result.get('data', {})
                authors = data.get('authors', [])
                pagination = data.get('pagination', {})
                
                print(f"   âœ… æˆåŠŸè·å– {len(authors)} ä¸ªè¾¾äºº")
                print(f"   è¿˜æœ‰æ›´å¤šæ•°æ®: {pagination.get('has_more', False)}")
                
                return result
            else:
                print(f"   âŒ API è¿”å›é”™è¯¯")
                return result
        else:
            print(f"   âŒ HTTP è¯·æ±‚å¤±è´¥: {response.text[:200]}")
            return {
                "error": f"HTTP {response.status_code}",
                "message": response.text
            }
            
    except Exception as e:
        print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
        return {"error": str(e)}


def parse_kol_data(author: Dict[str, Any]) -> Dict[str, Any]:
    """
    è§£æå•ä¸ªè¾¾äººçš„å…³é”®ä¸šåŠ¡æ•°æ®
    
    Args:
        author: åŸå§‹è¾¾äººæ•°æ®
        
    Returns:
        dict: è§£æåçš„å…³é”®ä¸šåŠ¡æ•°æ®
    """
    attr_data = author.get('attribute_datas', {})
    
    # è§£ææ ‡ç­¾
    tags_relation_str = attr_data.get('tags_relation', '{}')
    try:
        tags_relation = json.loads(tags_relation_str) if tags_relation_str else {}
    except:
        tags_relation = {}
    
    # è§£ææœ€è¿‘10ä¸ªä½œå“
    last_10_items_str = attr_data.get('last_10_items', '[]')
    try:
        last_10_items = json.loads(last_10_items_str) if last_10_items_str else []
    except:
        last_10_items = []
    
    # è®¡ç®—ä½œå“å¹³å‡æ•°æ®
    total_vv = 0
    total_like = 0
    total_comment = 0
    total_share = 0
    item_count = len(last_10_items)
    
    for item in last_10_items:
        total_vv += int(item.get('vv', 0))
        total_like += int(item.get('like_cnt', 0))
        total_comment += int(item.get('comment_cnt', 0))
        total_share += int(item.get('share_cnt', 0))
    
    avg_vv = total_vv // item_count if item_count > 0 else 0
    avg_like = total_like // item_count if item_count > 0 else 0
    avg_comment = total_comment // item_count if item_count > 0 else 0
    avg_share = total_share // item_count if item_count > 0 else 0
    
    # è§£ææŠ¥ä»·ä¿¡æ¯
    task_infos = author.get('task_infos', [])
    price_info = {}
    
    if task_infos:
        task_info = task_infos[0]
        price_infos = task_info.get('price_infos', [])
        
        for price in price_infos:
            video_type = price.get('video_type', 0)
            price_value = price.get('price', 0)
            
            if video_type == 1:
                price_info['è§†é¢‘å®šåˆ¶'] = price_value
            elif video_type == 2:
                price_info['å›¾æ–‡å®šåˆ¶'] = price_value
    
    # æ„å»ºå…³é”®ä¸šåŠ¡æ•°æ®
    kol_data = {
        # åŸºæœ¬ä¿¡æ¯
        'è¾¾äººID': author.get('star_id', ''),
        'æ˜µç§°': attr_data.get('nick_name', ''),
        'å¤´åƒ': attr_data.get('avatar_uri', ''),
        'æ€§åˆ«': 'å¥³' if attr_data.get('gender', '') == '2' else 'ç”·' if attr_data.get('gender', '') == '1' else 'æœªçŸ¥',
        'åœ°åŒº': f"{attr_data.get('province', '')} {attr_data.get('city', '')}".strip(),
        
        # ç²‰ä¸æ•°æ®
        'ç²‰ä¸æ•°': int(attr_data.get('follower', 0)),
        '15å¤©ç²‰ä¸å¢é‡': int(attr_data.get('fans_increment_within_15d', 0)),
        '30å¤©ç²‰ä¸å¢é‡': attr_data.get('fans_increment_within_30d', '0'),
        '15å¤©ç²‰ä¸å¢é•¿ç‡': float(attr_data.get('fans_increment_rate_within_15d', 0)),
        
        # æ˜Ÿå›¾æ•°æ®
        'æ˜Ÿå›¾è¯„åˆ†': float(attr_data.get('star_index', 0)),
        'ç²‰ä¸ç­‰çº§': attr_data.get('grade', '0'),
        'è¾¾äººç±»å‹': 'ä¸ªäºº' if attr_data.get('author_type', '') == '1' else 'æœºæ„' if attr_data.get('author_type', '') == '2' else 'æœªçŸ¥',
        'è´¦å·çŠ¶æ€': 'æ­£å¸¸' if attr_data.get('author_status', '') == '1' else 'å¼‚å¸¸',
        
        # å†…å®¹æ ‡ç­¾
        'å†…å®¹æ ‡ç­¾': tags_relation,
        
        # ä½œå“æ•°æ®
        'è¿‘æœŸä½œå“æ•°': item_count,
        'å¹³å‡æ’­æ”¾é‡': avg_vv,
        'å¹³å‡ç‚¹èµæ•°': avg_like,
        'å¹³å‡è¯„è®ºæ•°': avg_comment,
        'å¹³å‡åˆ†äº«æ•°': avg_share,
        'äº’åŠ¨ç‡': round((avg_like + avg_comment + avg_share) / avg_vv * 100, 2) if avg_vv > 0 else 0,
        
        # ç”µå•†æ•°æ®
        'ç”µå•†ç­‰çº§': attr_data.get('author_ecom_level', ''),
        'ç”µå•†å¯ç”¨': attr_data.get('e_commerce_enable', '0') == '1',
        '30å¤©å¸¦è´§è§†é¢‘æ•°': int(attr_data.get('ecom_video_product_num_30d', 0)),
        '30å¤©å¸¦è´§GMVåŒºé—´': attr_data.get('ecom_gmv_30d_range', ''),
        '30å¤©å¹³å‡å®¢å•ä»·åŒºé—´': attr_data.get('ecom_avg_order_value_30d_range', ''),
        
        # å•†ä¸šæŠ¥ä»·
        'æŠ¥ä»·ä¿¡æ¯': price_info,
        '1-20ç§’è§†é¢‘æŠ¥ä»·': int(attr_data.get('price_1_20', 0)),
        '20-60ç§’è§†é¢‘æŠ¥ä»·': int(attr_data.get('price_20_60', 0)),
        '60ç§’ä»¥ä¸Šè§†é¢‘æŠ¥ä»·': int(attr_data.get('price_60', 0)),
        
        # é¢„ä¼°æ•°æ®
        'é¢„ä¼°æ’­æ”¾é‡': int(attr_data.get('expected_play_num', 0)),
        'é¢„ä¼°è‡ªç„¶æ’­æ”¾é‡': int(attr_data.get('expected_natural_play_num', 0)),
        
        # ç‰¹æ®Šæ ‡è®°
        'æ˜¯å¦é»‘é©¬è¾¾äºº': attr_data.get('is_black_horse_author', 'false') == 'true',
        'æ˜¯å¦ä¼˜è´¨è¾¾äºº': attr_data.get('is_excellenct_author', '0') == '1',
        'æ˜¯å¦çŸ­å‰§è¾¾äºº': attr_data.get('is_short_drama', '0') == '1',
        'æ˜¯å¦æ”¯æŒå…±åˆ›': attr_data.get('is_cocreate_author', 'false') == 'true',
        
        # æœ€è¿‘10ä¸ªä½œå“è¯¦æƒ…
        'æœ€è¿‘ä½œå“åˆ—è¡¨': last_10_items
    }
    
    return kol_data


def save_raw_response(response_data: Dict[str, Any], page: int, output_dir: str):
    """
    ä¿å­˜åŸå§‹ API å“åº”åˆ° detail ç›®å½•
    
    Args:
        response_data: API å“åº”æ•°æ®
        page: é¡µç 
        output_dir: è¾“å‡ºç›®å½•
    """
    detail_dir = os.path.join(output_dir, 'detail')
    os.makedirs(detail_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'raw_page_{page}_{timestamp}.json'
    filepath = os.path.join(detail_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(response_data, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ’¾ åŸå§‹è¿”å›å·²ä¿å­˜: {filepath}")


def check_duplicate_authors(all_pages_data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ£€æŸ¥3é¡µæ•°æ®ä¸­æ˜¯å¦æœ‰é‡å¤çš„è¾¾äºº
    
    Args:
        all_pages_data: æ‰€æœ‰é¡µé¢çš„æ•°æ®åˆ—è¡¨
        
    Returns:
        dict: é‡å¤æ£€æŸ¥ç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” æ£€æŸ¥æ•°æ®é‡å¤æƒ…å†µ")
    print(f"{'='*60}")
    
    # æ”¶é›†æ‰€æœ‰è¾¾äººID
    author_ids_by_page = []
    
    for i, page_data in enumerate(all_pages_data, 1):
        data = page_data.get('data', {})
        authors = data.get('authors', [])
        
        author_ids = [author.get('star_id', '') for author in authors]
        author_ids_by_page.append({
            'page': i,
            'count': len(author_ids),
            'ids': author_ids
        })
        
        print(f"\nç¬¬ {i} é¡µ:")
        print(f"  è¾¾äººæ•°é‡: {len(author_ids)}")
    
    # æ£€æŸ¥è·¨é¡µé‡å¤
    all_ids = []
    for page_info in author_ids_by_page:
        all_ids.extend(page_info['ids'])
    
    unique_ids = set(all_ids)
    total_count = len(all_ids)
    unique_count = len(unique_ids)
    duplicate_count = total_count - unique_count
    
    print(f"\næ€»è®¡:")
    print(f"  æ€»è¾¾äººæ•°: {total_count}")
    print(f"  å”¯ä¸€è¾¾äººæ•°: {unique_count}")
    print(f"  é‡å¤è¾¾äººæ•°: {duplicate_count}")
    
    # æ‰¾å‡ºé‡å¤çš„è¾¾äººID
    duplicate_ids = []
    for author_id in unique_ids:
        count = all_ids.count(author_id)
        if count > 1:
            duplicate_ids.append({
                'id': author_id,
                'count': count
            })
    
    if duplicate_ids:
        print(f"\nâš ï¸ å‘ç° {len(duplicate_ids)} ä¸ªé‡å¤çš„è¾¾äºº:")
        for dup in duplicate_ids[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  ID: {dup['id']} (å‡ºç° {dup['count']} æ¬¡)")
        if len(duplicate_ids) > 5:
            print(f"  ... è¿˜æœ‰ {len(duplicate_ids) - 5} ä¸ªé‡å¤è¾¾äºº")
    else:
        print(f"\nâœ… æ²¡æœ‰å‘ç°é‡å¤çš„è¾¾äºº")
    
    return {
        'total_count': total_count,
        'unique_count': unique_count,
        'duplicate_count': duplicate_count,
        'duplicate_ids': duplicate_ids,
        'pages': author_ids_by_page
    }


def main():
    """ä¸»å‡½æ•°ï¼šè·å–3é¡µæ•°æ®å¹¶ç»“æ„åŒ–åˆ†æ"""
    
    print("=" * 60)
    print("æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººæœç´¢ä¸ç»“æ„åŒ–åˆ†æå·¥å…· V2")
    print("API æ¥å£: search_kol_v1")
    print("=" * 60)
    
    # 1. åŠ è½½ API Key
    print("\n1ï¸âƒ£ åŠ è½½ API é…ç½®...")
    try:
        api_key = load_api_key()
        print(f"âœ… API Key å·²åŠ è½½: {api_key[:10]}...{api_key[-10:]}")
    except ValueError as e:
        print(f"âŒ {e}")
        return
    
    # 2. è®¾ç½®è¾“å‡ºç›®å½•
    script_dir = Path(__file__).parent.parent
    output_dir = script_dir / "output"
    
    # 3. è·å–3é¡µæ•°æ®
    print("\n2ï¸âƒ£ å¼€å§‹è·å–3é¡µæ•°æ®...")
    keyword = "æŠ¤è‚¤"
    page_count = 3
    count_per_page = 20
    
    all_pages_data = []
    all_parsed_kols = []
    
    for page in range(1, page_count + 1):
        print(f"\n{'='*60}")
        print(f"ç¬¬ {page}/{page_count} é¡µ")
        print(f"{'='*60}")
        
        # è·å–æ•°æ®
        response_data = fetch_kol_page(api_key, keyword, page, count_per_page)
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
        if 'error' in response_data or response_data.get('code') != 200:
            print(f"âš ï¸ ç¬¬ {page} é¡µè·å–å¤±è´¥")
            continue
        
        # ä¿å­˜åŸå§‹è¿”å›
        save_raw_response(response_data, page, str(output_dir))
        
        # ä¿å­˜åˆ°åˆ—è¡¨
        all_pages_data.append(response_data)
        
        # è§£æå…³é”®ä¸šåŠ¡æ•°æ®
        data = response_data.get('data', {})
        authors = data.get('authors', [])
        
        print(f"\n   å¼€å§‹è§£æå…³é”®ä¸šåŠ¡æ•°æ®...")
        for author in authors:
            parsed_kol = parse_kol_data(author)
            all_parsed_kols.append(parsed_kol)
        
        print(f"   âœ… è§£æå®Œæˆ {len(authors)} ä¸ªè¾¾äºº")
        
        # æ·»åŠ å»¶è¿Ÿ
        if page < page_count:
            print(f"\n   â³ ç­‰å¾… 1 ç§’...")
            time.sleep(1)
    
    if not all_pages_data:
        print("\nâŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
        return
    
    # 4. æ£€æŸ¥é‡å¤
    print("\n3ï¸âƒ£ æ£€æŸ¥æ•°æ®é‡å¤...")
    duplicate_check = check_duplicate_authors(all_pages_data)
    
    # 5. ä¿å­˜è§£æåçš„å…³é”®ä¸šåŠ¡æ•°æ®
    print(f"\n4ï¸âƒ£ ä¿å­˜è§£æåçš„æ•°æ®...")
    
    detail_dir = os.path.join(output_dir, 'detail')
    os.makedirs(detail_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    parsed_file = os.path.join(detail_dir, f'parsed_kol_data_{timestamp}.json')
    
    # æ„å»ºè¾“å‡ºæ•°æ®ç»“æ„
    output_data = {
        'è¯´æ˜': {
            'æ•°æ®æ¥æº': 'æŠ–éŸ³æ˜Ÿå›¾ KOL æœç´¢ API (search_kol_v1)',
            'æ¥å£æ–‡æ¡£': 'https://api.tikhub.io/#/Douyin-Xingtu-API/search_kol_v1_api_v1_douyin_xingtu_search_kol_v1_get',
            'æœç´¢å…³é”®è¯': keyword,
            'è·å–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'æ•°æ®é¡µæ•°': page_count,
            'æ¯é¡µæ•°é‡': count_per_page,
            'æ€»è¾¾äººæ•°': len(all_parsed_kols),
            'å”¯ä¸€è¾¾äººæ•°': duplicate_check['unique_count'],
            'é‡å¤è¾¾äººæ•°': duplicate_check['duplicate_count']
        },
        'æ•°æ®å­—æ®µè¯´æ˜': {
            'åŸºæœ¬ä¿¡æ¯': ['è¾¾äººID', 'æ˜µç§°', 'å¤´åƒ', 'æ€§åˆ«', 'åœ°åŒº'],
            'ç²‰ä¸æ•°æ®': ['ç²‰ä¸æ•°', '15å¤©ç²‰ä¸å¢é‡', '30å¤©ç²‰ä¸å¢é‡', '15å¤©ç²‰ä¸å¢é•¿ç‡'],
            'æ˜Ÿå›¾æ•°æ®': ['æ˜Ÿå›¾è¯„åˆ†', 'ç²‰ä¸ç­‰çº§', 'è¾¾äººç±»å‹', 'è´¦å·çŠ¶æ€'],
            'å†…å®¹æ ‡ç­¾': ['å†…å®¹æ ‡ç­¾'],
            'ä½œå“æ•°æ®': ['è¿‘æœŸä½œå“æ•°', 'å¹³å‡æ’­æ”¾é‡', 'å¹³å‡ç‚¹èµæ•°', 'å¹³å‡è¯„è®ºæ•°', 'å¹³å‡åˆ†äº«æ•°', 'äº’åŠ¨ç‡'],
            'ç”µå•†æ•°æ®': ['ç”µå•†ç­‰çº§', 'ç”µå•†å¯ç”¨', '30å¤©å¸¦è´§è§†é¢‘æ•°', '30å¤©å¸¦è´§GMVåŒºé—´', '30å¤©å¹³å‡å®¢å•ä»·åŒºé—´'],
            'å•†ä¸šæŠ¥ä»·': ['æŠ¥ä»·ä¿¡æ¯', '1-20ç§’è§†é¢‘æŠ¥ä»·', '20-60ç§’è§†é¢‘æŠ¥ä»·', '60ç§’ä»¥ä¸Šè§†é¢‘æŠ¥ä»·'],
            'é¢„ä¼°æ•°æ®': ['é¢„ä¼°æ’­æ”¾é‡', 'é¢„ä¼°è‡ªç„¶æ’­æ”¾é‡'],
            'ç‰¹æ®Šæ ‡è®°': ['æ˜¯å¦é»‘é©¬è¾¾äºº', 'æ˜¯å¦ä¼˜è´¨è¾¾äºº', 'æ˜¯å¦çŸ­å‰§è¾¾äºº', 'æ˜¯å¦æ”¯æŒå…±åˆ›'],
            'ä½œå“è¯¦æƒ…': ['æœ€è¿‘ä½œå“åˆ—è¡¨']
        },
        'é‡å¤æ£€æŸ¥ç»“æœ': duplicate_check,
        'è¾¾äººæ•°æ®': all_parsed_kols
    }
    
    with open(parsed_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ è§£æåçš„æ•°æ®å·²ä¿å­˜: {parsed_file}")
    
    # 6. ç”Ÿæˆåˆ†ææŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ“Š å…³é”®æ•°æ®åˆ†æ")
    print(f"{'='*60}")
    
    # ç²‰ä¸æ•°ç»Ÿè®¡
    follower_counts = [kol['ç²‰ä¸æ•°'] for kol in all_parsed_kols]
    avg_followers = sum(follower_counts) // len(follower_counts) if follower_counts else 0
    max_followers = max(follower_counts) if follower_counts else 0
    min_followers = min(follower_counts) if follower_counts else 0
    
    print(f"\nç²‰ä¸æ•°ç»Ÿè®¡:")
    print(f"  å¹³å‡ç²‰ä¸æ•°: {avg_followers:,}")
    print(f"  æœ€é«˜ç²‰ä¸æ•°: {max_followers:,}")
    print(f"  æœ€ä½ç²‰ä¸æ•°: {min_followers:,}")
    
    # æ˜Ÿå›¾è¯„åˆ†ç»Ÿè®¡
    star_scores = [kol['æ˜Ÿå›¾è¯„åˆ†'] for kol in all_parsed_kols if kol['æ˜Ÿå›¾è¯„åˆ†'] > 0]
    avg_star_score = sum(star_scores) / len(star_scores) if star_scores else 0
    
    print(f"\næ˜Ÿå›¾è¯„åˆ†ç»Ÿè®¡:")
    print(f"  å¹³å‡æ˜Ÿå›¾è¯„åˆ†: {avg_star_score:.2f}")
    print(f"  æœ€é«˜æ˜Ÿå›¾è¯„åˆ†: {max(star_scores):.2f}" if star_scores else "  æ— æ•°æ®")
    print(f"  æœ€ä½æ˜Ÿå›¾è¯„åˆ†: {min(star_scores):.2f}" if star_scores else "  æ— æ•°æ®")
    
    # äº’åŠ¨ç‡ç»Ÿè®¡
    interaction_rates = [kol['äº’åŠ¨ç‡'] for kol in all_parsed_kols if kol['äº’åŠ¨ç‡'] > 0]
    avg_interaction_rate = sum(interaction_rates) / len(interaction_rates) if interaction_rates else 0
    
    print(f"\näº’åŠ¨ç‡ç»Ÿè®¡:")
    print(f"  å¹³å‡äº’åŠ¨ç‡: {avg_interaction_rate:.2f}%")
    print(f"  æœ€é«˜äº’åŠ¨ç‡: {max(interaction_rates):.2f}%" if interaction_rates else "  æ— æ•°æ®")
    
    # æŠ¥ä»·ç»Ÿè®¡
    prices_20_60 = [kol['20-60ç§’è§†é¢‘æŠ¥ä»·'] for kol in all_parsed_kols if kol['20-60ç§’è§†é¢‘æŠ¥ä»·'] > 0]
    avg_price = sum(prices_20_60) // len(prices_20_60) if prices_20_60 else 0
    
    print(f"\nå•†ä¸šæŠ¥ä»·ç»Ÿè®¡ (20-60ç§’è§†é¢‘):")
    print(f"  å¹³å‡æŠ¥ä»·: {avg_price:,} å…ƒ")
    print(f"  æœ€é«˜æŠ¥ä»·: {max(prices_20_60):,} å…ƒ" if prices_20_60 else "  æ— æ•°æ®")
    print(f"  æœ€ä½æŠ¥ä»·: {min(prices_20_60):,} å…ƒ" if prices_20_60 else "  æ— æ•°æ®")
    
    # TOP 5 è¾¾äºº
    print(f"\n{'='*60}")
    print(f"ğŸ† TOP 5 æŠ¤è‚¤è¾¾äºº (æŒ‰ç²‰ä¸æ•°æ’åº)")
    print(f"{'='*60}")
    
    sorted_kols = sorted(all_parsed_kols, key=lambda x: x['ç²‰ä¸æ•°'], reverse=True)
    
    for i, kol in enumerate(sorted_kols[:5], 1):
        print(f"\n{i}. {kol['æ˜µç§°']}")
        print(f"   ç²‰ä¸æ•°: {kol['ç²‰ä¸æ•°']:,}")
        print(f"   æ˜Ÿå›¾è¯„åˆ†: {kol['æ˜Ÿå›¾è¯„åˆ†']:.2f}")
        print(f"   å¹³å‡æ’­æ”¾é‡: {kol['å¹³å‡æ’­æ”¾é‡']:,}")
        print(f"   äº’åŠ¨ç‡: {kol['äº’åŠ¨ç‡']}%")
        print(f"   20-60ç§’è§†é¢‘æŠ¥ä»·: {kol['20-60ç§’è§†é¢‘æŠ¥ä»·']:,} å…ƒ")
        print(f"   å†…å®¹æ ‡ç­¾: {kol['å†…å®¹æ ‡ç­¾']}")
    
    print(f"\n{'='*60}")
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"\nğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®:")
    print(f"   åŸå§‹è¿”å›: {os.path.join(output_dir, 'detail', 'raw_page_*.json')}")
    print(f"   è§£ææ•°æ®: {parsed_file}")


if __name__ == "__main__":
    main()

