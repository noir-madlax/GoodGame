#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººå¯¹æ¯”æœç´¢è„šæœ¬

åŠŸèƒ½ï¼š
1. ä½¿ç”¨"ç¾å¦†æŠ¤è‚¤"å’Œ"æŠ¤è‚¤ä¿å…»"ä¸¤ä¸ªå…³é”®è¯åˆ†åˆ«æœç´¢10é¡µæ•°æ®
2. å°†æ•°æ®ä¿å­˜åˆ°ä¸åŒçš„ç›®å½•
3. åˆ†æä¸¤ä¸ªå…³é”®è¯çš„ç»“æœé‡åˆåº¦ã€tagåˆ†å¸ƒæƒ…å†µã€tagåŒ¹é…å’Œå…³è”æƒ…å†µ
4. ä¸ä¹‹å‰çš„æœç´¢ç»“æœè¿›è¡Œå¯¹æ¯”åˆ†æ

ä½œè€…: AI Agent
åˆ›å»ºæ—¶é—´: 2025-11-18
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from collections import defaultdict, Counter


def load_api_key():
    """
    ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key
    
    Returns:
        str: API Key
    
    Raises:
        ValueError: å¦‚æœ API Key æœªè®¾ç½®
    """
    # å®šä½åˆ° backend/.env æ–‡ä»¶
    backend_dir = Path(__file__).parent.parent.parent.parent.parent  # è¿”å›åˆ° backend ç›®å½•
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨ {env_path} æ–‡ä»¶ä¸­é…ç½®")
    
    return api_key


def search_kol(keyword, page=1, count=20, sort_type=1, api_key=None):
    """
    è°ƒç”¨ TikHub API æœç´¢æ˜Ÿå›¾ KOL
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        page: é¡µç ï¼Œä»1å¼€å§‹
        count: æ¯é¡µæ•°é‡ï¼Œé»˜è®¤20
        sort_type: æ’åºæ–¹å¼ï¼Œ1=ç»¼åˆæ’åº
        api_key: API Key
        
    Returns:
        dict: API å“åº”æ•°æ®
    """
    url = "https://api.tikhub.io/api/v1/douyin/xingtu/search_kol_v1"
    
    params = {
        "keyword": keyword,
        "page": str(page),
        "count": str(count),
        "sort_type": str(sort_type),
        "platformSource": "_1"  # æŠ–éŸ³å¹³å°
    }
    
    headers = {
        "Authorization": f"Bearer {api_key}"
    }
    
    try:
        response = requests.get(url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"âŒ API è¯·æ±‚å¤±è´¥: {e}")
        return None


def fetch_keyword_data(keyword, pages=10, api_key=None, output_dir=None):
    """
    è·å–æŒ‡å®šå…³é”®è¯çš„å¤šé¡µæ•°æ®
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        pages: è·å–é¡µæ•°
        api_key: API Key
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        list: æ‰€æœ‰è¾¾äººæ•°æ®åˆ—è¡¨
    """
    all_authors = []
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    detail_dir = output_dir / 'detail'
    detail_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è·å–å…³é”®è¯ '{keyword}' çš„æ•°æ®ï¼Œå…± {pages} é¡µ")
    print(f"{'='*60}\n")
    
    for page in range(1, pages + 1):
        print(f"ğŸ“„ æ­£åœ¨è·å–ç¬¬ {page}/{pages} é¡µ...")
        
        data = search_kol(keyword=keyword, page=page, api_key=api_key)
        
        if data and data.get('code') == 200:
            # ä¿å­˜åŸå§‹æ•°æ®
            filename = f"raw_page_{page}_{timestamp}.json"
            filepath = detail_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # æå–è¾¾äººæ•°æ®
            authors = data.get('data', {}).get('authors', [])
            all_authors.extend(authors)
            
            print(f"âœ… ç¬¬ {page} é¡µè·å–æˆåŠŸï¼Œè¾¾äººæ•°: {len(authors)}")
            print(f"   å·²ä¿å­˜åˆ°: {filepath}")
            
        else:
            print(f"âŒ ç¬¬ {page} é¡µè·å–å¤±è´¥")
            if data:
                print(f"   é”™è¯¯ä¿¡æ¯: {data.get('message', 'Unknown error')}")
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        if page < pages:
            time.sleep(2)
    
    print(f"\nâœ… å…³é”®è¯ '{keyword}' æ•°æ®è·å–å®Œæˆï¼Œå…± {len(all_authors)} ä¸ªè¾¾äºº\n")
    
    return all_authors


def analyze_authors_tags(authors, keyword):
    """
    åˆ†æè¾¾äººçš„æ ‡ç­¾åˆ†å¸ƒ
    
    Args:
        authors: è¾¾äººåˆ—è¡¨
        keyword: å…³é”®è¯åç§°
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    tag_counts = Counter()  # æ ‡ç­¾è®¡æ•°
    category_tags = defaultdict(Counter)  # åˆ†ç±» -> æ ‡ç­¾è®¡æ•°
    author_tags = {}  # è¾¾äººID -> æ ‡ç­¾åˆ—è¡¨
    
    for author in authors:
        author_id = author.get('attribute_datas', {}).get('id', 'unknown')
        tags_relation_str = author.get('attribute_datas', {}).get('tags_relation', '{}')
        
        try:
            tags_relation = json.loads(tags_relation_str)
            author_tag_list = []
            
            for category, tags in tags_relation.items():
                if isinstance(tags, list):
                    for tag in tags:
                        tag_counts[tag] += 1
                        category_tags[category][tag] += 1
                        author_tag_list.append(tag)
                else:
                    tag_counts[tags] += 1
                    category_tags[category][tags] += 1
                    author_tag_list.append(tags)
            
            author_tags[author_id] = author_tag_list
            
        except json.JSONDecodeError:
            pass
    
    return {
        'tag_counts': tag_counts,
        'category_tags': category_tags,
        'author_tags': author_tags,
        'total_authors': len(authors)
    }


def calculate_overlap(authors1, authors2):
    """
    è®¡ç®—ä¸¤ä¸ªè¾¾äººåˆ—è¡¨çš„é‡åˆåº¦
    
    Args:
        authors1: ç¬¬ä¸€ä¸ªè¾¾äººåˆ—è¡¨
        authors2: ç¬¬äºŒä¸ªè¾¾äººåˆ—è¡¨
        
    Returns:
        dict: é‡åˆåº¦åˆ†æç»“æœ
    """
    ids1 = set(a.get('attribute_datas', {}).get('id') for a in authors1)
    ids2 = set(a.get('attribute_datas', {}).get('id') for a in authors2)
    
    overlap = ids1 & ids2
    only_in_1 = ids1 - ids2
    only_in_2 = ids2 - ids1
    
    return {
        'total_1': len(ids1),
        'total_2': len(ids2),
        'overlap_count': len(overlap),
        'overlap_rate_1': len(overlap) / len(ids1) * 100 if ids1 else 0,
        'overlap_rate_2': len(overlap) / len(ids2) * 100 if ids2 else 0,
        'only_in_1': len(only_in_1),
        'only_in_2': len(only_in_2),
        'overlap_ids': overlap
    }


def compare_with_original(new_authors, original_dir):
    """
    ä¸åŸå§‹æœç´¢ç»“æœè¿›è¡Œå¯¹æ¯”
    
    Args:
        new_authors: æ–°æœç´¢çš„è¾¾äººåˆ—è¡¨
        original_dir: åŸå§‹æ•°æ®ç›®å½•
        
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    # è¯»å–åŸå§‹æ•°æ®
    original_authors = []
    detail_dir = original_dir / 'detail'
    
    if not detail_dir.exists():
        print(f"âš ï¸ åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {detail_dir}")
        return None
    
    for file in detail_dir.glob('raw_page_*.json'):
        try:
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                authors = data.get('data', {}).get('authors', [])
                original_authors.extend(authors)
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file}: {e}")
    
    if not original_authors:
        print("âš ï¸ æœªæ‰¾åˆ°åŸå§‹æ•°æ®")
        return None
    
    # è®¡ç®—é‡åˆåº¦
    overlap = calculate_overlap(new_authors, original_authors)
    
    return {
        'original_total': len(original_authors),
        'new_total': len(new_authors),
        'overlap': overlap
    }


def generate_analysis_report(keyword1, authors1, analysis1, 
                            keyword2, authors2, analysis2,
                            overlap_result, 
                            original_comparison1, original_comparison2,
                            output_base_dir):
    """
    ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š
    
    Args:
        keyword1: ç¬¬ä¸€ä¸ªå…³é”®è¯
        authors1: ç¬¬ä¸€ä¸ªå…³é”®è¯çš„è¾¾äººåˆ—è¡¨
        analysis1: ç¬¬ä¸€ä¸ªå…³é”®è¯çš„åˆ†æç»“æœ
        keyword2: ç¬¬äºŒä¸ªå…³é”®è¯
        authors2: ç¬¬äºŒä¸ªå…³é”®è¯çš„è¾¾äººåˆ—è¡¨
        analysis2: ç¬¬äºŒä¸ªå…³é”®è¯çš„åˆ†æç»“æœ
        overlap_result: ä¸¤ä¸ªå…³é”®è¯çš„é‡åˆåº¦ç»“æœ
        original_comparison1: å…³é”®è¯1ä¸åŸå§‹æ•°æ®çš„å¯¹æ¯”ç»“æœ
        original_comparison2: å…³é”®è¯2ä¸åŸå§‹æ•°æ®çš„å¯¹æ¯”ç»“æœ
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = output_base_dir / f'å¯¹æ¯”åˆ†ææŠ¥å‘Š_{timestamp}.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººå…³é”®è¯å¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**å¯¹æ¯”å…³é”®è¯**: \n")
        f.write(f"- å…³é”®è¯1: {keyword1}\n")
        f.write(f"- å…³é”®è¯2: {keyword2}\n\n")
        f.write(f"{'='*80}\n\n")
        
        # 1. æ•°æ®æ¦‚è§ˆ
        f.write(f"## ä¸€ã€æ•°æ®æ¦‚è§ˆ\n\n")
        f.write(f"| å…³é”®è¯ | æ€»è¾¾äººæ•° | æœ‰æ ‡ç­¾è¾¾äººæ•° | æ ‡ç­¾ç§ç±»æ•° |\n")
        f.write(f"|--------|---------|-------------|------------|\n")
        f.write(f"| {keyword1} | {analysis1['total_authors']} | {len(analysis1['author_tags'])} | {len(analysis1['tag_counts'])} |\n")
        f.write(f"| {keyword2} | {analysis2['total_authors']} | {len(analysis2['author_tags'])} | {len(analysis2['tag_counts'])} |\n\n")
        
        # 2. é‡åˆåº¦åˆ†æ
        f.write(f"## äºŒã€ä¸¤ä¸ªå…³é”®è¯çš„é‡åˆåº¦åˆ†æ\n\n")
        f.write(f"### 2.1 è¾¾äººé‡åˆæƒ…å†µ\n\n")
        f.write(f"| æŒ‡æ ‡ | æ•°å€¼ |\n")
        f.write(f"|------|------|\n")
        f.write(f"| {keyword1} æ€»è¾¾äººæ•° | {overlap_result['total_1']} |\n")
        f.write(f"| {keyword2} æ€»è¾¾äººæ•° | {overlap_result['total_2']} |\n")
        f.write(f"| é‡åˆè¾¾äººæ•° | {overlap_result['overlap_count']} |\n")
        f.write(f"| {keyword1} é‡åˆç‡ | {overlap_result['overlap_rate_1']:.2f}% |\n")
        f.write(f"| {keyword2} é‡åˆç‡ | {overlap_result['overlap_rate_2']:.2f}% |\n")
        f.write(f"| ä»…åœ¨ {keyword1} | {overlap_result['only_in_1']} |\n")
        f.write(f"| ä»…åœ¨ {keyword2} | {overlap_result['only_in_2']} |\n\n")
        
        # 3. ä¸åŸå§‹æ•°æ®å¯¹æ¯”
        f.write(f"## ä¸‰ã€ä¸åŸå§‹'æŠ¤è‚¤'æœç´¢ç»“æœçš„å¯¹æ¯”\n\n")
        
        if original_comparison1:
            f.write(f"### 3.1 '{keyword1}' ä¸åŸå§‹æ•°æ®çš„é‡åˆåº¦\n\n")
            f.write(f"| æŒ‡æ ‡ | æ•°å€¼ |\n")
            f.write(f"|------|------|\n")
            f.write(f"| åŸå§‹æ•°æ®æ€»è¾¾äººæ•° | {original_comparison1['original_total']} |\n")
            f.write(f"| {keyword1} æ€»è¾¾äººæ•° | {original_comparison1['new_total']} |\n")
            f.write(f"| é‡åˆè¾¾äººæ•° | {original_comparison1['overlap']['overlap_count']} |\n")
            f.write(f"| é‡åˆç‡ï¼ˆç›¸å¯¹åŸå§‹æ•°æ®ï¼‰ | {original_comparison1['overlap']['overlap_rate_1']:.2f}% |\n")
            f.write(f"| é‡åˆç‡ï¼ˆç›¸å¯¹æ–°æ•°æ®ï¼‰ | {original_comparison1['overlap']['overlap_rate_2']:.2f}% |\n\n")
        
        if original_comparison2:
            f.write(f"### 3.2 '{keyword2}' ä¸åŸå§‹æ•°æ®çš„é‡åˆåº¦\n\n")
            f.write(f"| æŒ‡æ ‡ | æ•°å€¼ |\n")
            f.write(f"|------|------|\n")
            f.write(f"| åŸå§‹æ•°æ®æ€»è¾¾äººæ•° | {original_comparison2['original_total']} |\n")
            f.write(f"| {keyword2} æ€»è¾¾äººæ•° | {original_comparison2['new_total']} |\n")
            f.write(f"| é‡åˆè¾¾äººæ•° | {original_comparison2['overlap']['overlap_count']} |\n")
            f.write(f"| é‡åˆç‡ï¼ˆç›¸å¯¹åŸå§‹æ•°æ®ï¼‰ | {original_comparison2['overlap']['overlap_rate_1']:.2f}% |\n")
            f.write(f"| é‡åˆç‡ï¼ˆç›¸å¯¹æ–°æ•°æ®ï¼‰ | {original_comparison2['overlap']['overlap_rate_2']:.2f}% |\n\n")
        
        # 4. æ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”
        f.write(f"## å››ã€æ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”\n\n")
        
        f.write(f"### 4.1 '{keyword1}' æ ‡ç­¾åˆ†å¸ƒï¼ˆå‰20åï¼‰\n\n")
        f.write(f"| æ ‡ç­¾ | è¾¾äººæ•° | å æ¯” |\n")
        f.write(f"|------|--------|------|\n")
        for tag, count in analysis1['tag_counts'].most_common(20):
            rate = count / analysis1['total_authors'] * 100
            f.write(f"| {tag} | {count} | {rate:.2f}% |\n")
        f.write(f"\n")
        
        f.write(f"### 4.2 '{keyword2}' æ ‡ç­¾åˆ†å¸ƒï¼ˆå‰20åï¼‰\n\n")
        f.write(f"| æ ‡ç­¾ | è¾¾äººæ•° | å æ¯” |\n")
        f.write(f"|------|--------|------|\n")
        for tag, count in analysis2['tag_counts'].most_common(20):
            rate = count / analysis2['total_authors'] * 100
            f.write(f"| {tag} | {count} | {rate:.2f}% |\n")
        f.write(f"\n")
        
        # 5. æŠ¤è‚¤ç›¸å…³æ ‡ç­¾ç»Ÿè®¡
        f.write(f"## äº”ã€æŠ¤è‚¤ç›¸å…³æ ‡ç­¾ç»Ÿè®¡\n\n")
        
        skincare_keywords = ['æŠ¤è‚¤', 'ç¾å¦†', 'ä¿å…»', 'çš®è‚¤', 'é¢éƒ¨', 'åŒ–å¦†', 'ç¾å®¹']
        
        f.write(f"### 5.1 '{keyword1}' æŠ¤è‚¤ç›¸å…³æ ‡ç­¾\n\n")
        f.write(f"| æ ‡ç­¾ | è¾¾äººæ•° | å æ¯” |\n")
        f.write(f"|------|--------|------|\n")
        skincare_tags1 = {tag: count for tag, count in analysis1['tag_counts'].items() 
                         if any(kw in tag for kw in skincare_keywords)}
        for tag, count in sorted(skincare_tags1.items(), key=lambda x: x[1], reverse=True):
            rate = count / analysis1['total_authors'] * 100
            f.write(f"| {tag} | {count} | {rate:.2f}% |\n")
        f.write(f"| **åˆè®¡** | **{sum(skincare_tags1.values())}** | **{sum(skincare_tags1.values())/analysis1['total_authors']*100:.2f}%** |\n\n")
        
        f.write(f"### 5.2 '{keyword2}' æŠ¤è‚¤ç›¸å…³æ ‡ç­¾\n\n")
        f.write(f"| æ ‡ç­¾ | è¾¾äººæ•° | å æ¯” |\n")
        f.write(f"|------|--------|------|\n")
        skincare_tags2 = {tag: count for tag, count in analysis2['tag_counts'].items() 
                         if any(kw in tag for kw in skincare_keywords)}
        for tag, count in sorted(skincare_tags2.items(), key=lambda x: x[1], reverse=True):
            rate = count / analysis2['total_authors'] * 100
            f.write(f"| {tag} | {count} | {rate:.2f}% |\n")
        f.write(f"| **åˆè®¡** | **{sum(skincare_tags2.values())}** | **{sum(skincare_tags2.values())/analysis2['total_authors']*100:.2f}%** |\n\n")
        
        # 6. åˆ†ç±»æ ‡ç­¾å¯¹æ¯”
        f.write(f"## å…­ã€åˆ†ç±»æ ‡ç­¾å¯¹æ¯”\n\n")
        
        f.write(f"### 6.1 '{keyword1}' åˆ†ç±»ç»Ÿè®¡\n\n")
        f.write(f"| åˆ†ç±» | æ ‡ç­¾ç§ç±» | æ€»è¾¾äººæ•° |\n")
        f.write(f"|------|---------|----------|\n")
        for category, tags_counter in sorted(analysis1['category_tags'].items(), 
                                            key=lambda x: sum(x[1].values()), reverse=True):
            f.write(f"| {category} | {len(tags_counter)} | {sum(tags_counter.values())} |\n")
        f.write(f"\n")
        
        f.write(f"### 6.2 '{keyword2}' åˆ†ç±»ç»Ÿè®¡\n\n")
        f.write(f"| åˆ†ç±» | æ ‡ç­¾ç§ç±» | æ€»è¾¾äººæ•° |\n")
        f.write(f"|------|---------|----------|\n")
        for category, tags_counter in sorted(analysis2['category_tags'].items(), 
                                            key=lambda x: sum(x[1].values()), reverse=True):
            f.write(f"| {category} | {len(tags_counter)} | {sum(tags_counter.values())} |\n")
        f.write(f"\n")
        
        # 7. ç»“è®ºä¸å»ºè®®
        f.write(f"## ä¸ƒã€ç»“è®ºä¸å»ºè®®\n\n")
        
        # è®¡ç®—æŠ¤è‚¤ç›¸å…³åº¦
        skincare_rate1 = sum(skincare_tags1.values()) / analysis1['total_authors'] * 100
        skincare_rate2 = sum(skincare_tags2.values()) / analysis2['total_authors'] * 100
        
        f.write(f"### 7.1 å…³é”®è¯æ•ˆæœè¯„ä¼°\n\n")
        f.write(f"| å…³é”®è¯ | æŠ¤è‚¤ç›¸å…³æ ‡ç­¾å æ¯” | è¯„ä»· |\n")
        f.write(f"|--------|-----------------|------|\n")
        f.write(f"| {keyword1} | {skincare_rate1:.2f}% | {'âœ… æ¨è' if skincare_rate1 > 50 else 'âš ï¸ ä¸€èˆ¬' if skincare_rate1 > 30 else 'âŒ ä¸æ¨è'} |\n")
        f.write(f"| {keyword2} | {skincare_rate2:.2f}% | {'âœ… æ¨è' if skincare_rate2 > 50 else 'âš ï¸ ä¸€èˆ¬' if skincare_rate2 > 30 else 'âŒ ä¸æ¨è'} |\n\n")
        
        f.write(f"### 7.2 ä¼˜åŒ–å»ºè®®\n\n")
        
        if skincare_rate1 > skincare_rate2:
            f.write(f"1. **æ¨èä½¿ç”¨ '{keyword1}'** è¿›è¡Œæœç´¢ï¼ŒæŠ¤è‚¤ç›¸å…³åº¦æ›´é«˜ï¼ˆ{skincare_rate1:.2f}%ï¼‰\n")
        else:
            f.write(f"1. **æ¨èä½¿ç”¨ '{keyword2}'** è¿›è¡Œæœç´¢ï¼ŒæŠ¤è‚¤ç›¸å…³åº¦æ›´é«˜ï¼ˆ{skincare_rate2:.2f}%ï¼‰\n")
        
        f.write(f"2. ä¸¤ä¸ªå…³é”®è¯çš„é‡åˆç‡ä¸º {overlap_result['overlap_rate_1']:.2f}%ï¼Œ")
        if overlap_result['overlap_rate_1'] < 50:
            f.write(f"å»ºè®®**åŒæ—¶ä½¿ç”¨ä¸¤ä¸ªå…³é”®è¯**ä»¥è·å¾—æ›´å…¨é¢çš„è¾¾äººè¦†ç›–\n")
        else:
            f.write(f"é‡åˆåº¦è¾ƒé«˜ï¼Œé€‰æ‹©å…¶ä¸­ä¸€ä¸ªå³å¯\n")
        
        f.write(f"3. é‡ç‚¹å…³æ³¨ä»¥ä¸‹æ ‡ç­¾çš„è¾¾äººï¼š\n")
        # æ‰¾å‡ºæœ€ç›¸å…³çš„æ ‡ç­¾
        all_skincare_tags = set(skincare_tags1.keys()) | set(skincare_tags2.keys())
        for tag in sorted(all_skincare_tags, 
                         key=lambda t: skincare_tags1.get(t, 0) + skincare_tags2.get(t, 0), 
                         reverse=True)[:5]:
            count1 = skincare_tags1.get(tag, 0)
            count2 = skincare_tags2.get(tag, 0)
            f.write(f"   - **{tag}**: {keyword1}({count1}äºº) + {keyword2}({count2}äºº)\n")
        
        f.write(f"\n")
        f.write(f"---\n\n")
        f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    print(f"âœ… å¯¹æ¯”åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")


def main():
    """
    ä¸»å‡½æ•°
    """
    print("\n" + "="*80)
    print("æŠ–éŸ³æ˜Ÿå›¾æŠ¤è‚¤è¾¾äººå…³é”®è¯å¯¹æ¯”æœç´¢å·¥å…·")
    print("="*80 + "\n")
    
    try:
        # 1. åŠ è½½ API Key
        api_key = load_api_key()
        print(f"âœ… API Key åŠ è½½æˆåŠŸ\n")
        
        # 2. è®¾ç½®è¾“å‡ºç›®å½•
        script_dir = Path(__file__).parent
        base_output_dir = script_dir.parent / 'output'
        
        # åˆ›å»ºä¸¤ä¸ªå…³é”®è¯çš„è¾“å‡ºç›®å½•
        keyword1 = "ç¾å¦†æŠ¤è‚¤"
        keyword2 = "æŠ¤è‚¤ä¿å…»"
        
        output_dir1 = base_output_dir / f'keyword_{keyword1}'
        output_dir2 = base_output_dir / f'keyword_{keyword2}'
        
        output_dir1.mkdir(parents=True, exist_ok=True)
        output_dir2.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ è¾“å‡ºç›®å½•:")
        print(f"   å…³é”®è¯1: {output_dir1}")
        print(f"   å…³é”®è¯2: {output_dir2}\n")
        
        # 3. è·å–ä¸¤ä¸ªå…³é”®è¯çš„æ•°æ®
        print(f"â³ å¼€å§‹è·å–æ•°æ®...\n")
        
        authors1 = fetch_keyword_data(keyword1, pages=10, api_key=api_key, output_dir=output_dir1)
        authors2 = fetch_keyword_data(keyword2, pages=10, api_key=api_key, output_dir=output_dir2)
        
        # 4. åˆ†ææ ‡ç­¾
        print(f"ğŸ“Š åˆ†ææ ‡ç­¾åˆ†å¸ƒ...\n")
        analysis1 = analyze_authors_tags(authors1, keyword1)
        analysis2 = analyze_authors_tags(authors2, keyword2)
        
        # 5. è®¡ç®—é‡åˆåº¦
        print(f"ğŸ” è®¡ç®—é‡åˆåº¦...\n")
        overlap_result = calculate_overlap(authors1, authors2)
        
        print(f"ä¸¤ä¸ªå…³é”®è¯çš„é‡åˆåº¦:")
        print(f"  - {keyword1}: {len(authors1)} äºº")
        print(f"  - {keyword2}: {len(authors2)} äºº")
        print(f"  - é‡åˆ: {overlap_result['overlap_count']} äºº ({overlap_result['overlap_rate_1']:.2f}%)\n")
        
        # 6. ä¸åŸå§‹æ•°æ®å¯¹æ¯”
        print(f"ğŸ“ˆ ä¸åŸå§‹'æŠ¤è‚¤'æœç´¢ç»“æœå¯¹æ¯”...\n")
        original_dir = base_output_dir
        
        original_comparison1 = compare_with_original(authors1, original_dir)
        original_comparison2 = compare_with_original(authors2, original_dir)
        
        if original_comparison1:
            print(f"'{keyword1}' ä¸åŸå§‹æ•°æ®é‡åˆ: {original_comparison1['overlap']['overlap_count']} äºº")
        if original_comparison2:
            print(f"'{keyword2}' ä¸åŸå§‹æ•°æ®é‡åˆ: {original_comparison2['overlap']['overlap_count']} äºº\n")
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        print(f"ğŸ“ ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š...\n")
        generate_analysis_report(
            keyword1, authors1, analysis1,
            keyword2, authors2, analysis2,
            overlap_result,
            original_comparison1, original_comparison2,
            base_output_dir
        )
        
        print(f"\n{'='*80}")
        print(f"âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        print(f"{'='*80}\n")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

