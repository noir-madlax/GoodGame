#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æœç´¢æŠ¤è‚¤è¾¾äººè„šæœ¬

åŠŸèƒ½ï¼š
1. è°ƒç”¨ TikHub API çš„ fetch_user_search_v4 æ¥å£æœç´¢"æŠ¤è‚¤è¾¾äºº"
2. è·å– 3 é¡µæœç´¢ç»“æœæ•°æ®
3. åˆ†æè…°éƒ¨è¾¾äººï¼ˆç²‰ä¸æ•° 10ä¸‡~100ä¸‡ï¼‰çš„æ•°é‡å’Œåˆ†å¸ƒ
4. å°†ç»“æœä¿å­˜åˆ° output ç›®å½•

æ¥å£æ–‡æ¡£: https://api.tikhub.io/#/Douyin-Search-API/fetch_user_search_v4_api_v1_douyin_search_fetch_user_search_v4_post
"""

import os
import json
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from collections import Counter


def load_api_key():
    """
    ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key
    
    Returns:
        str: API Key
    
    Raises:
        ValueError: å¦‚æœ API Key æœªè®¾ç½®
    """
    # å®šä½åˆ° backend/.env æ–‡ä»¶
    # ä» backend/test/kol/searchkol/code/ éœ€è¦ä¸Š 4 çº§åˆ° backend/
    backend_dir = Path(__file__).parent.parent.parent.parent.parent  # è¿”å›åˆ° backend ç›®å½•
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨ {env_path} æ–‡ä»¶ä¸­é…ç½®")
    
    return api_key


def fetch_user_search_v4(api_key: str, keyword: str, cursor: int = 0, offset: int = 0, 
                        page: int = 0, search_id: str = "", count: int = 10) -> dict:
    """
    è°ƒç”¨ TikHub API çš„ fetch_user_search_v4 æ¥å£æœç´¢æŠ–éŸ³ç”¨æˆ·
    
    Args:
        api_key: TikHub API å¯†é’¥
        keyword: æœç´¢å…³é”®è¯
        cursor: æ¸¸æ ‡ï¼Œç”¨äºç¿»é¡µï¼ˆé¦–æ¬¡è¯·æ±‚ä¼  0ï¼‰
        offset: åç§»é‡ï¼Œç”¨äºç¿»é¡µï¼ˆé¦–æ¬¡è¯·æ±‚ä¼  0ï¼‰
        page: é¡µç ï¼Œç”¨äºç¿»é¡µï¼ˆé¦–æ¬¡è¯·æ±‚ä¼  0ï¼Œä¹‹åæ¯æ¬¡åŠ  1ï¼‰
        search_id: æœç´¢IDï¼Œç”¨äºç¿»é¡µï¼ˆé¦–æ¬¡è¯·æ±‚ä¼ ç©ºå­—ç¬¦ä¸²ï¼Œç¿»é¡µæ—¶ä½¿ç”¨ä¸Šæ¬¡å“åº”çš„ search_idï¼‰
        count: æ¯é¡µè¿”å›çš„ç”¨æˆ·æ•°é‡ï¼Œé»˜è®¤ 10
        
    Returns:
        dict: API å“åº”çš„ JSON æ•°æ®
    """
    # API ç«¯ç‚¹
    url = "https://api.tikhub.io/api/v1/douyin/search/fetch_user_search_v4"
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    # è®¾ç½®è¯·æ±‚ä½“ï¼ˆæŒ‰ç…§ API æ–‡æ¡£è¦æ±‚ï¼‰
    payload = {
        'keyword': keyword,          # æœç´¢å…³é”®è¯
        'cursor': cursor,            # æ¸¸æ ‡
        'offset': offset,            # åç§»é‡
        'page': page,                # é¡µç 
        'search_id': search_id,      # æœç´¢ID
        'count': count,              # æ¯é¡µæ•°é‡
        'search_channel': 'aweme_user_web',  # æœç´¢æ¸ é“
        'sort_type': 0,             # æ’åºç±»å‹ï¼š0=ç»¼åˆæ’åºï¼Œ1=ç²‰ä¸æ•°æ’åº
        'publish_time': 0           # å‘å¸ƒæ—¶é—´ç­›é€‰ï¼š0=ä¸é™
    }
    
    print(f"\nğŸ“¡ å‘é€æœç´¢è¯·æ±‚...")
    print(f"   å…³é”®è¯: {keyword}")
    print(f"   æ¸¸æ ‡ cursor: {cursor}")
    print(f"   åç§»é‡ offset: {offset}")
    print(f"   é¡µç  page: {page}")
    print(f"   æœç´¢ID search_id: {search_id if search_id else '(ç©º)'}")
    print(f"   æ•°é‡ count: {count}")
    
    try:
        # å‘é€ POST è¯·æ±‚
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        
        print(f"   çŠ¶æ€ç : {response.status_code}")
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code == 200:
            result = response.json()
            
            # æ‰“å°å“åº”ç»“æ„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            print(f"   å“åº”ç»“æ„: {list(result.keys())}")
            
            # æ£€æŸ¥å“åº”ä»£ç ï¼ˆTikHub API è¿”å› 200 è¡¨ç¤ºæˆåŠŸï¼‰
            code = result.get('code', -1)
            if code == 200:
                data = result.get('data', {})
                
                # æ•°æ®ç»“æ„æ˜¯åµŒå¥—çš„ï¼šdata.data åŒ…å«ç”¨æˆ·åˆ—è¡¨
                inner_data = data.get('data', [])
                config = data.get('config', {})
                
                # ç”¨æˆ·åˆ—è¡¨ç›´æ¥åœ¨ data.data ä¸­
                user_list = inner_data if isinstance(inner_data, list) else []
                
                # has_more åœ¨ config ä¸­
                has_more = config.get('has_more', 0) == 1
                
                print(f"   âœ… æˆåŠŸè·å– {len(user_list)} ä¸ªç”¨æˆ·")
                print(f"   è¿˜æœ‰æ›´å¤šæ•°æ®: {has_more}")
                
                # åœ¨ç»“æœä¸­æ·»åŠ è¯·æ±‚ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•
                result['_request_payload'] = payload
                return result
            else:
                print(f"   âŒ API è¿”å›é”™è¯¯ç : {code}")
                print(f"   å®Œæ•´å“åº”: {json.dumps(result, ensure_ascii=False, indent=2)[:500]}")
                return result
        else:
            print(f"   âŒ HTTP è¯·æ±‚å¤±è´¥")
            print(f"   é”™è¯¯ä¿¡æ¯: {response.text[:200]}")
            return {
                "error": f"HTTP {response.status_code}",
                "message": response.text
            }
            
    except requests.exceptions.Timeout:
        print(f"   âŒ è¯·æ±‚è¶…æ—¶")
        return {"error": "timeout"}
    except requests.exceptions.RequestException as e:
        print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
        return {"error": str(e)}
    except Exception as e:
        print(f"   âŒ æœªçŸ¥é”™è¯¯: {str(e)}")
        return {"error": str(e)}


def fetch_multiple_pages(api_key: str, keyword: str, page_count: int = 3, count_per_page: int = 10, 
                        output_dir: str = None) -> tuple:
    """
    è·å–å¤šé¡µæœç´¢ç»“æœï¼ˆæ”¯æŒå»é‡ï¼‰
    
    Args:
        api_key: API å¯†é’¥
        keyword: æœç´¢å…³é”®è¯
        page_count: è¦è·å–çš„é¡µæ•°
        count_per_page: æ¯é¡µæ•°é‡
        output_dir: è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¿å­˜è¯¦ç»†çš„è¯·æ±‚/å“åº”ï¼‰
        
    Returns:
        tuple: (ç”¨æˆ·æ•°æ®åˆ—è¡¨, æ¯é¡µè¯¦æƒ…åˆ—è¡¨)
    """
    all_users = []
    seen_uids = set()  # ç”¨äºå»é‡çš„ UID é›†åˆ
    page_details = []  # ä¿å­˜æ¯é¡µçš„è¯¦ç»†ä¿¡æ¯
    
    # åˆå§‹åŒ–ç¿»é¡µå‚æ•°ï¼ˆé¦–æ¬¡è¯·æ±‚ï¼‰
    cursor = 0
    offset = 0
    page = 0
    search_id = ""
    
    print(f"\n{'='*60}")
    print(f"ğŸ” å¼€å§‹æœç´¢: {keyword}")
    print(f"   ç›®æ ‡é¡µæ•°: {page_count}")
    print(f"   æ¯é¡µæ•°é‡: {count_per_page}")
    print(f"{'='*60}")
    
    for page_num in range(1, page_count + 1):
        print(f"\n[ç¬¬ {page_num}/{page_count} é¡µ]")
        print(f"   å½“å‰ç¿»é¡µå‚æ•°: cursor={cursor}, offset={offset}, page={page}, search_id={search_id if search_id else '(ç©º)'}")
        print("-" * 60)
        
        # è°ƒç”¨ API
        result = fetch_user_search_v4(api_key, keyword, cursor, offset, page, search_id, count_per_page)
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸï¼ˆTikHub API è¿”å› 200 è¡¨ç¤ºæˆåŠŸï¼‰
        if 'error' in result or result.get('code') != 200:
            print(f"âš ï¸ ç¬¬ {page_num} é¡µè·å–å¤±è´¥ï¼Œåœæ­¢æœç´¢")
            break
        
        # æå–ç”¨æˆ·åˆ—è¡¨ï¼ˆæ•°æ®ç»“æ„æ˜¯åµŒå¥—çš„ï¼‰
        data = result.get('data', {})
        inner_data = data.get('data', [])
        config = data.get('config', {})
        
        # ç”¨æˆ·åˆ—è¡¨åœ¨ data.data ä¸­
        user_list = inner_data if isinstance(inner_data, list) else []
        
        # ä¿å­˜æœ¬é¡µè¯¦æƒ…ï¼ˆç”¨äºè¾“å‡ºï¼‰
        page_detail = {
            'page_num': page_num,
            'request': result.get('_request_payload', {}),
            'response_code': result.get('code'),
            'user_count': len(user_list),
            'config': config
        }
        page_details.append(page_detail)
        
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼Œä¿å­˜è¯¦ç»†çš„è¯·æ±‚/å“åº”
        if output_dir:
            detail_file = os.path.join(output_dir, 'detail', f'page_{page_num}_request_response.json')
            os.makedirs(os.path.dirname(detail_file), exist_ok=True)
            with open(detail_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'page_num': page_num,
                    'request_payload': result.get('_request_payload', {}),
                    'response': result
                }, f, ensure_ascii=False, indent=2)
            print(f"   ğŸ’¾ å·²ä¿å­˜è¯¦æƒ…åˆ°: {detail_file}")
        
        if not user_list:
            print(f"âš ï¸ ç¬¬ {page_num} é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢æœç´¢")
            break
        
        # å»é‡å¹¶æ·»åŠ åˆ°æ€»åˆ—è¡¨
        new_users = 0
        duplicate_users = 0
        for user in user_list:
            user_info = user.get('user_info', {})
            uid = user_info.get('uid', '')
            
            if uid and uid not in seen_uids:
                seen_uids.add(uid)
                all_users.append(user)
                new_users += 1
            else:
                duplicate_users += 1
        
        print(f"\næœ¬é¡µç»Ÿè®¡:")
        print(f"   åŸå§‹ç”¨æˆ·æ•°: {len(user_list)}")
        print(f"   æ–°å¢ç”¨æˆ·æ•°: {new_users}")
        print(f"   é‡å¤ç”¨æˆ·æ•°: {duplicate_users}")
        
        # æ˜¾ç¤ºæœ¬é¡µç”¨æˆ·ä¿¡æ¯ï¼ˆæ˜¾ç¤ºå‰3ä¸ªï¼‰
        if len(user_list) > 0:
            print(f"\næœ¬é¡µç”¨æˆ·é¢„è§ˆï¼ˆå‰3ä¸ªï¼‰:")
            for i, user in enumerate(user_list[:3], 1):
                user_info = user.get('user_info', {})
                nickname = user_info.get('nickname', 'N/A')
                follower_count = user_info.get('follower_count', 0)
                aweme_count = user_info.get('aweme_count', 0)
                print(f"   {i}. {nickname} - ç²‰ä¸: {follower_count:,} - ä½œå“: {aweme_count}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®ï¼ˆåœ¨ config ä¸­ï¼‰
        has_more = config.get('has_more', 0) == 1
        next_page_info = config.get('next_page', {})
        
        print(f"\nä¸‹ä¸€é¡µå‚æ•°:")
        print(f"   è¿˜æœ‰æ›´å¤š: {has_more}")
        print(f"   next_pageä¿¡æ¯: {json.dumps(next_page_info, ensure_ascii=False)}")
        
        if not has_more:
            print(f"\nâœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼ˆå…± {page_num} é¡µï¼‰")
            break
        
        # æ›´æ–°ç¿»é¡µå‚æ•°ï¼ˆä» next_page ä¸­è·å–ï¼‰
        # æ ¹æ® API æ–‡æ¡£ï¼šç¿»é¡µæ—¶ä»ä¸Šä¸€æ¬¡å“åº”ä¸­è·å– cursorã€offset å’Œ search_idï¼Œpage æ¯æ¬¡åŠ  1
        if next_page_info:
            cursor = next_page_info.get('cursor', cursor)
            offset = next_page_info.get('offset', offset)  # å°è¯•è·å– offset
            search_id = next_page_info.get('search_id', search_id)
            
            # ä» search_request_id è·å– search_idï¼ˆå¤‡é€‰ï¼‰
            if not search_id and 'search_request_id' in next_page_info:
                search_id = next_page_info.get('search_request_id', '')
        
        # page æ¯æ¬¡åŠ  1
        page += 1
        
        print(f"\næ›´æ–°åçš„ç¿»é¡µå‚æ•°:")
        print(f"   cursor: {cursor}")
        print(f"   offset: {offset}")
        print(f"   page: {page}")
        print(f"   search_id: {search_id if search_id else '(ç©º)'}")
        
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        if page_num < page_count:
            print(f"\nâ³ ç­‰å¾… 1 ç§’åç»§ç»­...")
            time.sleep(1)
    
    print(f"\n{'='*60}")
    print(f"âœ… æœç´¢å®Œæˆï¼")
    print(f"   å»é‡åç”¨æˆ·æ•°: {len(all_users)}")
    print(f"   å”¯ä¸€ UID æ•°: {len(seen_uids)}")
    print(f"{'='*60}")
    
    return all_users, page_details


def analyze_kol_distribution(users: list) -> dict:
    """
    åˆ†æè¾¾äººçš„ç²‰ä¸æ•°åˆ†å¸ƒ
    
    å®šä¹‰ï¼š
    - å¤´éƒ¨è¾¾äºº: ç²‰ä¸æ•° > 100ä¸‡
    - è…°éƒ¨è¾¾äºº: ç²‰ä¸æ•° 10ä¸‡ ~ 100ä¸‡
    - å°¾éƒ¨è¾¾äºº: ç²‰ä¸æ•° 1ä¸‡ ~ 10ä¸‡
    - ç´ äºº: ç²‰ä¸æ•° < 1ä¸‡
    
    Args:
        users: ç”¨æˆ·åˆ—è¡¨
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š å¼€å§‹åˆ†æè¾¾äººåˆ†å¸ƒ")
    print(f"{'='*60}")
    
    # åˆ†ç±»ç»Ÿè®¡
    categories = {
        'å¤´éƒ¨è¾¾äºº (>100ä¸‡)': [],
        'è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)': [],
        'å°¾éƒ¨è¾¾äºº (1ä¸‡~10ä¸‡)': [],
        'ç´ äºº (<1ä¸‡)': []
    }
    
    # ç²‰ä¸æ•°åŒºé—´ç»Ÿè®¡ï¼ˆæ›´ç»†è‡´çš„åŒºé—´ï¼‰
    follower_ranges = {
        '1ä¸‡ä»¥ä¸‹': 0,
        '1-5ä¸‡': 0,
        '5-10ä¸‡': 0,
        '10-20ä¸‡': 0,
        '20-50ä¸‡': 0,
        '50-100ä¸‡': 0,
        '100-200ä¸‡': 0,
        '200-500ä¸‡': 0,
        '500ä¸‡ä»¥ä¸Š': 0
    }
    
    # éå†ç”¨æˆ·è¿›è¡Œåˆ†ç±»
    for user in users:
        # ç”¨æˆ·ä¿¡æ¯åœ¨ user_info ä¸­
        user_info = user.get('user_info', {})
        follower_count = user_info.get('follower_count', 0)
        nickname = user_info.get('nickname', 'N/A')
        
        # åˆ†ç±»
        if follower_count > 1_000_000:
            categories['å¤´éƒ¨è¾¾äºº (>100ä¸‡)'].append(user)
        elif follower_count >= 100_000:
            categories['è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)'].append(user)
        elif follower_count >= 10_000:
            categories['å°¾éƒ¨è¾¾äºº (1ä¸‡~10ä¸‡)'].append(user)
        else:
            categories['ç´ äºº (<1ä¸‡)'].append(user)
        
        # ç»†åˆ†åŒºé—´ç»Ÿè®¡
        if follower_count < 10_000:
            follower_ranges['1ä¸‡ä»¥ä¸‹'] += 1
        elif follower_count < 50_000:
            follower_ranges['1-5ä¸‡'] += 1
        elif follower_count < 100_000:
            follower_ranges['5-10ä¸‡'] += 1
        elif follower_count < 200_000:
            follower_ranges['10-20ä¸‡'] += 1
        elif follower_count < 500_000:
            follower_ranges['20-50ä¸‡'] += 1
        elif follower_count < 1_000_000:
            follower_ranges['50-100ä¸‡'] += 1
        elif follower_count < 2_000_000:
            follower_ranges['100-200ä¸‡'] += 1
        elif follower_count < 5_000_000:
            follower_ranges['200-500ä¸‡'] += 1
        else:
            follower_ranges['500ä¸‡ä»¥ä¸Š'] += 1
    
    # æ‰“å°åˆ†æç»“æœ
    print(f"\næ€»ç”¨æˆ·æ•°: {len(users)}")
    print(f"\nè¾¾äººåˆ†ç±»ç»Ÿè®¡:")
    print("-" * 60)
    
    for category, user_list in categories.items():
        count = len(user_list)
        percentage = (count / len(users) * 100) if users else 0
        print(f"  {category}: {count} äºº ({percentage:.1f}%)")
    
    print(f"\nç²‰ä¸æ•°åŒºé—´åˆ†å¸ƒ:")
    print("-" * 60)
    
    for range_name, count in follower_ranges.items():
        percentage = (count / len(users) * 100) if users else 0
        bar = 'â–ˆ' * int(percentage / 2)  # å¯è§†åŒ–æ¡å½¢å›¾
        print(f"  {range_name:12s}: {count:3d} äºº ({percentage:5.1f}%) {bar}")
    
    # è…°éƒ¨è¾¾äººè¯¦ç»†åˆ†æ
    print(f"\n{'='*60}")
    print(f"ğŸ¯ è…°éƒ¨è¾¾äººè¯¦ç»†åˆ†æ (ç²‰ä¸æ•° 10ä¸‡~100ä¸‡)")
    print(f"{'='*60}")
    
    waist_kols = categories['è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)']
    print(f"è…°éƒ¨è¾¾äººæ€»æ•°: {len(waist_kols)}")
    
    if waist_kols:
        # æŒ‰ç²‰ä¸æ•°æ’åº
        waist_kols_sorted = sorted(waist_kols, key=lambda x: x.get('user_info', {}).get('follower_count', 0), reverse=True)
        
        # ç»Ÿè®¡
        follower_counts = [user.get('user_info', {}).get('follower_count', 0) for user in waist_kols]
        avg_followers = sum(follower_counts) / len(follower_counts)
        max_followers = max(follower_counts)
        min_followers = min(follower_counts)
        
        print(f"\nç²‰ä¸æ•°ç»Ÿè®¡:")
        print(f"  å¹³å‡ç²‰ä¸æ•°: {avg_followers:,.0f}")
        print(f"  æœ€é«˜ç²‰ä¸æ•°: {max_followers:,}")
        print(f"  æœ€ä½ç²‰ä¸æ•°: {min_followers:,}")
        
        print(f"\nè…°éƒ¨è¾¾äºº TOP 10:")
        print("-" * 60)
        
        for i, user in enumerate(waist_kols_sorted[:10], 1):
            user_info = user.get('user_info', {})
            nickname = user_info.get('nickname', 'N/A')
            follower_count = user_info.get('follower_count', 0)
            total_favorited = user_info.get('total_favorited', 0)
            aweme_count = user_info.get('aweme_count', 0)
            signature = user_info.get('signature', '')[:30]  # é™åˆ¶é•¿åº¦
            
            print(f"  {i:2d}. {nickname}")
            print(f"      ç²‰ä¸: {follower_count:,} | è·èµ: {total_favorited:,} | ä½œå“: {aweme_count}")
            if signature:
                print(f"      ç®€ä»‹: {signature}...")
    
    # æ„å»ºè¿”å›ç»“æœ
    analysis_result = {
        'summary': {
            'total_users': len(users),
            'head_kols': len(categories['å¤´éƒ¨è¾¾äºº (>100ä¸‡)']),
            'waist_kols': len(categories['è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)']),
            'tail_kols': len(categories['å°¾éƒ¨è¾¾äºº (1ä¸‡~10ä¸‡)']),
            'normal_users': len(categories['ç´ äºº (<1ä¸‡)'])
        },
        'follower_ranges': follower_ranges,
        'waist_kol_details': {
            'count': len(waist_kols),
            'avg_followers': avg_followers if waist_kols else 0,
            'max_followers': max_followers if waist_kols else 0,
            'min_followers': min_followers if waist_kols else 0,
            'top_10': waist_kols_sorted[:10] if waist_kols else []
        },
        'categories': {
            category: [
                {
                    'nickname': user.get('user_info', {}).get('nickname'),
                    'follower_count': user.get('user_info', {}).get('follower_count'),
                    'total_favorited': user.get('user_info', {}).get('total_favorited'),
                    'aweme_count': user.get('user_info', {}).get('aweme_count'),
                    'uid': user.get('user_info', {}).get('uid'),
                    'unique_id': user.get('user_info', {}).get('unique_id'),
                    'signature': user.get('user_info', {}).get('signature', '')
                }
                for user in user_list
            ]
            for category, user_list in categories.items()
        }
    }
    
    return analysis_result


def save_results(all_users: list, analysis: dict, page_details: list, output_dir: str, keyword: str):
    """
    ä¿å­˜æœç´¢ç»“æœå’Œåˆ†æç»“æœåˆ°æ–‡ä»¶
    
    Args:
        all_users: æ‰€æœ‰ç”¨æˆ·æ•°æ®
        analysis: åˆ†æç»“æœ
        page_details: æ¯é¡µè¯¦æƒ…åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        keyword: æœç´¢å…³é”®è¯
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # 1. ä¿å­˜åŸå§‹æœç´¢ç»“æœï¼ˆ3é¡µå®Œæ•´æ•°æ®ï¼‰
    raw_data_file = os.path.join(output_dir, f'search_results_3pages_{timestamp}.json')
    with open(raw_data_file, 'w', encoding='utf-8') as f:
        json.dump({
            'search_metadata': {
                'keyword': keyword,
                'search_date': datetime.now().isoformat(),
                'total_users': len(all_users),
                'api_interface': 'fetch_user_search_v4'
            },
            'users': all_users
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ åŸå§‹æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {raw_data_file}")
    
    # 2. ä¿å­˜åˆ†æç»“æœ
    analysis_file = os.path.join(output_dir, f'waist_kol_analysis_{timestamp}.json')
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump({
            'analysis_metadata': {
                'keyword': keyword,
                'analysis_date': datetime.now().isoformat(),
                'total_users': len(all_users)
            },
            'analysis': analysis
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_file}")
    
    # 3. ä¿å­˜è…°éƒ¨è¾¾äººå•ç‹¬åˆ—è¡¨ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
    waist_kols = analysis['categories']['è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡)']
    waist_kol_file = os.path.join(output_dir, f'waist_kols_only_{timestamp}.json')
    with open(waist_kol_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'keyword': keyword,
                'date': datetime.now().isoformat(),
                'count': len(waist_kols)
            },
            'waist_kols': waist_kols
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ è…°éƒ¨è¾¾äººåˆ—è¡¨å·²ä¿å­˜åˆ°: {waist_kol_file}")
    
    # 4. ä¿å­˜åˆ†é¡µè¯¦æƒ…æ‘˜è¦
    page_summary_file = os.path.join(output_dir, f'page_summary_{timestamp}.json')
    with open(page_summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'keyword': keyword,
                'date': datetime.now().isoformat(),
                'total_pages': len(page_details)
            },
            'page_details': page_details
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ åˆ†é¡µæ‘˜è¦å·²ä¿å­˜åˆ°: {page_summary_file}")
    
    # 5. ç”Ÿæˆç®€æŠ¥æ–‡ä»¶ï¼ˆMarkdown æ ¼å¼ï¼‰
    report_file = os.path.join(output_dir, f'analysis_report_{timestamp}.md')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"# æŠ¤è‚¤è¾¾äººæœç´¢åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**æœç´¢å…³é”®è¯**: {keyword}\n")
        f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ•°æ®æ¥æº**: æŠ–éŸ³ç”¨æˆ·æœç´¢ API (fetch_user_search_v4)\n\n")
        
        f.write(f"## æ€»ä½“ç»Ÿè®¡\n\n")
        f.write(f"- æ€»ç”¨æˆ·æ•°: {analysis['summary']['total_users']}\n")
        f.write(f"- å¤´éƒ¨è¾¾äºº (>100ä¸‡): {analysis['summary']['head_kols']} äºº\n")
        f.write(f"- **è…°éƒ¨è¾¾äºº (10ä¸‡~100ä¸‡): {analysis['summary']['waist_kols']} äºº**\n")
        f.write(f"- å°¾éƒ¨è¾¾äºº (1ä¸‡~10ä¸‡): {analysis['summary']['tail_kols']} äºº\n")
        f.write(f"- ç´ äºº (<1ä¸‡): {analysis['summary']['normal_users']} äºº\n\n")
        
        f.write(f"## ç²‰ä¸æ•°åŒºé—´åˆ†å¸ƒ\n\n")
        f.write(f"| åŒºé—´ | æ•°é‡ | å æ¯” |\n")
        f.write(f"|------|------|------|\n")
        total = analysis['summary']['total_users']
        for range_name, count in analysis['follower_ranges'].items():
            percentage = (count / total * 100) if total > 0 else 0
            f.write(f"| {range_name} | {count} | {percentage:.1f}% |\n")
        
        f.write(f"\n## è…°éƒ¨è¾¾äººè¯¦ç»†ä¿¡æ¯\n\n")
        waist_details = analysis['waist_kol_details']
        f.write(f"- **æ€»æ•°**: {waist_details['count']} äºº\n")
        f.write(f"- **å¹³å‡ç²‰ä¸æ•°**: {waist_details['avg_followers']:,.0f}\n")
        f.write(f"- **ç²‰ä¸æ•°èŒƒå›´**: {waist_details['min_followers']:,} ~ {waist_details['max_followers']:,}\n\n")
        
        if waist_details['top_10']:
            f.write(f"### è…°éƒ¨è¾¾äºº TOP 10\n\n")
            f.write(f"| æ’å | æ˜µç§° | ç²‰ä¸æ•° | è·èµæ•° | ä½œå“æ•° | æŠ–éŸ³å· |\n")
            f.write(f"|------|------|--------|--------|--------|--------|\n")
            
            for i, user in enumerate(waist_details['top_10'], 1):
                user_info = user.get('user_info', {})
                nickname = user_info.get('nickname', 'N/A')
                follower_count = user_info.get('follower_count', 0)
                total_favorited = user_info.get('total_favorited', 0)
                aweme_count = user_info.get('aweme_count', 0)
                unique_id = user_info.get('unique_id', 'N/A')
                
                f.write(f"| {i} | {nickname} | {follower_count:,} | {total_favorited:,} | {aweme_count} | {unique_id} |\n")
    
    print(f"ğŸ’¾ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    print(f"\n{'='*60}")
    print(f"âœ… æ‰€æœ‰æ–‡ä»¶ä¿å­˜å®Œæˆï¼")
    print(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°ï¼šæœç´¢æŠ¤è‚¤è¾¾äººå¹¶åˆ†æ"""
    
    print("=" * 60)
    print("æŠ–éŸ³æŠ¤è‚¤è¾¾äººæœç´¢ä¸åˆ†æå·¥å…·")
    print("API æ¥å£: fetch_user_search_v4")
    print("=" * 60)
    
    # 1. åŠ è½½ API Key
    print("\n1ï¸âƒ£ åŠ è½½ API é…ç½®...")
    try:
        api_key = load_api_key()
        print(f"âœ… API Key å·²åŠ è½½: {api_key[:10]}...{api_key[-10:]}")
    except ValueError as e:
        print(f"âŒ {e}")
        return
    
    # 2. æœç´¢æŠ¤è‚¤è¾¾äººï¼ˆè·å– 3 é¡µæ•°æ®ï¼‰
    print("\n2ï¸âƒ£ å¼€å§‹æœç´¢...")
    keyword = "æŠ¤è‚¤"
    page_count = 3
    count_per_page = 20  # æ¯é¡µ20ä¸ªç»“æœ
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    script_dir = Path(__file__).parent.parent  # backend/test/kol/searchkol/
    output_dir = script_dir / "output"
    
    all_users, page_details = fetch_multiple_pages(api_key, keyword, page_count, count_per_page, str(output_dir))
    
    if not all_users:
        print("âŒ æœªè·å–åˆ°ä»»ä½•ç”¨æˆ·æ•°æ®")
        return
    
    # 3. åˆ†æè¾¾äººåˆ†å¸ƒ
    print("\n3ï¸âƒ£ åˆ†æè¾¾äººåˆ†å¸ƒ...")
    analysis = analyze_kol_distribution(all_users)
    
    # 4. ä¿å­˜ç»“æœ
    print("\n4ï¸âƒ£ ä¿å­˜ç»“æœ...")
    save_results(all_users, analysis, page_details, str(output_dir), keyword)
    
    print("\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"\nğŸ“Œ å…³é”®å‘ç°:")
    print(f"   æœç´¢å…³é”®è¯: {keyword}")
    print(f"   æ€»ç”¨æˆ·æ•°: {len(all_users)}")
    print(f"   è…°éƒ¨è¾¾äººæ•°: {analysis['summary']['waist_kols']} äºº")
    print(f"   è…°éƒ¨è¾¾äººå æ¯”: {(analysis['summary']['waist_kols'] / len(all_users) * 100):.1f}%")


if __name__ == "__main__":
    main()

