#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æŠ±æ•è¯„è®ºæ•´ä½“åˆ†æè„šæœ¬ - Markdown è¾“å‡ºç‰ˆæœ¬

ä½¿ç”¨ Gemini 2.5 Flash ä¸€æ¬¡æ€§åˆ†ææ‰€æœ‰å°çº¢ä¹¦æŠ±æ•ç›¸å…³å¸–å­çš„è¯„è®ºï¼Œ
ç›´æ¥è¾“å‡º Markdown æ ¼å¼çš„åˆ†ææŠ¥å‘Šã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python analyze_pillow_comments_markdown.py
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

from dotenv import load_dotenv

try:
    from google import genai
    from google.genai import types
except ImportError:
    print("âŒ è¯·å®‰è£… google-genai: pip install google-genai")
    sys.exit(1)

# é…ç½®
GEMINI_MODEL = "gemini-2.5-flash"
TEMPERATURE = 0.3  # ç¨é«˜æ¸©åº¦å…è®¸æ›´è‡ªç„¶çš„æŠ¥å‘Šå†™ä½œ


def load_env() -> str:
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    backend_dir = Path(__file__).parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
    
    api_key = os.getenv('GEMINI_API_KEY') or os.getenv('GEMINI_API_KEY_ANALYZE')
    if not api_key:
        raise ValueError("æœªæ‰¾åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡")
    
    return api_key


def load_prompt() -> str:
    """åŠ è½½ Markdown è¾“å‡ºç‰ˆæœ¬çš„åˆ†æ prompt"""
    prompt_path = Path(__file__).parent / 'pillow_comment_analysis_prompt_markdown.txt'
    if not prompt_path.exists():
        raise FileNotFoundError(f"Prompt æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}")
    
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()


def load_post_info(note_id: str, search_dir: Path) -> Optional[Dict[str, Any]]:
    """ä»æœç´¢ç»“æœåŠ è½½å¸–å­ä¿¡æ¯"""
    for page_file in search_dir.glob('page_*.json'):
        with open(page_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        items = data.get('data', {}).get('data', {}).get('items', [])
        for item in items:
            if item.get('model_type') != 'note':
                continue
            note = item.get('note', {})
            if note.get('id') == note_id:
                user = note.get('user', {})
                return {
                    'note_id': note_id,
                    'title': note.get('title', ''),
                    'content': note.get('desc', ''),
                    'author_name': user.get('nickname', ''),
                    'note_type': note.get('type', 'normal'),
                    'liked_count': note.get('liked_count', 0),
                    'comments_count': note.get('comments_count', 0),
                    'collected_count': note.get('collected_count', 0)
                }
    return None


def load_all_data() -> Dict[str, Any]:
    """åŠ è½½æ‰€æœ‰å¸–å­å’Œè¯„è®ºæ•°æ®"""
    base_dir = Path(__file__).parent.parent
    comment_dir = base_dir / 'comment' / 'output'
    search_dir = base_dir / 'search' / 'output'
    
    all_posts = []
    total_main = 0
    total_sub = 0
    
    # éå†æ‰€æœ‰ä¸»è¯„è®ºæ–‡ä»¶
    for main_file in sorted(comment_dir.glob('main_comments_*.json')):
        note_id = main_file.stem.replace('main_comments_', '')
        
        # åŠ è½½å¸–å­ä¿¡æ¯
        post_info = load_post_info(note_id, search_dir)
        if not post_info:
            continue
        
        # åŠ è½½ä¸»è¯„è®º
        with open(main_file, 'r', encoding='utf-8') as f:
            main_data = json.load(f)
        main_comments = main_data.get('comments', [])
        
        # åŠ è½½å­è¯„è®º
        sub_file = comment_dir / f'sub_comments_{note_id}.json'
        sub_comments = {}
        if sub_file.exists():
            with open(sub_file, 'r', encoding='utf-8') as f:
                sub_data = json.load(f)
            sub_comments = sub_data.get('sub_comments', {})
        
        total_main += len(main_comments)
        total_sub += sum(len(subs) for subs in sub_comments.values())
        
        all_posts.append({
            'post_info': post_info,
            'main_comments': main_comments,
            'sub_comments': sub_comments
        })
    
    return {
        'posts': all_posts,
        'stats': {
            'total_posts': len(all_posts),
            'total_main_comments': total_main,
            'total_sub_comments': total_sub,
            'total_comments': total_main + total_sub
        }
    }


def format_all_data_for_llm(data: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–æ‰€æœ‰æ•°æ®ä¾› LLM åˆ†æ"""
    lines = []
    stats = data['stats']
    
    # æ•°æ®æ¦‚è§ˆ
    lines.append("=" * 60)
    lines.append("æ•°æ®æ¦‚è§ˆ")
    lines.append("=" * 60)
    lines.append(f"å¸–å­æ€»æ•°: {stats['total_posts']}")
    lines.append(f"ä¸»è¯„è®ºæ€»æ•°: {stats['total_main_comments']}")
    lines.append(f"å­è¯„è®ºæ€»æ•°: {stats['total_sub_comments']}")
    lines.append(f"è¯„è®ºæ€»æ•°: {stats['total_comments']}")
    lines.append("")
    
    # æ¯ä¸ªå¸–å­çš„æ•°æ®
    for i, post_data in enumerate(data['posts'], 1):
        post_info = post_data['post_info']
        main_comments = post_data['main_comments']
        sub_comments = post_data['sub_comments']
        
        lines.append("=" * 60)
        lines.append(f"ã€å¸–å­ {i}/{stats['total_posts']}ã€‘")
        lines.append("=" * 60)
        lines.append(f"æ ‡é¢˜: {post_info['title']}")
        lines.append(f"å†…å®¹: {post_info['content'][:200]}{'...' if len(post_info.get('content', '')) > 200 else ''}")
        lines.append(f"ä½œè€…: {post_info['author_name']}")
        lines.append(f"ç±»å‹: {post_info['note_type']}")
        lines.append(f"äº’åŠ¨: ğŸ‘{post_info['liked_count']} ğŸ’¬{post_info['comments_count']} â­{post_info['collected_count']}")
        lines.append(f"è¯„è®ºæ•°: ä¸»è¯„è®º {len(main_comments)} æ¡ï¼Œå­è¯„è®º {sum(len(s) for s in sub_comments.values())} æ¡")
        lines.append("")
        
        # è¯„è®ºåˆ—è¡¨
        lines.append("--- è¯„è®ºåˆ—è¡¨ ---")
        for j, comment in enumerate(main_comments, 1):
            comment_id = comment.get('id', '')
            content = comment.get('content', '')
            user = comment.get('user_nickname', 'åŒ¿å')
            likes = comment.get('like_count', 0)
            sub_count = comment.get('sub_comment_count', 0)
            
            lines.append(f"[{j}] {user}: {content} (ğŸ‘{likes})")
            
            # å­è¯„è®º
            if comment_id in sub_comments:
                for sub in sub_comments[comment_id]:
                    sub_content = sub.get('content', '')
                    sub_user = sub.get('user_nickname', 'åŒ¿å')
                    sub_likes = sub.get('like_count', 0)
                    lines.append(f"    â””â”€ {sub_user}: {sub_content} (ğŸ‘{sub_likes})")
        
        lines.append("")
    
    return "\n".join(lines)


def main():
    print("=" * 60, flush=True)
    print("æŠ±æ•è¯„è®ºæ•´ä½“åˆ†æå·¥å…· - Markdown æŠ¥å‘Šç‰ˆ", flush=True)
    print(f"æ¨¡å‹: {GEMINI_MODEL}", flush=True)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
    print("=" * 60, flush=True)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...", flush=True)
    data = load_all_data()
    stats = data['stats']
    print(f"   å¸–å­: {stats['total_posts']} ä¸ª", flush=True)
    print(f"   ä¸»è¯„è®º: {stats['total_main_comments']} æ¡", flush=True)
    print(f"   å­è¯„è®º: {stats['total_sub_comments']} æ¡", flush=True)
    print(f"   æ€»è®¡: {stats['total_comments']} æ¡", flush=True)
    
    # 2. æ ¼å¼åŒ–æ•°æ®
    print("\nğŸ“ æ ¼å¼åŒ–æ•°æ®...", flush=True)
    formatted_data = format_all_data_for_llm(data)
    print(f"   æ•°æ®é•¿åº¦: {len(formatted_data):,} å­—ç¬¦", flush=True)
    print(f"   ä¼°ç®— Token: {int(len(formatted_data) * 1.5):,}", flush=True)
    
    # 3. åŠ è½½ Prompt
    print("\nğŸ“„ åŠ è½½ Markdown è¾“å‡º Prompt...", flush=True)
    prompt = load_prompt()
    print(f"   Prompt é•¿åº¦: {len(prompt):,} å­—ç¬¦", flush=True)
    
    # 4. æ„å»ºå®Œæ•´è¾“å…¥
    full_input = f"{prompt}\n\n{formatted_data}"
    print(f"\nğŸ“Š æ€»è¾“å…¥é•¿åº¦: {len(full_input):,} å­—ç¬¦", flush=True)
    print(f"   ä¼°ç®— Token: {int(len(full_input) * 1.5):,}", flush=True)
    
    # 5. åˆå§‹åŒ–å®¢æˆ·ç«¯
    print("\nğŸ”Œ åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯...", flush=True)
    api_key = load_env()
    client = genai.Client(api_key=api_key)
    
    # 6. è°ƒç”¨ APIï¼ˆä¸æŒ‡å®š response_mime_typeï¼Œè®©æ¨¡å‹ç›´æ¥è¾“å‡º Markdownï¼‰
    print(f"\nğŸ¤– è°ƒç”¨ Gemini {GEMINI_MODEL}...", flush=True)
    print("   (è¿™å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…)", flush=True)
    
    start_time = time.time()
    
    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=full_input,
            config=types.GenerateContentConfig(
                temperature=TEMPERATURE
                # ä¸æŒ‡å®š response_mime_typeï¼Œè®©æ¨¡å‹è‡ªç”±è¾“å‡º Markdown
            )
        )
        
        elapsed = time.time() - start_time
        print(f"   âœ… å“åº”å®Œæˆï¼Œè€—æ—¶: {elapsed:.1f} ç§’", flush=True)
        
        result_text = response.text
        print(f"   è¾“å‡ºé•¿åº¦: {len(result_text):,} å­—ç¬¦", flush=True)
        
        # 7. ä¿å­˜ç»“æœä¸º Markdown æ–‡ä»¶
        output_dir = Path(__file__).parent / 'output'
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = output_dir / f"pillow_analysis_report_B_{timestamp}.md"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(result_text)
        
        print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {output_file}", flush=True)
        
        # 8. æ‰“å°æŠ¥å‘Šå¼€å¤´é¢„è§ˆ
        print(f"\n{'='*60}", flush=True)
        print("ğŸ“Š æŠ¥å‘Šé¢„è§ˆï¼ˆå‰ 1000 å­—ç¬¦ï¼‰", flush=True)
        print(f"{'='*60}", flush=True)
        print(result_text[:1000], flush=True)
        if len(result_text) > 1000:
            print("\n... (æ›´å¤šå†…å®¹è¯·æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š)", flush=True)
        
        print(f"\n{'='*60}", flush=True)
        print("âœ… åˆ†æå®Œæˆï¼", flush=True)
        print(f"   å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹: {output_file}", flush=True)
        print(f"{'='*60}", flush=True)
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}", flush=True)
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
