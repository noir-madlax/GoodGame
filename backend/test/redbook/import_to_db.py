#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ•°æ®å¯¼å…¥è„šæœ¬ï¼šå°†æœç´¢ç»“æœå’Œè¯„è®ºæ•°æ®å¯¼å…¥æ•°æ®åº“

å¯¼å…¥ç›®æ ‡è¡¨ï¼š
1. gg_redbook_pillow_project - æŠ±æ•é¡¹ç›®æ±‡æ€»è¡¨
2. gg_platform_post - é€šç”¨å¸–å­è¡¨
3. gg_platform_post_comments - é€šç”¨è¯„è®ºè¡¨

ç‰¹æ€§ï¼š
- å¢é‡å¯¼å…¥ï¼šå·²å­˜åœ¨çš„è®°å½•ä¸é‡å¤æ’å…¥
- æ‰¹æ¬¡å¤„ç†ï¼šæ¯æ‰¹æ¬¡å¤„ç†åæ‰“å°è¿›åº¦
- è¿›åº¦æ˜¾ç¤ºï¼šå®æ—¶æ˜¾ç¤ºå¯¼å…¥è¿›åº¦
"""

import os
import json
import sys
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from supabase import create_client, Client

# é…ç½®
BATCH_SIZE = 100  # æ¯æ‰¹æ¬¡å¤„ç†æ•°é‡
PROJECT_ID = '7ee2f0b2-de5a-4156-b52e-344fae7f499d'  # æŠ±æ•è¯„è®ºåˆ†æé¡¹ç›®


def load_env() -> tuple:
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    backend_dir = Path(__file__).parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
    
    url = os.getenv('SUPABASE_URL')
    key = os.getenv('SUPABASE_KEY')
    
    if not url or not key:
        raise ValueError("SUPABASE_URL æˆ– SUPABASE_KEY æœªè®¾ç½®")
    
    return url, key


def get_supabase_client() -> Client:
    """è·å– Supabase å®¢æˆ·ç«¯"""
    url, key = load_env()
    return create_client(url, key)


def load_search_results(search_dir: Path) -> list:
    """åŠ è½½æœç´¢ç»“æœï¼ˆä» page_*.json æ–‡ä»¶ï¼‰"""
    all_notes = []
    seen_ids = set()
    
    for i in range(1, 10):
        page_files = list(search_dir.glob(f"page_{i}_*.json"))
        if not page_files:
            break
        
        for page_file in page_files:
            with open(page_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            items = data.get('data', {}).get('data', {}).get('items', [])
            keyword = data.get('params', {}).get('keyword', 'æŠ±æ•')
            
            for item in items:
                if item.get('model_type') != 'note':
                    continue
                
                note = item.get('note', {})
                note_id = note.get('id')
                
                if not note_id or note_id in seen_ids:
                    continue
                
                seen_ids.add(note_id)
                
                user = note.get('user', {})
                images_list = note.get('images_list', [])
                cover_url = images_list[0].get('url_size_large', '') if images_list else ''
                
                all_notes.append({
                    'note_id': note_id,
                    'title': note.get('title', ''),
                    'content': note.get('desc', ''),
                    'author_id': user.get('userid', ''),
                    'author_name': user.get('nickname', ''),
                    'note_type': note.get('type', 'normal'),
                    'cover_url': cover_url,
                    'liked_count': note.get('liked_count', 0),
                    'comments_count': note.get('comments_count', 0),
                    'collected_count': note.get('collected_count', 0),
                    'shared_count': note.get('shared_count', 0),
                    'note_timestamp': note.get('timestamp'),
                    'search_keyword': keyword,
                    'raw_data': note
                })
    
    return all_notes


def get_existing_comment_ids(supabase: Client, post_id: int) -> set:
    """è·å–å·²å­˜åœ¨çš„è¯„è®º ID"""
    existing_ids = set()
    offset = 0
    limit = 1000
    
    while True:
        result = supabase.table('gg_platform_post_comments').select('platform_comment_id').eq('post_id', post_id).range(offset, offset + limit - 1).execute()
        
        if not result.data:
            break
        
        for row in result.data:
            existing_ids.add(row['platform_comment_id'])
        
        if len(result.data) < limit:
            break
        
        offset += limit
    
    return existing_ids


def import_comments_batch(supabase: Client, post_id: int, comments: list, existing_ids: set, is_sub: bool = False) -> int:
    """æ‰¹é‡å¯¼å…¥è¯„è®º"""
    inserted = 0
    batch = []
    
    for comment in comments:
        comment_id = comment.get('id') or comment.get('platform_comment_id')
        if not comment_id or comment_id in existing_ids:
            continue
        
        # è½¬æ¢æ—¶é—´æˆ³
        published_at = None
        if comment.get('time'):
            try:
                ts = comment['time']
                if ts > 10000000000:
                    ts = ts // 1000
                published_at = datetime.fromtimestamp(ts).isoformat()
            except:
                pass
        
        insert_data = {
            'post_id': post_id,
            'platform': 'xiaohongshu',
            'platform_comment_id': comment_id,
            'author_id': comment.get('user_id'),
            'author_name': comment.get('user_nickname'),
            'content': comment.get('content', ''),
            'like_count': comment.get('like_count', 0),
            'reply_count': comment.get('sub_comment_count', 0) if not is_sub else 0,
            'published_at': published_at
        }
        
        # å­è¯„è®ºæ·»åŠ çˆ¶è¯„è®ºä¿¡æ¯
        if is_sub and comment.get('parent_comment_id'):
            insert_data['parent_platform_comment_id'] = comment.get('parent_comment_id')
        
        batch.append(insert_data)
        existing_ids.add(comment_id)
        
        # æ‰¹é‡æ’å…¥
        if len(batch) >= BATCH_SIZE:
            try:
                supabase.table('gg_platform_post_comments').insert(batch).execute()
                inserted += len(batch)
                print(f"      ğŸ“¦ æ‰¹æ¬¡æ’å…¥ {len(batch)} æ¡ï¼Œç´¯è®¡ {inserted} æ¡", flush=True)
            except Exception as e:
                print(f"      âŒ æ‰¹æ¬¡æ’å…¥å¤±è´¥: {e}", flush=True)
            batch = []
    
    # æ’å…¥å‰©ä½™çš„
    if batch:
        try:
            supabase.table('gg_platform_post_comments').insert(batch).execute()
            inserted += len(batch)
            print(f"      ğŸ“¦ æœ€åæ‰¹æ¬¡æ’å…¥ {len(batch)} æ¡ï¼Œç´¯è®¡ {inserted} æ¡", flush=True)
        except Exception as e:
            print(f"      âŒ æœ€åæ‰¹æ¬¡æ’å…¥å¤±è´¥: {e}", flush=True)
    
    return inserted


def import_first_note_comments(supabase: Client, note_id: str, post_id: int, comments_dir: Path) -> tuple:
    """å¯¼å…¥ç¬¬ä¸€ä¸ªå¸–å­çš„è¯„è®º"""
    main_file = comments_dir / f"main_comments_{note_id}.json"
    sub_file = comments_dir / f"sub_comments_{note_id}.json"
    
    if not main_file.exists():
        print(f"   âŒ ä¸»è¯„è®ºæ–‡ä»¶ä¸å­˜åœ¨: {main_file}")
        return 0, 0
    
    # è·å–å·²å­˜åœ¨çš„è¯„è®º ID
    print(f"   ğŸ“Š æ£€æŸ¥å·²å­˜åœ¨çš„è¯„è®º...", flush=True)
    existing_ids = get_existing_comment_ids(supabase, post_id)
    print(f"   ğŸ“Š å·²å­˜åœ¨ {len(existing_ids)} æ¡è¯„è®º", flush=True)
    
    # åŠ è½½ä¸»è¯„è®º
    print(f"   ğŸ“‚ åŠ è½½ä¸»è¯„è®ºæ–‡ä»¶...", flush=True)
    with open(main_file, 'r', encoding='utf-8') as f:
        main_data = json.load(f)
    
    main_comments = main_data.get('comments', [])
    print(f"   ğŸ“Š ä¸»è¯„è®ºæ–‡ä»¶ä¸­æœ‰ {len(main_comments)} æ¡", flush=True)
    
    # å¯¼å…¥ä¸»è¯„è®º
    print(f"   ğŸ“¥ å¯¼å…¥ä¸»è¯„è®º...", flush=True)
    main_count = import_comments_batch(supabase, post_id, main_comments, existing_ids, is_sub=False)
    
    # åŠ è½½å¹¶å¯¼å…¥å­è¯„è®º
    sub_count = 0
    if sub_file.exists():
        print(f"   ğŸ“‚ åŠ è½½å­è¯„è®ºæ–‡ä»¶...", flush=True)
        with open(sub_file, 'r', encoding='utf-8') as f:
            sub_data = json.load(f)
        
        sub_comments_dict = sub_data.get('sub_comments', {})
        total_subs = sum(len(subs) for subs in sub_comments_dict.values())
        print(f"   ğŸ“Š å­è¯„è®ºæ–‡ä»¶ä¸­æœ‰ {total_subs} æ¡", flush=True)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        all_subs = []
        for parent_id, subs in sub_comments_dict.items():
            for sub in subs:
                sub['parent_comment_id'] = parent_id
                all_subs.append(sub)
        
        print(f"   ğŸ“¥ å¯¼å…¥å­è¯„è®º...", flush=True)
        sub_count = import_comments_batch(supabase, post_id, all_subs, existing_ids, is_sub=True)
    
    return main_count, sub_count


def get_or_create_platform_post(supabase: Client, note_id: str, note_info: dict) -> int:
    """è·å–æˆ–åˆ›å»º platform_post è®°å½•"""
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    existing = supabase.table('gg_platform_post').select('id').eq('platform', 'xiaohongshu').eq('platform_item_id', note_id).execute()
    
    if existing.data:
        return existing.data[0]['id']
    
    # æ’å…¥æ•°æ®
    insert_data = {
        'platform': 'xiaohongshu',
        'platform_item_id': note_id,
        'title': note_info.get('title', ''),
        'content': note_info.get('content', ''),
        'post_type': 'note' if note_info.get('note_type') == 'normal' else 'video',
        'like_count': note_info.get('liked_count', 0),
        'comment_count': note_info.get('comments_count', 0),
        'share_count': note_info.get('shared_count', 0),
        'cover_url': note_info.get('cover_url'),
        'author_id': note_info.get('author_id'),
        'author_name': note_info.get('author_name'),
        'analysis_status': 'init',
        'raw_details': note_info.get('raw_data'),
        'project_id': PROJECT_ID
    }
    
    result = supabase.table('gg_platform_post').insert(insert_data).execute()
    return result.data[0]['id'] if result.data else None


def update_pillow_project_status(supabase: Client, note_id: str, main_count: int, sub_count: int):
    """æ›´æ–°æŠ±æ•é¡¹ç›®çŠ¶æ€"""
    # å…ˆè·å–å½“å‰å€¼
    current = supabase.table('gg_redbook_pillow_project').select('main_comments_fetched, sub_comments_fetched').eq('note_id', note_id).execute()
    
    if current.data:
        current_main = current.data[0].get('main_comments_fetched', 0) or 0
        current_sub = current.data[0].get('sub_comments_fetched', 0) or 0
        new_main = current_main + main_count
        new_sub = current_sub + sub_count
    else:
        new_main = main_count
        new_sub = sub_count
    
    supabase.table('gg_redbook_pillow_project').update({
        'main_comments_fetched': new_main,
        'sub_comments_fetched': new_sub,
        'fetch_status': 'completed',
        'fetch_completed_at': datetime.now().isoformat(),
        'updated_at': datetime.now().isoformat()
    }).eq('note_id', note_id).execute()


def main():
    print("=" * 60, flush=True)
    print("æ•°æ®å¯¼å…¥å·¥å…·", flush=True)
    print("=" * 60, flush=True)
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}", flush=True)
    print(f"é¡¹ç›® ID: {PROJECT_ID}", flush=True)
    
    # åˆå§‹åŒ–
    print("\nğŸ”Œ è¿æ¥æ•°æ®åº“...", flush=True)
    supabase = get_supabase_client()
    print("   âœ… è¿æ¥æˆåŠŸ", flush=True)
    
    script_dir = Path(__file__).parent
    search_dir = script_dir / "search" / "output"
    comments_dir = script_dir / "comment" / "output"
    
    # 1. åŠ è½½æœç´¢ç»“æœ
    print("\nğŸ“‚ åŠ è½½æœç´¢ç»“æœ...", flush=True)
    notes = load_search_results(search_dir)
    print(f"   å…±åŠ è½½ {len(notes)} ä¸ªå¸–å­", flush=True)
    
    # 2. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šå¸–å­
    import sys
    if len(sys.argv) > 1:
        arg = sys.argv[1]
        if arg.startswith('--id='):
            target_note_id = arg.split('=')[1]
        elif arg.startswith('--index='):
            # ä»æ•°æ®åº“è·å–å¾…å¤„ç†å¸–å­
            pending = supabase.table('gg_redbook_pillow_project').select('note_id').eq('fetch_status', 'pending').order('comments_count', desc=True).execute()
            index = int(arg.split('=')[1])
            if index >= len(pending.data):
                print(f"âŒ ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ï¼Œå…± {len(pending.data)} ä¸ªå¾…å¤„ç†å¸–å­")
                return
            target_note_id = pending.data[index]['note_id']
        else:
            target_note_id = arg
    else:
        # é»˜è®¤å¤„ç†ç¬¬ä¸€ä¸ªå¾…å¤„ç†çš„å¸–å­
        pending = supabase.table('gg_redbook_pillow_project').select('note_id').eq('fetch_status', 'pending').order('comments_count', desc=True).limit(1).execute()
        if pending.data:
            target_note_id = pending.data[0]['note_id']
        else:
            print("âœ… æ²¡æœ‰å¾…å¤„ç†çš„å¸–å­")
            return
    
    target_note = next((n for n in notes if n['note_id'] == target_note_id), None)
    
    if target_note:
        print(f"\n{'='*60}", flush=True)
        print(f"ğŸ“¥ å¯¼å…¥å¸–å­è¯„è®ºæ•°æ®", flush=True)
        print(f"{'='*60}", flush=True)
        print(f"   å¸–å­ ID: {target_note_id}", flush=True)
        print(f"   æ ‡é¢˜: {target_note['title'][:40]}...", flush=True)
        
        # è·å–æˆ–åˆ›å»º platform_post
        print(f"\n   ğŸ“ æ£€æŸ¥/åˆ›å»º platform_post...", flush=True)
        post_id = get_or_create_platform_post(supabase, target_note_id, target_note)
        print(f"   âœ… post_id: {post_id}", flush=True)
        
        if post_id:
            # å¯¼å…¥è¯„è®º
            main_count, sub_count = import_first_note_comments(supabase, target_note_id, post_id, comments_dir)
            
            print(f"\n   ğŸ“Š æœ¬æ¬¡å¯¼å…¥ç»“æœ:", flush=True)
            print(f"      ä¸»è¯„è®º: {main_count} æ¡", flush=True)
            print(f"      å­è¯„è®º: {sub_count} æ¡", flush=True)
            
            # æ›´æ–°é¡¹ç›®çŠ¶æ€ï¼ˆä½¿ç”¨å®é™…è·å–çš„æ•°é‡ï¼‰
            # ä»æ–‡ä»¶è¯»å–å®é™…æ•°é‡
            main_file = comments_dir / f"main_comments_{target_note_id}.json"
            sub_file = comments_dir / f"sub_comments_{target_note_id}.json"
            
            actual_main = 0
            actual_sub = 0
            if main_file.exists():
                with open(main_file, 'r', encoding='utf-8') as f:
                    actual_main = len(json.load(f).get('comments', []))
            if sub_file.exists():
                with open(sub_file, 'r', encoding='utf-8') as f:
                    sub_data = json.load(f).get('sub_comments', {})
                    actual_sub = sum(len(subs) for subs in sub_data.values())
            
            # æ›´æ–°çŠ¶æ€
            supabase.table('gg_redbook_pillow_project').update({
                'main_comments_fetched': actual_main,
                'sub_comments_fetched': actual_sub,
                'fetch_status': 'completed',
                'fetch_completed_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat()
            }).eq('note_id', target_note_id).execute()
            print(f"   âœ… é¡¹ç›®çŠ¶æ€å·²æ›´æ–°", flush=True)
    else:
        print(f"âŒ æœªæ‰¾åˆ°å¸–å­ {target_note_id} çš„ä¿¡æ¯")
        return
    
    # 3. æŸ¥è¯¢æœ€ç»ˆçŠ¶æ€
    print(f"\n{'='*60}", flush=True)
    print(f"ğŸ“Š æœ€ç»ˆçŠ¶æ€", flush=True)
    print(f"{'='*60}", flush=True)
    
    # æŸ¥è¯¢æŠ±æ•é¡¹ç›®è¡¨
    pillow_result = supabase.table('gg_redbook_pillow_project').select('note_id, title, comments_count, main_comments_fetched, sub_comments_fetched, fetch_status').eq('note_id', target_note_id).execute()
    
    if pillow_result.data:
        row = pillow_result.data[0]
        print(f"   å¸–å­: {row['title'][:30]}...", flush=True)
        print(f"   API è¯„è®ºæ•°: {row['comments_count']}", flush=True)
        print(f"   å·²è·å–ä¸»è¯„è®º: {row['main_comments_fetched']}", flush=True)
        print(f"   å·²è·å–å­è¯„è®º: {row['sub_comments_fetched']}", flush=True)
        print(f"   çŠ¶æ€: {row['fetch_status']}", flush=True)
    
    # æŸ¥è¯¢è¯„è®ºè¡¨
    comments_result = supabase.table('gg_platform_post_comments').select('id', count='exact').eq('post_id', post_id).execute()
    print(f"   æ•°æ®åº“ä¸­è¯„è®ºæ€»æ•°: {comments_result.count}", flush=True)
    
    print(f"\n{'='*60}", flush=True)
    print("âœ… æ•°æ®å¯¼å…¥å®Œæˆ", flush=True)
    print(f"{'='*60}", flush=True)


if __name__ == "__main__":
    main()
