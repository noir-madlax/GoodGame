#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦è¯„è®ºè·å–è„šæœ¬ V2

ä¼˜åŒ–ç‚¹:
1. å¢é‡ä¿å­˜ï¼šæ¯è·å–ä¸€é¡µè¯„è®ºå°±ä¿å­˜ï¼Œé¿å…ä¸­æ–­ä¸¢å¤±
2. æ–­ç‚¹ç»­ä¼ ï¼šå·²ä¿å­˜çš„å†…å®¹ä¸é‡å¤è·å–
3. å¹¶å‘è·å–å­è¯„è®ºï¼šä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘
4. å…ˆç»Ÿè®¡å­è¯„è®ºæ•°é‡ï¼šè®©ç”¨æˆ·å†³å®šæ˜¯å¦è·å–

æ¥å£æ–‡æ¡£:
- è·å–ç¬”è®°è¯„è®º: https://docs.tikhub.io/310965840e0
- è·å–å­è¯„è®º: https://docs.tikhub.io/310965841e0
"""

import os
import json
import requests
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, List, Optional, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import sys


# é…ç½®
MAX_CONCURRENT = 5  # å¹¶å‘æ•°
REQUEST_DELAY = 0.3  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰


def load_api_key() -> str:
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key"""
    backend_dir = Path(__file__).parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®")
    
    return api_key


def get_note_comments(
    api_key: str,
    note_id: str,
    start: str = "",
    sort_strategy: int = 1
) -> Dict[str, Any]:
    """è·å–ç¬”è®°è¯„è®º"""
    url = "https://api.tikhub.io/api/v1/xiaohongshu/app/get_note_comments"
    
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    params = {
        'note_id': note_id,
        'sort_strategy': sort_strategy
    }
    
    if start:
        params['start'] = start
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"HTTP {response.status_code}", "message": response.text[:200]}
    except Exception as e:
        return {"error": str(e)}


def get_sub_comments(
    api_key: str,
    note_id: str,
    comment_id: str,
    start: str = ""
) -> Dict[str, Any]:
    """è·å–å­è¯„è®º"""
    url = "https://api.tikhub.io/api/v1/xiaohongshu/app/get_sub_comments"
    
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    params = {
        'note_id': note_id,
        'comment_id': comment_id
    }
    
    if start:
        params['start'] = start
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"HTTP {response.status_code}", "message": response.text[:200]}
    except Exception as e:
        return {"error": str(e)}


def load_existing_comments(output_dir: Path, note_id: str) -> Dict[str, Any]:
    """åŠ è½½å·²ä¿å­˜çš„è¯„è®ºæ•°æ®"""
    comments_file = output_dir / f"comments_{note_id}.json"
    if comments_file.exists():
        with open(comments_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        'note_id': note_id,
        'comments': {},  # ä½¿ç”¨ dictï¼Œkey ä¸º comment_id
        'fetched_comment_ids': [],  # å·²è·å–çš„è¯„è®º ID åˆ—è¡¨
        'last_cursor': '',  # ä¸Šæ¬¡çš„æ¸¸æ ‡
        'sub_comments_fetched': set(),  # å·²è·å–å­è¯„è®ºçš„è¯„è®º ID
    }


def save_comments(output_dir: Path, note_id: str, data: Dict[str, Any]) -> None:
    """ä¿å­˜è¯„è®ºæ•°æ®"""
    comments_file = output_dir / f"comments_{note_id}.json"
    # è½¬æ¢ set ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–
    save_data = data.copy()
    if isinstance(save_data.get('sub_comments_fetched'), set):
        save_data['sub_comments_fetched'] = list(save_data['sub_comments_fetched'])
    
    with open(comments_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)


def fetch_comments_only(
    api_key: str,
    note_id: str,
    output_dir: Path,
    max_pages: int = 10
) -> Dict[str, Any]:
    """
    åªè·å–ä¸€çº§è¯„è®ºï¼ˆä¸è·å–å­è¯„è®ºï¼‰
    å¢é‡ä¿å­˜ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
    """
    # åŠ è½½å·²æœ‰æ•°æ®
    data = load_existing_comments(output_dir, note_id)
    
    # è½¬æ¢ sub_comments_fetched ä¸º set
    if isinstance(data.get('sub_comments_fetched'), list):
        data['sub_comments_fetched'] = set(data['sub_comments_fetched'])
    elif not isinstance(data.get('sub_comments_fetched'), set):
        data['sub_comments_fetched'] = set()
    
    existing_ids = set(data.get('fetched_comment_ids', []))
    cursor = data.get('last_cursor', '')
    
    print(f"\nğŸ“¥ è·å–ç¬”è®° {note_id} çš„è¯„è®º...", flush=True)
    print(f"   å·²æœ‰ {len(existing_ids)} æ¡è¯„è®º", flush=True)
    
    page = 0
    new_count = 0
    
    while page < max_pages:
        page += 1
        print(f"   è·å–ç¬¬ {page} é¡µ...", flush=True)
        
        result = get_note_comments(api_key, note_id, start=cursor)
        
        if result.get('code') != 200:
            print(f"   âŒ è·å–å¤±è´¥: {result.get('error', result)}", flush=True)
            break
        
        # è§£æå“åº”
        api_data = result.get('data', {})
        inner_data = api_data.get('data', {})
        comments = inner_data.get('comments', [])
        has_more = inner_data.get('has_more', False)
        cursor = inner_data.get('cursor', '')
        
        if not comments:
            print(f"   æ²¡æœ‰æ›´å¤šè¯„è®º", flush=True)
            break
        
        # å¤„ç†è¯„è®º
        page_new = 0
        for comment in comments:
            comment_id = comment.get('id')
            if comment_id and comment_id not in existing_ids:
                # æå–ç”¨æˆ·ä¿¡æ¯
                user = comment.get('user', {})
                data['comments'][comment_id] = {
                    'id': comment_id,
                    'content': comment.get('content', ''),
                    'time': comment.get('time'),
                    'user_nickname': user.get('nickname', ''),
                    'user_id': user.get('userid', ''),
                    'like_count': comment.get('like_count', 0),
                    'sub_comment_count': comment.get('sub_comment_count', 0),
                    'ip_location': comment.get('ip_location', ''),
                    'sub_comments': []
                }
                existing_ids.add(comment_id)
                data['fetched_comment_ids'].append(comment_id)
                page_new += 1
                new_count += 1
        
        # ä¿å­˜æ¸¸æ ‡
        data['last_cursor'] = cursor
        
        # ç«‹å³ä¿å­˜
        save_comments(output_dir, note_id, data)
        print(f"   âœ… æœ¬é¡µ {len(comments)} æ¡ï¼Œæ–°å¢ {page_new} æ¡ï¼Œå·²ä¿å­˜", flush=True)
        
        if not has_more:
            print(f"   å·²è·å–å…¨éƒ¨è¯„è®º", flush=True)
            break
        
        time.sleep(REQUEST_DELAY)
    
    print(f"   ğŸ“Š å…±è·å– {new_count} æ¡æ–°è¯„è®ºï¼Œæ€»è®¡ {len(existing_ids)} æ¡", flush=True)
    return data


def analyze_sub_comments(data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    åˆ†æå­è¯„è®ºæƒ…å†µï¼Œè¿”å›éœ€è¦è·å–å­è¯„è®ºçš„è¯„è®ºåˆ—è¡¨
    """
    comments_with_subs = []
    
    for comment_id, comment in data.get('comments', {}).items():
        sub_count = comment.get('sub_comment_count', 0)
        if sub_count > 0:
            # æ£€æŸ¥æ˜¯å¦å·²è·å–
            already_fetched = comment_id in data.get('sub_comments_fetched', set())
            fetched_count = len(comment.get('sub_comments', []))
            
            comments_with_subs.append({
                'comment_id': comment_id,
                'content': comment.get('content', '')[:50],
                'sub_comment_count': sub_count,
                'already_fetched': already_fetched,
                'fetched_count': fetched_count
            })
    
    # æŒ‰å­è¯„è®ºæ•°æ’åº
    comments_with_subs.sort(key=lambda x: x['sub_comment_count'], reverse=True)
    return comments_with_subs


def fetch_single_sub_comments(
    api_key: str,
    note_id: str,
    comment_id: str,
    max_pages: int = 5
) -> List[Dict[str, Any]]:
    """è·å–å•ä¸ªè¯„è®ºçš„æ‰€æœ‰å­è¯„è®º"""
    all_subs = []
    cursor = ""
    page = 0
    
    while page < max_pages:
        page += 1
        result = get_sub_comments(api_key, note_id, comment_id, start=cursor)
        
        if result.get('code') != 200:
            break
        
        sub_data = result.get('data', {})
        inner = sub_data.get('data', {})
        comments = inner.get('comments', [])
        
        if not comments:
            break
        
        for sub in comments:
            user = sub.get('user', {})
            target = sub.get('target_comment', {})
            target_user = target.get('user', {}) if target else {}
            
            all_subs.append({
                'id': sub.get('id'),
                'content': sub.get('content', ''),
                'time': sub.get('time'),
                'user_nickname': user.get('nickname', ''),
                'user_id': user.get('userid', ''),
                'reply_to_nickname': target_user.get('nickname', ''),
                'ip_location': sub.get('ip_location', '')
            })
        
        # ä½¿ç”¨æœ€åä¸€æ¡çš„ ID ä½œä¸ºæ¸¸æ ‡
        cursor = comments[-1].get('id', '')
        
        # å¦‚æœè¿”å›æ•°é‡å°‘äºé¢„æœŸï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šäº†
        if len(comments) < 10:
            break
        
        time.sleep(REQUEST_DELAY)
    
    return all_subs


def fetch_sub_comments_concurrent(
    api_key: str,
    note_id: str,
    output_dir: Path,
    data: Dict[str, Any],
    comment_ids: List[str],
    max_concurrent: int = MAX_CONCURRENT
) -> None:
    """
    å¹¶å‘è·å–å¤šä¸ªè¯„è®ºçš„å­è¯„è®º
    """
    # ç¡®ä¿ sub_comments_fetched æ˜¯ set
    if isinstance(data.get('sub_comments_fetched'), list):
        data['sub_comments_fetched'] = set(data['sub_comments_fetched'])
    elif not isinstance(data.get('sub_comments_fetched'), set):
        data['sub_comments_fetched'] = set()
    
    # è¿‡æ»¤æ‰å·²è·å–çš„
    to_fetch = [cid for cid in comment_ids if cid not in data['sub_comments_fetched']]
    
    if not to_fetch:
        print("   æ‰€æœ‰å­è¯„è®ºå·²è·å–ï¼Œæ— éœ€é‡å¤è·å–", flush=True)
        return
    
    print(f"\nğŸ“¥ å¹¶å‘è·å– {len(to_fetch)} ä¸ªè¯„è®ºçš„å­è¯„è®º (å¹¶å‘æ•°: {max_concurrent})...", flush=True)
    
    completed = 0
    
    def fetch_one(comment_id: str):
        return comment_id, fetch_single_sub_comments(api_key, note_id, comment_id)
    
    with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
        futures = {executor.submit(fetch_one, cid): cid for cid in to_fetch}
        
        for future in as_completed(futures):
            comment_id = futures[future]
            try:
                cid, subs = future.result()
                
                # æ›´æ–°æ•°æ®
                if cid in data['comments']:
                    data['comments'][cid]['sub_comments'] = subs
                    data['sub_comments_fetched'].add(cid)
                
                completed += 1
                
                # æ¯å®Œæˆä¸€ä¸ªå°±ä¿å­˜
                save_comments(output_dir, note_id, data)
                
                sub_count = data['comments'].get(cid, {}).get('sub_comment_count', 0)
                print(f"   âœ… [{completed}/{len(to_fetch)}] è¯„è®º {cid[:8]}... è·å– {len(subs)}/{sub_count} æ¡å­è¯„è®º", flush=True)
                
            except Exception as e:
                print(f"   âŒ è¯„è®º {comment_id[:8]}... è·å–å¤±è´¥: {e}", flush=True)
    
    print(f"   ğŸ“Š å®Œæˆ {completed}/{len(to_fetch)} ä¸ªè¯„è®ºçš„å­è¯„è®ºè·å–", flush=True)


def format_timestamp(ts: int) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
    if not ts:
        return "æœªçŸ¥æ—¶é—´"
    try:
        if ts > 10000000000:
            ts = ts // 1000
        return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
    except:
        return "æœªçŸ¥æ—¶é—´"


def generate_summary(
    note_info: Dict[str, Any],
    data: Dict[str, Any],
    output_path: str
) -> None:
    """ç”Ÿæˆè¯„è®ºæ±‡æ€»æ–‡ä»¶"""
    lines = []
    
    # å¸–å­ä¿¡æ¯
    lines.append("=" * 60)
    lines.append(f"ğŸ“ å°çº¢ä¹¦å¸–å­è¯„è®ºæ±‡æ€»")
    lines.append("=" * 60)
    lines.append("")
    lines.append(f"ã€å¸–å­ä¿¡æ¯ã€‘")
    lines.append(f"ID: {note_info.get('id', 'N/A')}")
    lines.append(f"æ ‡é¢˜: {note_info.get('title', 'æ— æ ‡é¢˜')}")
    lines.append(f"ä½œè€…: {note_info.get('author', 'N/A')}")
    lines.append(f"ç‚¹èµæ•°: {note_info.get('liked_count', 0)}")
    lines.append(f"è¯„è®ºæ•°: {note_info.get('comments_count', 0)}")
    lines.append(f"æ”¶è—æ•°: {note_info.get('collected_count', 0)}")
    lines.append("")
    lines.append("-" * 60)
    
    # ç»Ÿè®¡
    comments = data.get('comments', {})
    total_comments = len(comments)
    total_subs = sum(len(c.get('sub_comments', [])) for c in comments.values())
    
    lines.append("")
    lines.append(f"ã€è¯„è®ºç»Ÿè®¡ã€‘")
    lines.append(f"ä¸€çº§è¯„è®ºæ•°: {total_comments}")
    lines.append(f"å­è¯„è®ºæ•°: {total_subs}")
    lines.append(f"æ€»è®¡: {total_comments + total_subs}")
    lines.append("")
    lines.append("=" * 60)
    lines.append("ğŸ’¬ è¯„è®ºè¯¦æƒ…")
    lines.append("=" * 60)
    lines.append("")
    
    # è¯„è®ºè¯¦æƒ…
    for i, (cid, comment) in enumerate(comments.items(), 1):
        nickname = comment.get('user_nickname', 'åŒ¿åç”¨æˆ·')
        content = comment.get('content', '')
        create_time = format_timestamp(comment.get('time', 0))
        like_count = comment.get('like_count', 0)
        sub_count = comment.get('sub_comment_count', 0)
        location = comment.get('ip_location', '')
        
        lines.append(f"ã€{i}æ¥¼ã€‘{nickname} ({location})")
        lines.append(f"   {content}")
        lines.append(f"   â¤ï¸ {like_count}  ğŸ’¬ {sub_count}  ğŸ• {create_time}")
        
        # å­è¯„è®º
        for sub in comment.get('sub_comments', []):
            sub_nickname = sub.get('user_nickname', 'åŒ¿åç”¨æˆ·')
            sub_content = sub.get('content', '')
            sub_time = format_timestamp(sub.get('time', 0))
            reply_to = sub.get('reply_to_nickname', '')
            sub_location = sub.get('ip_location', '')
            
            if reply_to:
                lines.append(f"   â””â”€ {sub_nickname} å›å¤ {reply_to}: {sub_content}")
            else:
                lines.append(f"   â””â”€ {sub_nickname}: {sub_content}")
            lines.append(f"      ({sub_location}) ğŸ• {sub_time}")
        
        lines.append("")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"ğŸ’¾ è¯„è®ºæ±‡æ€»å·²ä¿å­˜: {output_path}", flush=True)


def main():
    print("=" * 60, flush=True)
    print("å°çº¢ä¹¦è¯„è®ºè·å–å·¥å…· V2", flush=True)
    print("=" * 60, flush=True)
    
    api_key = load_api_key()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    script_dir = Path(__file__).parent
    output_dir = script_dir / "output"
    output_dir.mkdir(exist_ok=True)
    
    # è¯»å– Top3 å¸–å­ä¿¡æ¯
    search_output_dir = script_dir.parent / "search" / "output"
    top3_file = search_output_dir / "top3_notes.json"
    
    if not top3_file.exists():
        print(f"âŒ æœªæ‰¾åˆ° Top3 å¸–å­æ–‡ä»¶: {top3_file}", flush=True)
        return
    
    with open(top3_file, 'r', encoding='utf-8') as f:
        top3_notes = json.load(f)
    
    print(f"\nğŸ“‹ å°†å¤„ç†ä»¥ä¸‹ {len(top3_notes)} ä¸ªå¸–å­:", flush=True)
    for i, note in enumerate(top3_notes, 1):
        print(f"   {i}. {note['title'][:40]}... (è¯„è®ºæ•°: {note['comments_count']})", flush=True)
    
    # ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰ä¸€çº§è¯„è®º
    print(f"\n{'='*60}", flush=True)
    print("ğŸ“Œ ç¬¬ä¸€æ­¥ï¼šè·å–ä¸€çº§è¯„è®º", flush=True)
    print(f"{'='*60}", flush=True)
    
    all_data = {}
    for note in top3_notes:
        note_id = note['id']
        data = fetch_comments_only(api_key, note_id, output_dir, max_pages=5)
        all_data[note_id] = data
    
    # ç¬¬äºŒæ­¥ï¼šåˆ†æå­è¯„è®ºæƒ…å†µ
    print(f"\n{'='*60}", flush=True)
    print("ğŸ“Œ ç¬¬äºŒæ­¥ï¼šå­è¯„è®ºç»Ÿè®¡", flush=True)
    print(f"{'='*60}", flush=True)
    
    for note in top3_notes:
        note_id = note['id']
        data = all_data[note_id]
        
        print(f"\nã€{note['title'][:30]}...ã€‘", flush=True)
        
        subs_analysis = analyze_sub_comments(data)
        
        if not subs_analysis:
            print("   æ²¡æœ‰å­è¯„è®º", flush=True)
            continue
        
        # ç»Ÿè®¡
        total_sub_count = sum(s['sub_comment_count'] for s in subs_analysis)
        already_fetched = sum(1 for s in subs_analysis if s['already_fetched'])
        
        print(f"   æœ‰å­è¯„è®ºçš„è¯„è®ºæ•°: {len(subs_analysis)}", flush=True)
        print(f"   å­è¯„è®ºæ€»æ•°: {total_sub_count}", flush=True)
        print(f"   å·²è·å–å­è¯„è®ºçš„è¯„è®ºæ•°: {already_fetched}", flush=True)
        
        # æ˜¾ç¤ºå‰10ä¸ª
        print(f"\n   å­è¯„è®ºæ•°é‡ Top 10:", flush=True)
        print(f"   {'è¯„è®ºID':<26} {'å­è¯„è®ºæ•°':<10} {'å·²è·å–':<10} {'å†…å®¹'}", flush=True)
        print(f"   {'-'*70}", flush=True)
        
        for s in subs_analysis[:10]:
            status = "âœ…" if s['already_fetched'] else "âŒ"
            print(f"   {s['comment_id']:<26} {s['sub_comment_count']:<10} {status:<10} {s['content'][:30]}", flush=True)
    
    # è¯¢é—®æ˜¯å¦è·å–å­è¯„è®º
    print(f"\n{'='*60}", flush=True)
    print("ğŸ’¡ å­è¯„è®ºè·å–è¯´æ˜:", flush=True)
    print("   - æ¯æ¬¡ API è°ƒç”¨éƒ½ä¼šäº§ç”Ÿè´¹ç”¨", flush=True)
    print("   - å­è¯„è®ºæ•°é‡å¤šçš„è¯„è®ºå¯èƒ½éœ€è¦å¤šæ¬¡è°ƒç”¨", flush=True)
    print("   - å·²è·å–çš„å­è¯„è®ºä¸ä¼šé‡å¤è·å–", flush=True)
    print(f"{'='*60}", flush=True)
    
    # è‡ªåŠ¨è·å–å­è¯„è®ºï¼ˆåªè·å–å­è¯„è®ºæ•° >= 5 çš„ï¼‰
    print(f"\nğŸ“Œ ç¬¬ä¸‰æ­¥ï¼šè·å–å­è¯„è®º (å­è¯„è®ºæ•° >= 5 çš„è¯„è®º)", flush=True)
    
    for note in top3_notes:
        note_id = note['id']
        data = all_data[note_id]
        
        print(f"\nã€{note['title'][:30]}...ã€‘", flush=True)
        
        subs_analysis = analyze_sub_comments(data)
        
        # åªè·å–å­è¯„è®ºæ•° >= 5 çš„
        to_fetch = [s['comment_id'] for s in subs_analysis 
                   if s['sub_comment_count'] >= 5 and not s['already_fetched']]
        
        if to_fetch:
            fetch_sub_comments_concurrent(api_key, note_id, output_dir, data, to_fetch)
        else:
            print("   æ— éœ€è·å–å­è¯„è®º", flush=True)
    
    # ç¬¬å››æ­¥ï¼šç”Ÿæˆæ±‡æ€»æ–‡ä»¶
    print(f"\n{'='*60}", flush=True)
    print("ğŸ“Œ ç¬¬å››æ­¥ï¼šç”Ÿæˆæ±‡æ€»æ–‡ä»¶", flush=True)
    print(f"{'='*60}", flush=True)
    
    for note in top3_notes:
        note_id = note['id']
        # é‡æ–°åŠ è½½æœ€æ–°æ•°æ®
        data = load_existing_comments(output_dir, note_id)
        summary_file = output_dir / f"summary_{note_id}.txt"
        generate_summary(note, data, str(summary_file))
    
    print(f"\n{'='*60}", flush=True)
    print("âœ… å®Œæˆï¼", flush=True)
    print(f"{'='*60}", flush=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}", flush=True)


if __name__ == "__main__":
    main()
