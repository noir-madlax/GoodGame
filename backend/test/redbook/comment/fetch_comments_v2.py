#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦è¯„è®ºè·å–è„šæœ¬ V2

ä¼˜åŒ–ç‚¹:
1. å¢é‡ä¿å­˜ï¼šæ¯è·å–ä¸€é¡µè¯„è®ºå°±ä¿å­˜ï¼Œé¿å…ä¸­æ–­ä¸¢å¤±
2. æ–­ç‚¹ç»­ä¼ ï¼šå·²ä¿å­˜çš„å†…å®¹ä¸é‡å¤è·å–
3. å¹¶å‘è·å–å­è¯„è®ºï¼šä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘
4. å…ˆç»Ÿè®¡å­è¯„è®ºæ•°é‡ï¼šè®©ç”¨æˆ·å†³å®šæ˜¯å¦è·å–
5. æ‰€æœ‰å‚æ•°ä»é…ç½®æ–‡ä»¶è¯»å–

æ¥å£æ–‡æ¡£:
- è·å–ç¬”è®°è¯„è®º: https://docs.tikhub.io/310965840e0
- è·å–å­è¯„è®º: https://docs.tikhub.io/310965841e0

é…ç½®æ–‡ä»¶: params/config.json
"""

import os
import json
import requests
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, List, Optional, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import sys


# ============================================================
# é…ç½®åŠ è½½
# ============================================================

def load_config() -> Dict[str, Any]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°"""
    config_path = Path(__file__).parent / "params" / "config.json"
    
    # é»˜è®¤é…ç½®
    default_config = {
        "max_concurrent": 5,
        "request_delay": 0.3,
        "max_pages": 100,
        "sub_comment_threshold": 5,
        "max_sub_pages": 10,
        "note_ids": []
    }
    
    if not config_path.exists():
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}", flush=True)
        print(f"   ä½¿ç”¨é»˜è®¤é…ç½®", flush=True)
        return default_config
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            raw_config = json.load(f)
        
        # è§£æé…ç½®
        config = {
            "max_concurrent": raw_config.get("å¹¶å‘è®¾ç½®", {}).get("max_concurrent", default_config["max_concurrent"]),
            "request_delay": raw_config.get("è¯·æ±‚é—´éš”", {}).get("request_delay", default_config["request_delay"]),
            "max_pages": raw_config.get("ä¸€çº§è¯„è®ºè·å–", {}).get("max_pages", default_config["max_pages"]),
            "sub_comment_threshold": raw_config.get("å­è¯„è®ºè·å–", {}).get("sub_comment_threshold", default_config["sub_comment_threshold"]),
            "max_sub_pages": raw_config.get("å­è¯„è®ºè·å–", {}).get("max_sub_pages", default_config["max_sub_pages"]),
            "note_ids": raw_config.get("ç›®æ ‡å¸–å­", {}).get("note_ids", default_config["note_ids"])
        }
        
        return config
    except Exception as e:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}", flush=True)
        print(f"   ä½¿ç”¨é»˜è®¤é…ç½®", flush=True)
        return default_config


# åŠ è½½é…ç½®ï¼ˆå…¨å±€ï¼‰
CONFIG = load_config()


def load_api_key() -> str:
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ TikHub API Key"""
    backend_dir = Path(__file__).parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
    
    api_key = os.getenv('tikhub_API_KEY')
    if not api_key:
        raise ValueError(f"ç¯å¢ƒå˜é‡ tikhub_API_KEY æœªè®¾ç½®")
    
    return api_key


def get_note_comments(
    api_key: str,
    note_id: str,
    start: str = "",
    sort_strategy: int = 1
) -> Dict[str, Any]:
    """è·å–ç¬”è®°è¯„è®º"""
    url = "https://api.tikhub.io/api/v1/xiaohongshu/app/get_note_comments"
    
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    params = {
        'note_id': note_id,
        'sort_strategy': sort_strategy
    }
    
    if start:
        params['start'] = start
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        if response.status_code == 200:
            return response.json()
        else:
            # è¿”å›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            try:
                error_json = response.json()
                return {"error": f"HTTP {response.status_code}", "code": response.status_code, "detail": error_json}
            except:
                return {"error": f"HTTP {response.status_code}", "code": response.status_code, "message": response.text[:500]}
    except Exception as e:
        return {"error": str(e)}


def get_sub_comments(
    api_key: str,
    note_id: str,
    comment_id: str,
    start: str = ""
) -> Dict[str, Any]:
    """è·å–å­è¯„è®º"""
    url = "https://api.tikhub.io/api/v1/xiaohongshu/app/get_sub_comments"
    
    headers = {
        'accept': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    params = {
        'note_id': note_id,
        'comment_id': comment_id
    }
    
    if start:
        params['start'] = start
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"HTTP {response.status_code}", "message": response.text[:200]}
    except Exception as e:
        return {"error": str(e)}


def load_existing_comments(output_dir: Path, note_id: str) -> Dict[str, Any]:
    """åŠ è½½å·²ä¿å­˜çš„è¯„è®ºæ•°æ®"""
    comments_file = output_dir / f"comments_{note_id}.json"
    if comments_file.exists():
        with open(comments_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        'note_id': note_id,
        'comments': {},  # ä½¿ç”¨ dictï¼Œkey ä¸º comment_id
        'fetched_comment_ids': [],  # å·²è·å–çš„è¯„è®º ID åˆ—è¡¨
        'last_cursor': '',  # ä¸Šæ¬¡çš„æ¸¸æ ‡
        'sub_comments_fetched': set(),  # å·²è·å–å­è¯„è®ºçš„è¯„è®º ID
    }


def save_comments(output_dir: Path, note_id: str, data: Dict[str, Any]) -> None:
    """ä¿å­˜è¯„è®ºæ•°æ®"""
    comments_file = output_dir / f"comments_{note_id}.json"
    # è½¬æ¢ set ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–
    save_data = data.copy()
    if isinstance(save_data.get('sub_comments_fetched'), set):
        save_data['sub_comments_fetched'] = list(save_data['sub_comments_fetched'])
    
    with open(comments_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)


def fetch_comments_only(
    api_key: str,
    note_id: str,
    output_dir: Path,
    max_pages: int = None
) -> Dict[str, Any]:
    """
    åªè·å–ä¸€çº§è¯„è®ºï¼ˆä¸è·å–å­è¯„è®ºï¼‰
    å¢é‡ä¿å­˜ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
    
    å‚æ•°:
        max_pages: æœ€å¤§è·å–é¡µæ•°ï¼Œé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–
    """
    # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼
    if max_pages is None:
        max_pages = CONFIG["max_pages"]
    
    request_delay = CONFIG["request_delay"]
    
    # åŠ è½½å·²æœ‰æ•°æ®
    data = load_existing_comments(output_dir, note_id)
    
    # è½¬æ¢ sub_comments_fetched ä¸º set
    if isinstance(data.get('sub_comments_fetched'), list):
        data['sub_comments_fetched'] = set(data['sub_comments_fetched'])
    elif not isinstance(data.get('sub_comments_fetched'), set):
        data['sub_comments_fetched'] = set()
    
    existing_ids = set(data.get('fetched_comment_ids', []))
    cursor = data.get('last_cursor', '')
    
    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè·å–
    fetch_completed = data.get('fetch_completed', False)
    if fetch_completed and len(existing_ids) > 0:
        print(f"\nğŸ“¥ ç¬”è®° {note_id} çš„ä¸€çº§è¯„è®ºå·²å…¨éƒ¨è·å–å®Œæˆ", flush=True)
        print(f"   å·²æœ‰ {len(existing_ids)} æ¡è¯„è®ºï¼Œè·³è¿‡è·å–", flush=True)
        return data
    
    print(f"\nğŸ“¥ è·å–ç¬”è®° {note_id} çš„ä¸€çº§è¯„è®º...", flush=True)
    print(f"   å·²æœ‰ {len(existing_ids)} æ¡è¯„è®º", flush=True)
    print(f"   æœ€å¤§è·å–é¡µæ•°: {max_pages}", flush=True)
    
    page = 0
    new_count = 0
    retry_without_cursor = False
    
    while page < max_pages:
        page += 1
        print(f"   è·å–ç¬¬ {page}/{max_pages} é¡µ...", flush=True)
        
        result = get_note_comments(api_key, note_id, start=cursor)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ HTTP é”™è¯¯
        if 'error' in result and result.get('code') in [400, 401, 403]:
            # å¦‚æœæœ‰ cursor ä¸”æ˜¯ 400 é”™è¯¯ï¼Œå¯èƒ½æ˜¯ cursor è¿‡æœŸï¼Œå°è¯•ä»å¤´å¼€å§‹
            if cursor and result.get('code') == 400 and not retry_without_cursor:
                print(f"   âš ï¸ cursor å¯èƒ½å·²è¿‡æœŸï¼Œå°è¯•ä»å¤´è·å–...", flush=True)
                cursor = ""
                data['last_cursor'] = ""
                save_comments(output_dir, note_id, data)
                retry_without_cursor = True
                page -= 1  # é‡è¯•å½“å‰é¡µ
                continue
            print(f"   âŒ è·å–å¤±è´¥: {result.get('error', result)}", flush=True)
            if result.get('detail'):
                print(f"   è¯¦æƒ…: {result.get('detail')}", flush=True)
            break
        
        if result.get('code') != 200:
            print(f"   âŒ è·å–å¤±è´¥: {result.get('error', result)}", flush=True)
            break
        
        # è§£æå“åº”
        api_data = result.get('data', {})
        inner_data = api_data.get('data', {})
        comments = inner_data.get('comments', [])
        has_more = inner_data.get('has_more', False)
        cursor = inner_data.get('cursor', '')
        
        if not comments:
            print(f"   æ²¡æœ‰æ›´å¤šè¯„è®º", flush=True)
            data['fetch_completed'] = True
            save_comments(output_dir, note_id, data)
            break
        
        # å¤„ç†è¯„è®º
        page_new = 0
        for comment in comments:
            comment_id = comment.get('id')
            if comment_id and comment_id not in existing_ids:
                # æå–ç”¨æˆ·ä¿¡æ¯
                user = comment.get('user', {})
                data['comments'][comment_id] = {
                    'id': comment_id,
                    'content': comment.get('content', ''),
                    'time': comment.get('time'),
                    'user_nickname': user.get('nickname', ''),
                    'user_id': user.get('userid', ''),
                    'like_count': comment.get('like_count', 0),
                    'sub_comment_count': comment.get('sub_comment_count', 0),
                    'ip_location': comment.get('ip_location', ''),
                    'sub_comments': []
                }
                existing_ids.add(comment_id)
                data['fetched_comment_ids'].append(comment_id)
                page_new += 1
                new_count += 1
        
        # ä¿å­˜æ¸¸æ ‡
        data['last_cursor'] = cursor
        
        # ç«‹å³ä¿å­˜
        save_comments(output_dir, note_id, data)
        print(f"   âœ… æœ¬é¡µ {len(comments)} æ¡ï¼Œæ–°å¢ {page_new} æ¡ï¼Œå·²ä¿å­˜", flush=True)
        
        if not has_more:
            print(f"   âœ… å·²è·å–å…¨éƒ¨ä¸€çº§è¯„è®º", flush=True)
            data['fetch_completed'] = True
            save_comments(output_dir, note_id, data)
            break
        
        time.sleep(request_delay)
    
    print(f"   ğŸ“Š å…±è·å– {new_count} æ¡æ–°è¯„è®ºï¼Œæ€»è®¡ {len(existing_ids)} æ¡ä¸€çº§è¯„è®º", flush=True)
    return data


def analyze_sub_comments(data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    åˆ†æå­è¯„è®ºæƒ…å†µï¼Œè¿”å›éœ€è¦è·å–å­è¯„è®ºçš„è¯„è®ºåˆ—è¡¨
    """
    comments_with_subs = []
    
    for comment_id, comment in data.get('comments', {}).items():
        sub_count = comment.get('sub_comment_count', 0)
        if sub_count > 0:
            # æ£€æŸ¥æ˜¯å¦å·²è·å–
            already_fetched = comment_id in data.get('sub_comments_fetched', set())
            fetched_count = len(comment.get('sub_comments', []))
            
            comments_with_subs.append({
                'comment_id': comment_id,
                'content': comment.get('content', '')[:50],
                'sub_comment_count': sub_count,
                'already_fetched': already_fetched,
                'fetched_count': fetched_count
            })
    
    # æŒ‰å­è¯„è®ºæ•°æ’åº
    comments_with_subs.sort(key=lambda x: x['sub_comment_count'], reverse=True)
    return comments_with_subs


def fetch_single_sub_comments(
    api_key: str,
    note_id: str,
    comment_id: str,
    max_pages: int = None
) -> List[Dict[str, Any]]:
    """
    è·å–å•ä¸ªè¯„è®ºçš„æ‰€æœ‰å­è¯„è®º
    
    å‚æ•°:
        max_pages: æœ€å¤§è·å–é¡µæ•°ï¼Œé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–
    """
    # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼
    if max_pages is None:
        max_pages = CONFIG["max_sub_pages"]
    
    request_delay = CONFIG["request_delay"]
    
    all_subs = []
    cursor = ""
    page = 0
    
    while page < max_pages:
        page += 1
        result = get_sub_comments(api_key, note_id, comment_id, start=cursor)
        
        if result.get('code') != 200:
            break
        
        sub_data = result.get('data', {})
        inner = sub_data.get('data', {})
        comments = inner.get('comments', [])
        
        if not comments:
            break
        
        for sub in comments:
            user = sub.get('user', {})
            target = sub.get('target_comment', {})
            target_user = target.get('user', {}) if target else {}
            
            all_subs.append({
                'id': sub.get('id'),
                'content': sub.get('content', ''),
                'time': sub.get('time'),
                'user_nickname': user.get('nickname', ''),
                'user_id': user.get('userid', ''),
                'reply_to_nickname': target_user.get('nickname', ''),
                'ip_location': sub.get('ip_location', '')
            })
        
        # ä½¿ç”¨æœ€åä¸€æ¡çš„ ID ä½œä¸ºæ¸¸æ ‡
        cursor = comments[-1].get('id', '')
        
        # å¦‚æœè¿”å›æ•°é‡å°‘äºé¢„æœŸï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šäº†
        if len(comments) < 10:
            break
        
        time.sleep(request_delay)
    
    return all_subs


def fetch_sub_comments_concurrent(
    api_key: str,
    note_id: str,
    output_dir: Path,
    data: Dict[str, Any],
    comment_ids: List[str],
    max_concurrent: int = None
) -> None:
    """
    å¹¶å‘è·å–å¤šä¸ªè¯„è®ºçš„å­è¯„è®º
    
    å‚æ•°:
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–
    """
    # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼
    if max_concurrent is None:
        max_concurrent = CONFIG["max_concurrent"]
    
    # ç¡®ä¿ sub_comments_fetched æ˜¯ set
    if isinstance(data.get('sub_comments_fetched'), list):
        data['sub_comments_fetched'] = set(data['sub_comments_fetched'])
    elif not isinstance(data.get('sub_comments_fetched'), set):
        data['sub_comments_fetched'] = set()
    
    # è¿‡æ»¤æ‰å·²è·å–çš„
    to_fetch = [cid for cid in comment_ids if cid not in data['sub_comments_fetched']]
    
    if not to_fetch:
        print("   âœ… æ‰€æœ‰å­è¯„è®ºå·²è·å–ï¼Œæ— éœ€é‡å¤è·å–", flush=True)
        return
    
    print(f"\nğŸ“¥ å¹¶å‘è·å– {len(to_fetch)} ä¸ªè¯„è®ºçš„å­è¯„è®º (å¹¶å‘æ•°: {max_concurrent})...", flush=True)
    
    completed = 0
    total_subs = 0
    
    def fetch_one(comment_id: str):
        return comment_id, fetch_single_sub_comments(api_key, note_id, comment_id)
    
    with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
        futures = {executor.submit(fetch_one, cid): cid for cid in to_fetch}
        
        for future in as_completed(futures):
            comment_id = futures[future]
            try:
                cid, subs = future.result()
                
                # æ›´æ–°æ•°æ®
                if cid in data['comments']:
                    data['comments'][cid]['sub_comments'] = subs
                    data['sub_comments_fetched'].add(cid)
                
                completed += 1
                total_subs += len(subs)
                
                # æ¯å®Œæˆä¸€ä¸ªå°±ä¿å­˜ï¼ˆå¢é‡ä¿å­˜ï¼Œé˜²æ­¢ä¸­æ–­ä¸¢å¤±ï¼‰
                save_comments(output_dir, note_id, data)
                
                sub_count = data['comments'].get(cid, {}).get('sub_comment_count', 0)
                print(f"   âœ… [{completed}/{len(to_fetch)}] è¯„è®º {cid[:8]}... è·å– {len(subs)}/{sub_count} æ¡å­è¯„è®ºï¼Œå·²ä¿å­˜", flush=True)
                
            except Exception as e:
                print(f"   âŒ è¯„è®º {comment_id[:8]}... è·å–å¤±è´¥: {e}", flush=True)
    
    print(f"   ğŸ“Š å®Œæˆ {completed}/{len(to_fetch)} ä¸ªè¯„è®ºçš„å­è¯„è®ºè·å–ï¼Œå…± {total_subs} æ¡å­è¯„è®º", flush=True)


def format_timestamp(ts: int) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
    if not ts:
        return "æœªçŸ¥æ—¶é—´"
    try:
        if ts > 10000000000:
            ts = ts // 1000
        return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
    except:
        return "æœªçŸ¥æ—¶é—´"


def generate_summary(
    note_info: Dict[str, Any],
    data: Dict[str, Any],
    output_path: str
) -> None:
    """ç”Ÿæˆè¯„è®ºæ±‡æ€»æ–‡ä»¶"""
    lines = []
    
    # å¸–å­ä¿¡æ¯
    lines.append("=" * 60)
    lines.append(f"ğŸ“ å°çº¢ä¹¦å¸–å­è¯„è®ºæ±‡æ€»")
    lines.append("=" * 60)
    lines.append("")
    lines.append(f"ã€å¸–å­ä¿¡æ¯ã€‘")
    lines.append(f"ID: {note_info.get('id', 'N/A')}")
    lines.append(f"æ ‡é¢˜: {note_info.get('title', 'æ— æ ‡é¢˜')}")
    lines.append(f"ä½œè€…: {note_info.get('author', 'N/A')}")
    lines.append(f"ç‚¹èµæ•°: {note_info.get('liked_count', 0)}")
    lines.append(f"è¯„è®ºæ•°: {note_info.get('comments_count', 0)}")
    lines.append(f"æ”¶è—æ•°: {note_info.get('collected_count', 0)}")
    lines.append("")
    lines.append("-" * 60)
    
    # ç»Ÿè®¡
    comments = data.get('comments', {})
    total_comments = len(comments)
    total_subs = sum(len(c.get('sub_comments', [])) for c in comments.values())
    
    lines.append("")
    lines.append(f"ã€è¯„è®ºç»Ÿè®¡ã€‘")
    lines.append(f"ä¸€çº§è¯„è®ºæ•°: {total_comments}")
    lines.append(f"å­è¯„è®ºæ•°: {total_subs}")
    lines.append(f"æ€»è®¡: {total_comments + total_subs}")
    lines.append("")
    lines.append("=" * 60)
    lines.append("ğŸ’¬ è¯„è®ºè¯¦æƒ…")
    lines.append("=" * 60)
    lines.append("")
    
    # è¯„è®ºè¯¦æƒ…
    for i, (cid, comment) in enumerate(comments.items(), 1):
        nickname = comment.get('user_nickname', 'åŒ¿åç”¨æˆ·')
        content = comment.get('content', '')
        create_time = format_timestamp(comment.get('time', 0))
        like_count = comment.get('like_count', 0)
        sub_count = comment.get('sub_comment_count', 0)
        location = comment.get('ip_location', '')
        
        lines.append(f"ã€{i}æ¥¼ã€‘{nickname} ({location})")
        lines.append(f"   {content}")
        lines.append(f"   â¤ï¸ {like_count}  ğŸ’¬ {sub_count}  ğŸ• {create_time}")
        
        # å­è¯„è®º
        for sub in comment.get('sub_comments', []):
            sub_nickname = sub.get('user_nickname', 'åŒ¿åç”¨æˆ·')
            sub_content = sub.get('content', '')
            sub_time = format_timestamp(sub.get('time', 0))
            reply_to = sub.get('reply_to_nickname', '')
            sub_location = sub.get('ip_location', '')
            
            if reply_to:
                lines.append(f"   â””â”€ {sub_nickname} å›å¤ {reply_to}: {sub_content}")
            else:
                lines.append(f"   â””â”€ {sub_nickname}: {sub_content}")
            lines.append(f"      ({sub_location}) ğŸ• {sub_time}")
        
        lines.append("")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"ğŸ’¾ è¯„è®ºæ±‡æ€»å·²ä¿å­˜: {output_path}", flush=True)


def main():
    print("=" * 60, flush=True)
    print("å°çº¢ä¹¦è¯„è®ºè·å–å·¥å…· V2", flush=True)
    print("=" * 60, flush=True)
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    print(f"\nğŸ“‹ å½“å‰é…ç½® (æ¥è‡ª params/config.json):", flush=True)
    print(f"   å¹¶å‘æ•°: {CONFIG['max_concurrent']}", flush=True)
    print(f"   è¯·æ±‚é—´éš”: {CONFIG['request_delay']}ç§’", flush=True)
    print(f"   ä¸€çº§è¯„è®ºæœ€å¤§é¡µæ•°: {CONFIG['max_pages']}", flush=True)
    print(f"   å­è¯„è®ºé˜ˆå€¼: >= {CONFIG['sub_comment_threshold']} æ¡æ‰è·å–", flush=True)
    print(f"   å­è¯„è®ºæœ€å¤§é¡µæ•°: {CONFIG['max_sub_pages']}", flush=True)
    
    api_key = load_api_key()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    script_dir = Path(__file__).parent
    output_dir = script_dir / "output"
    output_dir.mkdir(exist_ok=True)
    
    # ç¡®å®šè¦å¤„ç†çš„å¸–å­
    note_ids = CONFIG.get("note_ids", [])
    
    if note_ids:
        # ä»é…ç½®æ–‡ä»¶è¯»å–å¸–å­ ID
        print(f"\nğŸ“‹ ä»é…ç½®æ–‡ä»¶è¯»å– {len(note_ids)} ä¸ªå¸–å­ ID", flush=True)
        
        # å°è¯•ä» top3_notes.json è·å–å¸–å­è¯¦æƒ…
        search_output_dir = script_dir.parent / "search" / "output"
        top3_file = search_output_dir / "top3_notes.json"
        
        notes_info = {}
        if top3_file.exists():
            with open(top3_file, 'r', encoding='utf-8') as f:
                top3_notes = json.load(f)
                for note in top3_notes:
                    notes_info[note['id']] = note
        
        # æ„å»ºå¸–å­åˆ—è¡¨
        top3_notes = []
        for nid in note_ids:
            if nid in notes_info:
                top3_notes.append(notes_info[nid])
            else:
                # å¦‚æœæ²¡æœ‰è¯¦æƒ…ï¼Œåˆ›å»ºåŸºæœ¬ä¿¡æ¯
                top3_notes.append({
                    'id': nid,
                    'title': f'å¸–å­ {nid[:8]}...',
                    'comments_count': 0,
                    'liked_count': 0,
                    'collected_count': 0,
                    'author': 'æœªçŸ¥'
                })
    else:
        # ä» top3_notes.json è¯»å–
        search_output_dir = script_dir.parent / "search" / "output"
        top3_file = search_output_dir / "top3_notes.json"
        
        if not top3_file.exists():
            print(f"âŒ æœªæ‰¾åˆ° Top3 å¸–å­æ–‡ä»¶: {top3_file}", flush=True)
            print(f"   è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š note_ids æˆ–ç¡®ä¿ top3_notes.json å­˜åœ¨", flush=True)
            return
        
        with open(top3_file, 'r', encoding='utf-8') as f:
            top3_notes = json.load(f)
    
    print(f"\nğŸ“‹ å°†å¤„ç†ä»¥ä¸‹ {len(top3_notes)} ä¸ªå¸–å­:", flush=True)
    for i, note in enumerate(top3_notes, 1):
        title = note.get('title', 'æ— æ ‡é¢˜')[:40]
        comments = note.get('comments_count', 0)
        print(f"   {i}. {title}... (è¯„è®ºæ•°: {comments})", flush=True)
    
    # ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰ä¸€çº§è¯„è®º
    print(f"\n{'='*60}", flush=True)
    print("ğŸ“Œ ç¬¬ä¸€æ­¥ï¼šè·å–ä¸€çº§è¯„è®ºï¼ˆå¢é‡è·å–ï¼Œå·²æœ‰æ•°æ®ä¸é‡å¤è·å–ï¼‰", flush=True)
    print(f"{'='*60}", flush=True)
    
    all_data = {}
    for note in top3_notes:
        note_id = note['id']
        data = fetch_comments_only(api_key, note_id, output_dir)
        all_data[note_id] = data
    
    # ç¬¬äºŒæ­¥ï¼šåˆ†æå­è¯„è®ºæƒ…å†µ
    print(f"\n{'='*60}", flush=True)
    print("ğŸ“Œ ç¬¬äºŒæ­¥ï¼šå­è¯„è®ºç»Ÿè®¡", flush=True)
    print(f"{'='*60}", flush=True)
    
    sub_comment_threshold = CONFIG["sub_comment_threshold"]
    
    for note in top3_notes:
        note_id = note['id']
        data = all_data[note_id]
        
        title = note.get('title', 'æ— æ ‡é¢˜')[:30]
        print(f"\nã€{title}...ã€‘", flush=True)
        
        subs_analysis = analyze_sub_comments(data)
        
        if not subs_analysis:
            print("   æ²¡æœ‰å­è¯„è®º", flush=True)
            continue
        
        # ç»Ÿè®¡
        total_sub_count = sum(s['sub_comment_count'] for s in subs_analysis)
        already_fetched = sum(1 for s in subs_analysis if s['already_fetched'])
        need_fetch = sum(1 for s in subs_analysis 
                        if s['sub_comment_count'] >= sub_comment_threshold and not s['already_fetched'])
        
        print(f"   æœ‰å­è¯„è®ºçš„è¯„è®ºæ•°: {len(subs_analysis)}", flush=True)
        print(f"   å­è¯„è®ºæ€»æ•°ï¼ˆé¢„ä¼°ï¼‰: {total_sub_count}", flush=True)
        print(f"   å·²è·å–å­è¯„è®ºçš„è¯„è®ºæ•°: {already_fetched}", flush=True)
        print(f"   å¾…è·å–ï¼ˆå­è¯„è®ºæ•° >= {sub_comment_threshold}ï¼‰: {need_fetch}", flush=True)
        
        # æ˜¾ç¤ºå‰10ä¸ª
        print(f"\n   å­è¯„è®ºæ•°é‡ Top 10:", flush=True)
        print(f"   {'è¯„è®ºID':<26} {'å­è¯„è®ºæ•°':<10} {'å·²è·å–':<10} {'å†…å®¹'}", flush=True)
        print(f"   {'-'*70}", flush=True)
        
        for s in subs_analysis[:10]:
            status = "âœ…" if s['already_fetched'] else "âŒ"
            content = s['content'][:30] if s['content'] else '(æ— å†…å®¹)'
            print(f"   {s['comment_id']:<26} {s['sub_comment_count']:<10} {status:<10} {content}", flush=True)
    
    # è¯¢é—®æ˜¯å¦è·å–å­è¯„è®º
    print(f"\n{'='*60}", flush=True)
    print("ğŸ’¡ å­è¯„è®ºè·å–è¯´æ˜:", flush=True)
    print(f"   - åªè·å–å­è¯„è®ºæ•° >= {sub_comment_threshold} çš„è¯„è®ºçš„å­è¯„è®º", flush=True)
    print("   - æ¯æ¬¡ API è°ƒç”¨éƒ½ä¼šäº§ç”Ÿè´¹ç”¨", flush=True)
    print("   - å·²è·å–çš„å­è¯„è®ºä¸ä¼šé‡å¤è·å–ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰", flush=True)
    print("   - æ¯è·å–å®Œä¸€ä¸ªè¯„è®ºçš„å­è¯„è®ºå°±ç«‹å³ä¿å­˜", flush=True)
    print(f"{'='*60}", flush=True)
    
    # è‡ªåŠ¨è·å–å­è¯„è®ºï¼ˆåªè·å–å­è¯„è®ºæ•° >= é˜ˆå€¼ çš„ï¼‰
    print(f"\nğŸ“Œ ç¬¬ä¸‰æ­¥ï¼šè·å–å­è¯„è®º (å­è¯„è®ºæ•° >= {sub_comment_threshold} çš„è¯„è®º)", flush=True)
    
    for note in top3_notes:
        note_id = note['id']
        data = all_data[note_id]
        
        title = note.get('title', 'æ— æ ‡é¢˜')[:30]
        print(f"\nã€{title}...ã€‘", flush=True)
        
        subs_analysis = analyze_sub_comments(data)
        
        # åªè·å–å­è¯„è®ºæ•° >= é˜ˆå€¼ çš„
        to_fetch = [s['comment_id'] for s in subs_analysis 
                   if s['sub_comment_count'] >= sub_comment_threshold and not s['already_fetched']]
        
        if to_fetch:
            print(f"   éœ€è¦è·å– {len(to_fetch)} ä¸ªè¯„è®ºçš„å­è¯„è®º", flush=True)
            fetch_sub_comments_concurrent(api_key, note_id, output_dir, data, to_fetch)
        else:
            print("   âœ… æ— éœ€è·å–å­è¯„è®ºï¼ˆå·²å…¨éƒ¨è·å–æˆ–æ— ç¬¦åˆæ¡ä»¶çš„è¯„è®ºï¼‰", flush=True)
    
    # ç¬¬å››æ­¥ï¼šç”Ÿæˆæ±‡æ€»æ–‡ä»¶
    print(f"\n{'='*60}", flush=True)
    print("ğŸ“Œ ç¬¬å››æ­¥ï¼šç”Ÿæˆæ±‡æ€»æ–‡ä»¶", flush=True)
    print(f"{'='*60}", flush=True)
    
    for note in top3_notes:
        note_id = note['id']
        # é‡æ–°åŠ è½½æœ€æ–°æ•°æ®
        data = load_existing_comments(output_dir, note_id)
        summary_file = output_dir / f"summary_{note_id}.txt"
        generate_summary(note, data, str(summary_file))
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*60}", flush=True)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡", flush=True)
    print(f"{'='*60}", flush=True)
    
    for note in top3_notes:
        note_id = note['id']
        data = load_existing_comments(output_dir, note_id)
        
        comments = data.get('comments', {})
        total_comments = len(comments)
        total_subs = sum(len(c.get('sub_comments', [])) for c in comments.values())
        
        title = note.get('title', 'æ— æ ‡é¢˜')[:30]
        api_total = note.get('comments_count', 0)
        
        print(f"\nã€{title}...ã€‘", flush=True)
        print(f"   æœç´¢æ¥å£æ˜¾ç¤ºè¯„è®ºæ•°: {api_total}", flush=True)
        print(f"   å®é™…è·å–ä¸€çº§è¯„è®ºæ•°: {total_comments}", flush=True)
        print(f"   å®é™…è·å–å­è¯„è®ºæ•°: {total_subs}", flush=True)
        print(f"   å®é™…è·å–æ€»è®¡: {total_comments + total_subs}", flush=True)
    
    print(f"\n{'='*60}", flush=True)
    print("âœ… å®Œæˆï¼", flush=True)
    print(f"{'='*60}", flush=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}", flush=True)
    print(f"é…ç½®æ–‡ä»¶: {Path(__file__).parent / 'params' / 'config.json'}", flush=True)


if __name__ == "__main__":
    main()
