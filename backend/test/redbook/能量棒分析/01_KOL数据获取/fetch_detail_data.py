#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é˜¶æ®µ4: è·å–è¯¦ç»†æ•°æ®

åŠŸèƒ½ï¼š
1. ä»ç­›é€‰ç»“æœä¸­è¯»å–é€šè¿‡ç­›é€‰çš„KOL
2. è°ƒç”¨7ä¸ªè¯¦ç»†API:
   - kol_fans_portrait: ç²‰ä¸ç”»åƒ
   - kol_fans_summary: ç²‰ä¸è´¨é‡
   - kol_note_list: ç¬”è®°åˆ—è¡¨
   - kol_data_summary_v1: æ•°æ®æ±‡æ€»V1
   - kol_data_summary_v2: æ•°æ®æ±‡æ€»V2
   - kol_cost_effective: æ€§ä»·æ¯”
   - kol_core_data: æ ¸å¿ƒæ•°æ®
3. æ¯ä¸ªAPIè°ƒç”¨åç«‹å³ä¿å­˜
4. æ”¯æŒæ–­ç‚¹ç»­ä¼ 

é¢„è®¡APIè°ƒç”¨: 8 KOL Ã— 7 API = 56æ¬¡
"""

import os
import json
import asyncio
import aiohttp
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, List
from dataclasses import dataclass
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ==================== é…ç½® ====================
CONFIG = {
    "api_base_url": "https://api.justoneapi.com",
    "concurrency": 7,
    "timeout": 30,
    "retry_count": 3,
    "retry_delay": 2,
    "api_delay": 0.5,
    
    # è¯¦ç»†æ•°æ®APIï¼ˆ7ä¸ªï¼‰
    "detail_apis": {
        "kol_fans_portrait": {
            "endpoint": "/api/xiaohongshu-pgy/get-kol-fans-portrait/v1",
            "params": {"acceptCache": "true"}
        },
        "kol_fans_summary": {
            "endpoint": "/api/xiaohongshu-pgy/get-kol-fans-summary/v1",
            "params": {"acceptCache": "true"}
        },
        "kol_note_list": {
            "endpoint": "/api/xiaohongshu-pgy/get-kol-note-list/v1",
            "params": {
                "page": 1,
                "adSwitch": "_1",
                "orderType": "_1",
                "noteType": "_4",
                "acceptCache": "true"
            }
        },
        "kol_data_summary_v1": {
            "endpoint": "/api/xiaohongshu-pgy/get-kol-data-summary/v1",
            "params": {"business": "_0", "acceptCache": "true"}
        },
        "kol_data_summary_v2": {
            "endpoint": "/api/xiaohongshu-pgy/get-kol-data-summary/v2",
            "params": {"business": "_0", "acceptCache": "true"}
        },
        "kol_cost_effective": {
            "endpoint": "/api/xiaohongshu-pgy/get-kol-cost-effective/v1",
            "params": {"acceptCache": "true"}
        },
        "kol_core_data": {
            "endpoint": "/api/xiaohongshu-pgy/get-kol-core-data/v1",
            "params": {
                "dateType": "_1",
                "noteType": "_3",
                "adSwitch": "_1",
                "business": "_0",
                "acceptCache": "true"
            }
        }
    },
    
    # æ’é™¤çš„KOL IDï¼ˆè¿‘30å¤©æ— å‘å¸–ï¼‰
    "excluded_kol_ids": [
        "59b8ccb682ec3904bb6f4b57",  # å¸éƒ½æ˜Ÿå…‰æµ·å½’
        "60f18e370000000001016085",  # Katrina
        "5d2f4bbb000000001102b119",  # å°ç”Ÿé¥­é¥­
        "58d094f482ec396e6c9f634f",  # Alexçˆ±è¿åŠ¨
        "6553833c0000000002036a22",  # ç¾ä¼¢ï¼ˆå‡è„‚ç‰ˆï¼‰
        "6055b9340000000001006c61",  # äºŒå§ç¾é£Ÿ
        "6653287700000000070050c6",  # å§—å§—æ¥è¿Ÿ
    ]
}


@dataclass
class FetchProgress:
    """è·å–è¿›åº¦"""
    total_kols: int = 0
    completed_kols: int = 0
    total_apis: int = 0
    success_apis: int = 0
    failed_apis: int = 0
    skipped_apis: int = 0


class DetailDataFetcher:
    """è¯¦ç»†æ•°æ®è·å–å™¨"""
    
    def __init__(self):
        self.config = CONFIG
        self.token = self._load_api_token()
        self.base_url = self.config['api_base_url']
        self.output_dir = Path(__file__).parent / "02_è¯¦ç»†æ•°æ®"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½KOLåˆ—è¡¨
        self.kol_list_file = Path(__file__).parent / "kol_list.json"
        self.kols = self._load_filtered_kols()
        
        # è¿›åº¦è·Ÿè¸ª
        self.progress = FetchProgress()
        
        # å·²è·å–è®°å½•
        self.fetched_record_file = self.output_dir / "_fetched_record.json"
        self.fetched_record = self._load_fetched_record()
        
        # ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        self.semaphore = asyncio.Semaphore(self.config['concurrency'])
    
    def _load_api_token(self) -> str:
        """ä»ç¯å¢ƒå˜é‡åŠ è½½ Just One API Token"""
        backend_dir = Path(__file__).parent.parent.parent.parent.parent
        env_path = backend_dir / '.env'
        
        if env_path.exists():
            load_dotenv(env_path)
        
        token = os.getenv('JUSTONEAPI_API_KEY', '')
        if not token:
            raise ValueError("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® JUSTONEAPI_API_KEY")
        return token
    
    def _load_filtered_kols(self) -> List[Dict[str, Any]]:
        """åŠ è½½ç­›é€‰åçš„KOLåˆ—è¡¨ï¼ˆæ’é™¤æ— å‘å¸–çš„ï¼‰"""
        if not self.kol_list_file.exists():
            raise FileNotFoundError(f"KOLåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {self.kol_list_file}")
        
        with open(self.kol_list_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        kols = data.get('kols', [])
        excluded = set(self.config['excluded_kol_ids'])
        
        # åªè¿”å›æœ‰kol_idä¸”ä¸åœ¨æ’é™¤åˆ—è¡¨ä¸­çš„
        filtered = [k for k in kols if k.get('kol_id') and k['kol_id'] not in excluded]
        
        logger.info(f"ä» {len(kols)} ä¸ªKOLä¸­ç­›é€‰å‡º {len(filtered)} ä¸ªï¼ˆæ’é™¤ {len(excluded)} ä¸ªæ— å‘å¸–ï¼‰")
        return filtered
    
    def _load_fetched_record(self) -> Dict[str, Dict[str, bool]]:
        """åŠ è½½å·²è·å–è®°å½•"""
        if self.fetched_record_file.exists():
            with open(self.fetched_record_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_fetched_record(self):
        """ä¿å­˜å·²è·å–è®°å½•"""
        with open(self.fetched_record_file, 'w', encoding='utf-8') as f:
            json.dump(self.fetched_record, f, ensure_ascii=False, indent=2)
    
    def _is_api_fetched(self, kol_id: str, api_name: str) -> bool:
        """æ£€æŸ¥æŸä¸ªAPIæ˜¯å¦å·²è·å–"""
        return self.fetched_record.get(kol_id, {}).get(api_name, False)
    
    def _mark_api_fetched(self, kol_id: str, api_name: str, success: bool = True):
        """æ ‡è®°APIå·²è·å–"""
        if kol_id not in self.fetched_record:
            self.fetched_record[kol_id] = {}
        self.fetched_record[kol_id][api_name] = success
        self._save_fetched_record()
    
    async def _call_api(self, session: aiohttp.ClientSession, endpoint: str, 
                        params: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨å•ä¸ªAPI"""
        async with self.semaphore:
            url = f"{self.base_url}{endpoint}"
            params['token'] = self.token
            
            for attempt in range(self.config['retry_count']):
                try:
                    async with session.get(url, params=params,
                                          timeout=aiohttp.ClientTimeout(total=self.config['timeout'])) as response:
                        if response.status == 200:
                            result = await response.json()
                            await asyncio.sleep(self.config['api_delay'])
                            return result
                        else:
                            logger.warning(f"HTTP {response.status}")
                except asyncio.TimeoutError:
                    logger.warning(f"è¶…æ—¶ï¼Œé‡è¯• {attempt + 1}/{self.config['retry_count']}")
                except Exception as e:
                    logger.warning(f"é”™è¯¯: {e}ï¼Œé‡è¯• {attempt + 1}/{self.config['retry_count']}")
                
                if attempt < self.config['retry_count'] - 1:
                    await asyncio.sleep(self.config['retry_delay'])
            
            return {"error": "APIè°ƒç”¨å¤±è´¥"}
    
    def _save_api_result(self, kol_id: str, kol_name: str, api_name: str, result: Dict[str, Any]):
        """ä¿å­˜å•ä¸ªAPIç»“æœ"""
        kol_dir = self.output_dir / f"kol_{kol_id}"
        kol_dir.mkdir(parents=True, exist_ok=True)
        
        result_file = kol_dir / f"{api_name}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                "kol_id": kol_id,
                "kol_name": kol_name,
                "api_name": api_name,
                "fetch_time": datetime.now().isoformat(),
                "result": result
            }, f, ensure_ascii=False, indent=2)
    
    async def fetch_kol_detail_data(self, session: aiohttp.ClientSession, 
                                     kol: Dict[str, Any], index: int) -> Dict[str, Any]:
        """è·å–å•ä¸ªKOLçš„è¯¦ç»†æ•°æ®"""
        kol_id = kol['kol_id']
        kol_name = kol['name']
        
        logger.info(f"[{index}/{self.progress.total_kols}] è·å–è¯¦ç»†æ•°æ®: {kol_name} ({kol_id[:16]}...)")
        
        results = {}
        
        for api_name, api_config in self.config['detail_apis'].items():
            # æ£€æŸ¥æ˜¯å¦å·²è·å–
            if self._is_api_fetched(kol_id, api_name):
                logger.info(f"  â­ï¸ {api_name}: å·²è·å–ï¼Œè·³è¿‡")
                self.progress.skipped_apis += 1
                continue
            
            # è°ƒç”¨API
            params = {"kolId": kol_id}
            params.update(api_config['params'])
            
            result = await self._call_api(session, api_config['endpoint'], params)
            self.progress.total_apis += 1
            
            # åˆ¤æ–­æˆåŠŸ/å¤±è´¥
            if 'error' in result:
                logger.error(f"  âŒ {api_name}: {result['error']}")
                self.progress.failed_apis += 1
                self._mark_api_fetched(kol_id, api_name, False)
            else:
                code = result.get('code', -1)
                if code == 0:
                    logger.info(f"  âœ… {api_name}: æˆåŠŸ")
                    self.progress.success_apis += 1
                    self._mark_api_fetched(kol_id, api_name, True)
                else:
                    logger.warning(f"  âš ï¸ {api_name}: code={code}")
                    self.progress.failed_apis += 1
                    self._mark_api_fetched(kol_id, api_name, False)
            
            # ç«‹å³ä¿å­˜ç»“æœ
            self._save_api_result(kol_id, kol_name, api_name, result)
            results[api_name] = result
        
        self.progress.completed_kols += 1
        
        # æ˜¾ç¤ºè¿›åº¦
        total_api_calls = self.progress.success_apis + self.progress.failed_apis + self.progress.skipped_apis
        logger.info(f"  ğŸ“Š è¿›åº¦: {self.progress.completed_kols}/{self.progress.total_kols} KOL | "
                   f"APIè°ƒç”¨: {total_api_calls} (æˆåŠŸ:{self.progress.success_apis} å¤±è´¥:{self.progress.failed_apis} è·³è¿‡:{self.progress.skipped_apis})")
        
        return results
    
    async def fetch_all(self, limit: int = None):
        """è·å–æ‰€æœ‰KOLçš„è¯¦ç»†æ•°æ®"""
        kols = self.kols
        if limit:
            kols = kols[:limit]
        
        if not kols:
            logger.warning("æ²¡æœ‰å¯è·å–çš„KOL")
            return
        
        self.progress.total_kols = len(kols)
        expected_api_calls = len(kols) * len(self.config['detail_apis'])
        
        logger.info("=" * 60)
        logger.info(f"ğŸš€ é˜¶æ®µ4: è·å–è¯¦ç»†æ•°æ®")
        logger.info("=" * 60)
        logger.info(f"KOLæ•°é‡: {len(kols)}")
        logger.info(f"æ¯KOL APIæ•°: {len(self.config['detail_apis'])}")
        logger.info(f"é¢„è®¡APIè°ƒç”¨: {expected_api_calls}æ¬¡")
        logger.info(f"å¹¶å‘æ•°: {self.config['concurrency']}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info("=" * 60)
        
        # æ˜¾ç¤ºè¦å¤„ç†çš„KOL
        logger.info("å¾…å¤„ç†KOL:")
        for kol in kols:
            logger.info(f"  - {kol['name']}")
        logger.info("")
        
        async with aiohttp.ClientSession() as session:
            for i, kol in enumerate(kols, 1):
                await self.fetch_kol_detail_data(session, kol, i)
        
        self._print_summary()
        self._merge_all_data()
    
    def _print_summary(self):
        """æ‰“å°æ±‡æ€»"""
        logger.info("")
        logger.info("=" * 60)
        logger.info("ğŸ“‹ é˜¶æ®µ4å®Œæˆæ±‡æ€»")
        logger.info("=" * 60)
        logger.info(f"KOLæ€»æ•°: {self.progress.total_kols}")
        logger.info(f"å®ŒæˆKOL: {self.progress.completed_kols}")
        logger.info(f"APIæˆåŠŸ: {self.progress.success_apis}")
        logger.info(f"APIå¤±è´¥: {self.progress.failed_apis}")
        logger.info(f"APIè·³è¿‡: {self.progress.skipped_apis}")
        logger.info(f"æ•°æ®ç›®å½•: {self.output_dir}")
    
    def _merge_all_data(self):
        """åˆå¹¶æ‰€æœ‰æ•°æ®åˆ°ä¸€ä¸ªæ±‡æ€»æ–‡ä»¶"""
        all_data = []
        screening_dir = Path(__file__).parent / "01_åŸºç¡€ç­›é€‰æ•°æ®"
        
        for kol in self.kols:
            kol_id = kol['kol_id']
            kol_name = kol['name']
            
            kol_data = {
                "kol_id": kol_id,
                "kol_name": kol_name,
                "screening_data": {},
                "detail_data": {}
            }
            
            # è¯»å–ç­›é€‰æ•°æ®
            screening_kol_dir = screening_dir / f"kol_{kol_id}"
            if screening_kol_dir.exists():
                for api_file in screening_kol_dir.glob("*.json"):
                    api_name = api_file.stem
                    with open(api_file, 'r', encoding='utf-8') as f:
                        kol_data['screening_data'][api_name] = json.load(f)
            
            # è¯»å–è¯¦ç»†æ•°æ®
            detail_kol_dir = self.output_dir / f"kol_{kol_id}"
            if detail_kol_dir.exists():
                for api_file in detail_kol_dir.glob("*.json"):
                    api_name = api_file.stem
                    with open(api_file, 'r', encoding='utf-8') as f:
                        kol_data['detail_data'][api_name] = json.load(f)
            
            all_data.append(kol_data)
        
        # ä¿å­˜æ±‡æ€»
        summary_file = self.output_dir / "_all_kol_data.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump({
                "generated_at": datetime.now().isoformat(),
                "total_kols": len(all_data),
                "kols": all_data
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ“Š æ±‡æ€»æ•°æ®å·²ä¿å­˜: {summary_file}")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è·å–KOLè¯¦ç»†æ•°æ®')
    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶è·å–æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼Œåªè·å–å‰2ä¸ª')
    args = parser.parse_args()
    
    limit = 2 if args.test else args.limit
    
    fetcher = DetailDataFetcher()
    await fetcher.fetch_all(limit=limit)


if __name__ == "__main__":
    asyncio.run(main())
