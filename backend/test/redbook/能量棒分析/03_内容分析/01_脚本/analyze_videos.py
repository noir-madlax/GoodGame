#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é˜¶æ®µ6-3: AIè§†é¢‘åˆ†æ

åŠŸèƒ½ï¼š
1. ä½¿ç”¨Gemini 2.5 Flashåˆ†æè§†é¢‘
2. æ”¯æŒåŒAPI Keyå¹¶è¡Œï¼ˆGEMINI_API_KEY2 + GEMINI_API_KEY3ï¼‰
3. 4å¹¶å‘å¤„ç†
4. æ–­ç‚¹ç»­ä¼ ï¼Œè·³è¿‡å·²åˆ†æçš„è§†é¢‘
5. ä¿å­˜åŸå§‹è¿”å›å’Œè¿›åº¦

ç›®æ ‡KOL (4äºº, 18ä¸ªè§†é¢‘)
"""

import os
import sys
import json
import time
import hashlib
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from typing import Dict, Any, Optional, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import threading

# åŠ è½½ç¯å¢ƒå˜é‡
BACKEND_DIR = Path("/Users/rigel/project/hdl-tikhub-goodgame/backend")
load_dotenv(BACKEND_DIR / '.env')

# é¡¹ç›®è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_DIR = SCRIPT_DIR.parent
DATA_DIR = PROJECT_DIR / "02_è§†é¢‘æ•°æ®"
RESULT_DIR = PROJECT_DIR / "03_åˆ†æç»“æœ"
PROMPT_DIR = PROJECT_DIR / "prompts"

# ç¡®ä¿ç›®å½•å­˜åœ¨
RESULT_DIR.mkdir(parents=True, exist_ok=True)

# API Keys
API_KEYS = []
key2 = os.getenv('GEMINI_API_KEY2', '')
key3 = os.getenv('GEMINI_API_KEY3', '')
if key2:
    API_KEYS.append(key2)
if key3:
    API_KEYS.append(key3)
if not API_KEYS:
    # å¤‡ç”¨key
    key1 = os.getenv('GEMINI_API_KEY', '') or os.getenv('GEMINI_API_KEY_ANALYZE', '')
    if key1:
        API_KEYS.append(key1)

print(f"ğŸ“Œ å¯ç”¨API Keyæ•°é‡: {len(API_KEYS)}")

# é…ç½®
CONFIG = {
    "max_workers": 4,  # 4å¹¶å‘
    "max_retries": 3,
    "model_name": "gemini-2.5-flash",
    "temperature": 0.3,
    "max_output_tokens": 8192,
    "request_timeout": 300,
}


def log(msg: str):
    """å®æ—¶æ‰“å°æ—¥å¿—"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}] {msg}", flush=True)


def load_system_prompt() -> str:
    """åŠ è½½ç³»ç»Ÿprompt"""
    prompt_file = PROMPT_DIR / "video_analysis_prompt.txt"
    if prompt_file.exists():
        with open(prompt_file, 'r', encoding='utf-8') as f:
            return f.read()
    raise FileNotFoundError(f"Promptæ–‡ä»¶ä¸å­˜åœ¨: {prompt_file}")


def load_video_list() -> List[Dict]:
    """åŠ è½½è§†é¢‘åˆ—è¡¨"""
    video_list_file = DATA_DIR / "video_list.json"
    with open(video_list_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('videos', [])


def get_analyzed_videos() -> set:
    """è·å–å·²åˆ†æçš„è§†é¢‘ID"""
    analyzed = set()
    for kol_dir in RESULT_DIR.glob("kol_*"):
        for f in kol_dir.glob("*_analysis.json"):
            note_id = f.stem.replace('_analysis', '')
            analyzed.add(note_id)
    return analyzed


def save_analysis_result(video_meta: Dict, result: Dict, raw_response: str):
    """ä¿å­˜åˆ†æç»“æœ"""
    kol_id = video_meta.get('kol_id')
    note_id = video_meta.get('note_id')
    
    result_dir = RESULT_DIR / f"kol_{kol_id}"
    result_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜è§£æåçš„ç»“æœ
    result_file = result_dir / f"{note_id}_analysis.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump({
            'video_metadata': video_meta,
            'analysis_result': result,
            'analyzed_at': datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜åŸå§‹è¿”å›
    raw_file = result_dir / f"{note_id}_raw_response.txt"
    with open(raw_file, 'w', encoding='utf-8') as f:
        f.write(raw_response)


def create_analysis_prompt(video_meta: Dict) -> str:
    """åˆ›å»ºåˆ†ææç¤º"""
    content = video_meta.get('content', 'æ— æ­£æ–‡')
    if len(content) > 1000:
        content = content[:1000] + "..."
    
    return f"""
## è§†é¢‘èƒŒæ™¯ä¿¡æ¯

- **åšä¸»åç§°**: {video_meta.get('kol_name', 'æœªçŸ¥')}
- **è§†é¢‘æ ‡é¢˜**: {video_meta.get('title', 'æ— æ ‡é¢˜')}
- **è§†é¢‘æ­£æ–‡**: 
{content}
- **ç‚¹èµæ•°**: {video_meta.get('like_num', 0)}
- **æ”¶è—æ•°**: {video_meta.get('collect_num', 0)}

è¯·è§‚çœ‹ä¸Šä¼ çš„è§†é¢‘ï¼Œç»“åˆä»¥ä¸ŠèƒŒæ™¯ä¿¡æ¯ï¼Œè¿›è¡Œä¸“ä¸šåˆ†æã€‚
"""


def analyze_video_with_key(video_path: str, video_meta: Dict, api_key: str, 
                           video_index: int, total: int) -> Tuple[Optional[Dict], str]:
    """ä½¿ç”¨æŒ‡å®šAPI Keyåˆ†æè§†é¢‘"""
    import google.generativeai as genai
    
    genai.configure(api_key=api_key)
    
    note_id = video_meta.get('note_id', 'unknown')
    kol_name = video_meta.get('kol_name', 'Unknown')
    title = (video_meta.get('title') or 'æ— æ ‡é¢˜')[:25]
    
    # è·å–keyçš„å4ä½ç”¨äºæ—¥å¿—
    key_suffix = api_key[-4:] if len(api_key) > 4 else '????'
    
    for attempt in range(CONFIG['max_retries']):
        try:
            system_prompt = load_system_prompt()
            
            if attempt == 0:
                log(f"  [{video_index}/{total}] ğŸ“¤ ä¸Šä¼ : {kol_name} - {title}... (key:...{key_suffix})")
            else:
                log(f"  [{video_index}/{total}] ğŸ”„ é‡è¯•({attempt+1}): {title}...")
                time.sleep(30 * attempt)
            
            # ä¸Šä¼ è§†é¢‘
            video_file = genai.upload_file(video_path, mime_type="video/mp4")
            
            log(f"  [{video_index}/{total}] â³ å¤„ç†ä¸­...")
            while video_file.state.name == "PROCESSING":
                time.sleep(3)
                video_file = genai.get_file(video_file.name)
            
            if video_file.state.name != "ACTIVE":
                log(f"  [{video_index}/{total}] âŒ å¤„ç†å¤±è´¥: {video_file.state.name}")
                continue
            
            # åˆ›å»ºæ¨¡å‹
            model = genai.GenerativeModel(
                model_name=CONFIG['model_name'],
                generation_config={
                    "temperature": CONFIG['temperature'],
                    "max_output_tokens": CONFIG['max_output_tokens']
                }
            )
            
            user_prompt = create_analysis_prompt(video_meta)
            
            log(f"  [{video_index}/{total}] ğŸ” åˆ†æä¸­...")
            response = model.generate_content(
                [video_file, system_prompt, user_prompt],
                request_options={"timeout": CONFIG['request_timeout']}
            )
            
            raw_response = response.text
            
            # æå–JSON
            result_text = raw_response
            if "```json" in result_text:
                json_start = result_text.find("```json") + 7
                json_end = result_text.find("```", json_start)
                result_text = result_text[json_start:json_end].strip()
            elif "```" in result_text:
                json_start = result_text.find("```") + 3
                json_end = result_text.find("```", json_start)
                result_text = result_text[json_start:json_end].strip()
            
            # ä¿®å¤ä¸å®Œæ•´JSON
            try:
                result = json.loads(result_text)
            except json.JSONDecodeError:
                if result_text.count('{') > result_text.count('}'):
                    missing = result_text.count('{') - result_text.count('}')
                    result_text = result_text.rstrip(',\n ') + '\n' + '}' * missing
                    result = json.loads(result_text)
                else:
                    raise
            
            # æ¸…ç†ä¸Šä¼ æ–‡ä»¶
            try:
                genai.delete_file(video_file.name)
            except:
                pass
            
            score = result.get('overall_assessment', {}).get('overall_score', 'N/A')
            log(f"  [{video_index}/{total}] âœ… å®Œæˆ: {kol_name} - {title} | è¯„åˆ†: {score}")
            
            return result, raw_response
            
        except json.JSONDecodeError as e:
            log(f"  [{video_index}/{total}] âŒ JSONé”™è¯¯: {title}")
            return None, f"JSONè§£æé”™è¯¯: {e}"
        except Exception as e:
            err_str = str(e)
            if "429" in err_str or "quota" in err_str.lower():
                log(f"  [{video_index}/{total}] âš ï¸ é…é¢é™åˆ¶(key:...{key_suffix})ï¼Œç­‰å¾…é‡è¯•...")
                if attempt < CONFIG['max_retries'] - 1:
                    time.sleep(60 * (attempt + 1))
                    continue
            log(f"  [{video_index}/{total}] âŒ å¼‚å¸¸: {title} - {err_str[:80]}")
            if attempt == CONFIG['max_retries'] - 1:
                return None, f"å¼‚å¸¸: {err_str}"
    
    return None, "è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"


class ApiKeyPool:
    """API Keyè½®è¯¢æ± """
    
    def __init__(self, keys: List[str]):
        self.keys = keys
        self.index = 0
        self.lock = threading.Lock()
    
    def get_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„key"""
        with self.lock:
            key = self.keys[self.index % len(self.keys)]
            self.index += 1
            return key


def analyze_single_task(args: Tuple) -> Tuple[str, Optional[Dict], str]:
    """å•ä¸ªè§†é¢‘åˆ†æä»»åŠ¡"""
    video_meta, video_index, total, key_pool = args
    
    note_id = video_meta['note_id']
    file_path = video_meta.get('file_path')
    
    if not file_path or not Path(file_path).exists():
        log(f"  [{video_index}/{total}] âŒ æ–‡ä»¶ä¸å­˜åœ¨: {note_id}")
        return note_id, None, "æ–‡ä»¶ä¸å­˜åœ¨"
    
    # ä»æ± ä¸­è·å–API Key
    api_key = key_pool.get_key()
    
    result, raw_response = analyze_video_with_key(
        file_path, video_meta, api_key, video_index, total
    )
    
    if result:
        save_analysis_result(video_meta, result, raw_response)
    
    return note_id, result, raw_response


def analyze_all_videos():
    """åˆ†ææ‰€æœ‰è§†é¢‘"""
    log("=" * 60)
    log("ğŸš€ é˜¶æ®µ6-3: AIè§†é¢‘åˆ†æ")
    log("=" * 60)
    
    # åŠ è½½è§†é¢‘åˆ—è¡¨
    videos = load_video_list()
    videos = [v for v in videos if v.get('downloaded') and v.get('file_path')]
    
    # è·å–å·²åˆ†æçš„
    analyzed = get_analyzed_videos()
    
    # ç­›é€‰å¾…åˆ†æçš„
    to_analyze = [v for v in videos if v['note_id'] not in analyzed]
    skipped = len(videos) - len(to_analyze)
    
    log(f"æ€»è§†é¢‘æ•°: {len(videos)}")
    log(f"å·²åˆ†æ: {skipped}")
    log(f"å¾…åˆ†æ: {len(to_analyze)}")
    log(f"API Keys: {len(API_KEYS)}")
    log(f"å¹¶å‘æ•°: {CONFIG['max_workers']}")
    log("")
    
    if not to_analyze:
        log("âœ… æ‰€æœ‰è§†é¢‘å·²åˆ†æå®Œæˆ")
        return
    
    if not API_KEYS:
        log("âŒ æ²¡æœ‰å¯ç”¨çš„API Keyï¼Œè¯·é…ç½®GEMINI_API_KEY2æˆ–GEMINI_API_KEY3")
        return
    
    # åˆ›å»ºKeyæ± 
    key_pool = ApiKeyPool(API_KEYS)
    
    # å‡†å¤‡ä»»åŠ¡
    tasks = [(v, i+1, len(to_analyze), key_pool) for i, v in enumerate(to_analyze)]
    
    # å¹¶è¡Œæ‰§è¡Œ
    results = []
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:
        futures = {executor.submit(analyze_single_task, task): task[0]['note_id'] for task in tasks}
        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                log(f"ä»»åŠ¡å¼‚å¸¸: {e}")
    
    # ç»Ÿè®¡
    elapsed = time.time() - start_time
    success = sum(1 for r in results if r[1] is not None)
    
    log("")
    log("=" * 60)
    log(f"ğŸ“‹ åˆ†æå®Œæˆæ±‡æ€»")
    log("=" * 60)
    log(f"æ€»è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
    log(f"æˆåŠŸ: {success}/{len(to_analyze)}")
    log(f"å¤±è´¥: {len(to_analyze) - success}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary_reports()


def generate_kol_summary(kol_id: str, kol_name: str) -> Optional[str]:
    """ç”Ÿæˆå•ä¸ªKOLçš„æ±‡æ€»æŠ¥å‘Š"""
    kol_result_dir = RESULT_DIR / f"kol_{kol_id}"
    if not kol_result_dir.exists():
        return None
    
    analysis_files = list(kol_result_dir.glob("*_analysis.json"))
    if not analysis_files:
        return None
    
    video_analyses = []
    for f in analysis_files:
        with open(f, 'r', encoding='utf-8') as file:
            video_analyses.append(json.load(file))
    
    # æ”¶é›†æ•°æ®
    scores_d1, scores_d2, scores_d3 = [], [], []
    all_evidence = []
    
    for va in video_analyses:
        meta = va.get('video_metadata', {})
        result = va.get('analysis_result', {})
        
        d1 = result.get('dimension_1_style', {})
        d2 = result.get('dimension_2_editing', {})
        d3 = result.get('dimension_3_speaking', {})
        
        if d1.get('score'): scores_d1.append(d1['score'])
        if d2.get('score'): scores_d2.append(d2['score'])
        if d3.get('score'): scores_d3.append(d3['score'])
        
        video_info = {
            'title': meta.get('title', ''),
            'video_summary': result.get('video_summary', ''),
            'd1_score': d1.get('score', 0),
            'd2_score': d2.get('score', 0),
            'd3_score': d3.get('score', 0),
            'd1_reasoning': d1.get('score_reasoning', 'æ— '),
            'd2_reasoning': d2.get('score_reasoning', 'æ— '),
            'd3_reasoning': d3.get('score_reasoning', 'æ— '),
            'overall': result.get('overall_assessment', {})
        }
        all_evidence.append(video_info)
    
    # è®¡ç®—å¹³å‡åˆ†
    avg_d1 = sum(scores_d1) / len(scores_d1) if scores_d1 else 0
    avg_d2 = sum(scores_d2) / len(scores_d2) if scores_d2 else 0
    avg_d3 = sum(scores_d3) / len(scores_d3) if scores_d3 else 0
    avg_overall = (avg_d1 + avg_d2 + avg_d3) / 3
    
    def get_rating(score):
        if score >= 4.5: return "ä¼˜ç§€ â­â­â­"
        elif score >= 4: return "è‰¯å¥½ â­â­"
        elif score >= 3: return "ä¸€èˆ¬ â­"
        else: return "è¾ƒå¼±"
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# {kol_name} å†…å®¹èƒ½åŠ›ç»¼åˆè¯„ä¼°æŠ¥å‘Š

> **è¯„ä¼°ç›®çš„**ï¼šåˆ¤æ–­è¯¥åšä¸»æ˜¯å¦é€‚åˆè¿›è¡Œ**èƒ½é‡æ£’/å¥åº·é£Ÿå“å“ç‰Œ**çš„æ¨å¹¿åˆä½œ

---

## ä¸€ã€åšä¸»åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | ä¿¡æ¯ |
|------|------|
| åšä¸»åç§° | {kol_name} |
| åˆ†æè§†é¢‘æ•° | {len(video_analyses)} ä¸ª |
| è¯„ä¼°æ—¶é—´ | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |

---

## äºŒã€ä¸‰ç»´åº¦ç»¼åˆè¯„åˆ†

| è¯„ä¼°ç»´åº¦ | å¹³å‡å¾—åˆ† | è¯„çº§ | è¯´æ˜ |
|----------|----------|------|------|
| ç»´åº¦1ï¼šå½¢è±¡ä¸ç”Ÿæ´»æ–¹å¼ | {avg_d1:.1f} åˆ† | {get_rating(avg_d1)} | ä¸å¥åº·/è¿åŠ¨çš„å…³è”åº¦ |
| ç»´åº¦2ï¼šå‰ªè¾‘èƒ½åŠ› | {avg_d2:.1f} åˆ† | {get_rating(avg_d2)} | è§†é¢‘åˆ¶ä½œä¸“ä¸šåº¦ |
| ç»´åº¦3ï¼šå£æ’­èƒ½åŠ› | {avg_d3:.1f} åˆ† | {get_rating(avg_d3)} | äº§å“å–ç‚¹ä¼ è¾¾èƒ½åŠ› |
| **ç»¼åˆè¯„åˆ†** | **{avg_overall:.1f} åˆ†** | **{get_rating(avg_overall)}** | - |

---

## ä¸‰ã€å„è§†é¢‘åˆ†ææ‘˜è¦

"""
    
    for i, ev in enumerate(all_evidence, 1):
        report += f"""### è§†é¢‘{i}: {ev['title']}

**å†…å®¹æ¦‚è¦**: {ev['video_summary']}

**è¯„åˆ†è¯¦æƒ…**:

| ç»´åº¦ | å¾—åˆ† | è¯„åˆ†ç†ç”± |
|------|------|----------|
| å½¢è±¡ç”Ÿæ´»æ–¹å¼ | {ev['d1_score']}åˆ† | {ev['d1_reasoning'][:100]}... |
| å‰ªè¾‘èƒ½åŠ› | {ev['d2_score']}åˆ† | {ev['d2_reasoning'][:100]}... |
| å£æ’­èƒ½åŠ› | {ev['d3_score']}åˆ† | {ev['d3_reasoning'][:100]}... |

---

"""
    
    # ä¼˜åŠ£åŠ¿
    all_strengths, all_weaknesses = [], []
    for ev in all_evidence:
        overall = ev.get('overall', {})
        all_strengths.extend(overall.get('strengths', []))
        all_weaknesses.extend(overall.get('weaknesses', []))
    
    report += """## å››ã€ç»¼åˆè¯„ä¼°

### 4.1 ä¸»è¦ä¼˜åŠ¿

"""
    for s in list(set(all_strengths))[:5]:
        report += f"- {s}\n"
    
    report += "\n### 4.2 ä¸è¶³ä¹‹å¤„\n\n"
    for w in list(set(all_weaknesses))[:5]:
        report += f"- {w}\n"
    
    # æœ€ç»ˆç»“è®º
    if avg_overall >= 4:
        recommendation, fit = "â­â­â­ å¼ºçƒˆæ¨è", "éå¸¸é€‚åˆ"
    elif avg_overall >= 3.5:
        recommendation, fit = "â­â­ æ¨è", "é€‚åˆ"
    elif avg_overall >= 3:
        recommendation, fit = "â­ è°¨æ…", "ä¸€èˆ¬"
    else:
        recommendation, fit = "æš‚ä¸æ¨è", "ä¸å¤ªé€‚åˆ"
    
    report += f"""
---

## äº”ã€æœ€ç»ˆç»“è®º

| é¡¹ç›® | ç»“è®º |
|------|------|
| **æ¨èç­‰çº§** | {recommendation} |
| **èƒ½é‡æ£’æ¨å¹¿é€‚åˆåº¦** | {fit} |
| **å½¢è±¡ç”Ÿæ´»æ–¹å¼** | {avg_d1:.1f}åˆ† |
| **å‰ªè¾‘èƒ½åŠ›** | {avg_d2:.1f}åˆ† |
| **å£æ’­èƒ½åŠ›** | {avg_d3:.1f}åˆ† |
| **ç»¼åˆè¯„åˆ†** | {avg_overall:.1f}åˆ† |

---

*æŠ¥å‘Šç”Ÿæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | æ¨¡å‹: Gemini 2.5 Flash | è§†é¢‘æ•°: {len(video_analyses)}*
"""
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = kol_result_dir / f"{kol_name}_ç»¼åˆè¯„ä¼°æŠ¥å‘Š.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # ä¿å­˜JSONæ±‡æ€»
    summary_json = {
        'kol_id': kol_id,
        'kol_name': kol_name,
        'video_count': len(video_analyses),
        'scores': {
            'd1_avg': round(avg_d1, 1),
            'd2_avg': round(avg_d2, 1),
            'd3_avg': round(avg_d3, 1),
            'overall': round(avg_overall, 1)
        },
        'recommendation': recommendation,
        'energy_bar_fit': fit,
        'generated_at': datetime.now().isoformat()
    }
    
    json_file = kol_result_dir / f"{kol_name}_ç»¼åˆè¯„ä¼°.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(summary_json, f, ensure_ascii=False, indent=2)
    
    log(f"  ğŸ“„ {kol_name}: æŠ¥å‘Šå·²ç”Ÿæˆ")
    return report


def generate_summary_reports():
    """ç”Ÿæˆæ‰€æœ‰KOLçš„æ±‡æ€»æŠ¥å‘Š"""
    log("")
    log("ğŸ“Š ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    
    # åŠ è½½è§†é¢‘åˆ—è¡¨è·å–KOLä¿¡æ¯
    videos = load_video_list()
    kol_map = {}
    for v in videos:
        kol_id = v['kol_id']
        if kol_id not in kol_map:
            kol_map[kol_id] = v['kol_name']
    
    # ä¸ºæ¯ä¸ªKOLç”ŸæˆæŠ¥å‘Š
    all_summaries = []
    for kol_id, kol_name in kol_map.items():
        report = generate_kol_summary(kol_id, kol_name)
        if report:
            # è¯»å–JSONæ±‡æ€»
            json_file = RESULT_DIR / f"kol_{kol_id}" / f"{kol_name}_ç»¼åˆè¯„ä¼°.json"
            if json_file.exists():
                with open(json_file, 'r', encoding='utf-8') as f:
                    all_summaries.append(json.load(f))
    
    # ç”Ÿæˆæ¨ªå‘æ¯”è¾ƒæŠ¥å‘Š
    if all_summaries:
        generate_comparison_report(all_summaries)


def generate_comparison_report(summaries: List[Dict]):
    """ç”ŸæˆKOLæ¨ªå‘æ¯”è¾ƒæŠ¥å‘Š"""
    # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
    summaries.sort(key=lambda x: x['scores']['overall'], reverse=True)
    
    report = f"""# èƒ½é‡æ£’KOLæ¨ªå‘æ¯”è¾ƒæŠ¥å‘Š

> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
> **KOLæ•°é‡**: {len(summaries)}

---

## ä¸€ã€ç»¼åˆæ’å

| æ’å | KOLåç§° | ç»¼åˆè¯„åˆ† | å½¢è±¡ç”Ÿæ´» | å‰ªè¾‘èƒ½åŠ› | å£æ’­èƒ½åŠ› | æ¨èç­‰çº§ | èƒ½é‡æ£’é€‚åˆåº¦ |
|------|---------|----------|----------|----------|----------|----------|--------------|
"""
    
    for i, s in enumerate(summaries, 1):
        report += f"| {i} | {s['kol_name']} | {s['scores']['overall']:.1f} | {s['scores']['d1_avg']:.1f} | {s['scores']['d2_avg']:.1f} | {s['scores']['d3_avg']:.1f} | {s['recommendation']} | {s['energy_bar_fit']} |\n"
    
    report += """
---

## äºŒã€å„ç»´åº¦å¯¹æ¯”

### 2.1 å½¢è±¡ä¸ç”Ÿæ´»æ–¹å¼ï¼ˆä¸å¥åº·/è¿åŠ¨å…³è”åº¦ï¼‰

"""
    for s in summaries:
        report += f"- **{s['kol_name']}**: {s['scores']['d1_avg']:.1f}åˆ†\n"
    
    report += """
### 2.2 å‰ªè¾‘èƒ½åŠ›

"""
    for s in summaries:
        report += f"- **{s['kol_name']}**: {s['scores']['d2_avg']:.1f}åˆ†\n"
    
    report += """
### 2.3 å£æ’­èƒ½åŠ›

"""
    for s in summaries:
        report += f"- **{s['kol_name']}**: {s['scores']['d3_avg']:.1f}åˆ†\n"
    
    report += f"""
---

## ä¸‰ã€æ¨èç»“è®º

### å¼ºçƒˆæ¨è â­â­â­

"""
    strong = [s for s in summaries if 'å¼ºçƒˆæ¨è' in s['recommendation']]
    if strong:
        for s in strong:
            report += f"- **{s['kol_name']}** (ç»¼åˆ{s['scores']['overall']:.1f}åˆ†)\n"
    else:
        report += "æ— \n"
    
    report += """
### æ¨è â­â­

"""
    recommend = [s for s in summaries if s['recommendation'] == 'â­â­ æ¨è']
    if recommend:
        for s in recommend:
            report += f"- **{s['kol_name']}** (ç»¼åˆ{s['scores']['overall']:.1f}åˆ†)\n"
    else:
        report += "æ— \n"
    
    report += """
### è°¨æ… / ä¸æ¨è

"""
    others = [s for s in summaries if 'è°¨æ…' in s['recommendation'] or 'ä¸æ¨è' in s['recommendation']]
    if others:
        for s in others:
            report += f"- **{s['kol_name']}** ({s['recommendation']})\n"
    else:
        report += "æ— \n"
    
    report += f"""
---

*æŠ¥å‘Šç”Ÿæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = RESULT_DIR / "KOLæ¨ªå‘æ¯”è¾ƒæŠ¥å‘Š.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # ä¿å­˜JSON
    json_file = RESULT_DIR / "KOLæ¨ªå‘æ¯”è¾ƒæ•°æ®.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'generated_at': datetime.now().isoformat(),
            'summaries': summaries
        }, f, ensure_ascii=False, indent=2)
    
    log(f"  ğŸ“„ æ¨ªå‘æ¯”è¾ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='AIè§†é¢‘åˆ†æ')
    parser.add_argument('--report-only', action='store_true', help='ä»…ç”ŸæˆæŠ¥å‘Šï¼Œä¸åˆ†æ')
    args = parser.parse_args()
    
    if args.report_only:
        generate_summary_reports()
    else:
        analyze_all_videos()


if __name__ == "__main__":
    main()
