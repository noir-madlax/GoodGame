#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å°†çˆ¬å–çš„APIæ•°æ®å¯¼å…¥æ•°æ®åº“

åŠŸèƒ½ï¼š
1. è¯»å– output/api_data/kol_xxx/all_data.json æ–‡ä»¶
2. å°†10ä¸ªAPIçš„æ•°æ®å¯¼å…¥åˆ°å¯¹åº”çš„8ä¸ªæ•°æ®åº“è¡¨
3. æ”¯æŒå¹¶å‘å¯¼å…¥ï¼ˆé»˜è®¤10å¹¶å‘ï¼‰
4. ä½¿ç”¨upserté¿å…é‡å¤
5. å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š3æ¬¡ï¼‰
6. å®æ—¶è¿›åº¦æ˜¾ç¤º
7. æ•°æ®éªŒè¯å’Œå®Œæ•´æ€§æ£€æŸ¥
8. æœ€ç»ˆæ•°æ®æŠ¥å‘Šï¼ˆå­—æ®µç©ºå€¼æ¯”ä¾‹ç»Ÿè®¡ï¼‰
"""

import os
import json
import time
import sys
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import threading

# é…ç½®æ—¥å¿— - å®æ—¶è¾“å‡º
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ç¡®ä¿å®æ—¶è¾“å‡º
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(line_buffering=True)


@dataclass
class ImportStats:
    """å¯¼å…¥ç»Ÿè®¡"""
    total_kols: int = 0
    imported_kols: int = 0
    failed_kols: int = 0
    skipped_kols: int = 0
    retried_kols: int = 0
    base_info_updated: int = 0
    audience_inserted: int = 0
    fans_summary_inserted: int = 0
    fans_trend_inserted: int = 0
    note_rate_inserted: int = 0
    notes_inserted: int = 0
    cost_effective_inserted: int = 0
    core_data_inserted: int = 0
    errors: List[str] = field(default_factory=list)
    failed_kol_ids: List[str] = field(default_factory=list)
    _lock: threading.Lock = field(default_factory=threading.Lock)
    
    def increment(self, field_name: str, value: int = 1):
        """çº¿ç¨‹å®‰å…¨çš„å¢é‡æ“ä½œ"""
        with self._lock:
            current = getattr(self, field_name)
            setattr(self, field_name, current + value)
    
    def add_error(self, error: str):
        """çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ é”™è¯¯"""
        with self._lock:
            self.errors.append(error)
    
    def add_failed_kol(self, kol_id: str):
        """çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ å¤±è´¥çš„KOL ID"""
        with self._lock:
            self.failed_kol_ids.append(kol_id)


class ApiDataImporter:
    """APIæ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self, concurrency: int = 10, max_retries: int = 3):
        self.concurrency = concurrency
        self.max_retries = max_retries
        self.data_dir = Path(__file__).parent / "output" / "api_data"
        self.stats = ImportStats()
        self.start_time = None
        self.processed_count = 0
        self._count_lock = threading.Lock()
        
        # æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„å®¢æˆ·ç«¯
        self._client_local = threading.local()
    
    def _get_client(self):
        """è·å–çº¿ç¨‹æœ¬åœ°çš„Supabaseå®¢æˆ·ç«¯"""
        if not hasattr(self._client_local, 'client'):
            from supabase import create_client
            
            backend_dir = Path(__file__).parent.parent.parent.parent
            env_path = backend_dir / '.env'
            
            if env_path.exists():
                load_dotenv(env_path)
            
            url = os.getenv('SUPABASE_URL')
            key = os.getenv('SUPABASE_KEY')
            
            if not url or not key:
                raise ValueError("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® SUPABASE_URL å’Œ SUPABASE_KEY")
            
            self._client_local.client = create_client(url, key)
        
        return self._client_local.client
    
    def _safe_float(self, value: Any, default: float = None) -> Optional[float]:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        if value is None:
            return default
        try:
            return float(value)
        except (ValueError, TypeError):
            return default
    
    def _safe_int(self, value: Any, default: int = None) -> Optional[int]:
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
        if value is None:
            return default
        try:
            return int(value)
        except (ValueError, TypeError):
            return default
    
    def _parse_date(self, date_str: str) -> Optional[str]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None
        try:
            for fmt in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d %H:%M:%S']:
                try:
                    return datetime.strptime(date_str[:19], fmt).strftime('%Y-%m-%d')
                except ValueError:
                    continue
            return date_str[:10] if len(date_str) >= 10 else None
        except Exception:
            return None
    
    def _extract_base_info(self, kol_id: str, apis: Dict[str, Any]) -> Dict[str, Any]:
        """ä»kol_infoæå–åŸºç¡€ä¿¡æ¯æ›´æ–°"""
        kol_info = apis.get('kol_info', {})
        if kol_info.get('code') != 0:
            return None
        
        data = kol_info.get('data', {})
        if not data:
            return None
        
        summary_v1 = apis.get('kol_data_summary_v1', {}).get('data', {}) or {}
        summary_v2 = apis.get('kol_data_summary_v2', {}).get('data', {}) or {}
        
        raw_data = {
            'kol_info': kol_info,
            'kol_data_summary_v1': apis.get('kol_data_summary_v1', {}),
            'kol_data_summary_v2': apis.get('kol_data_summary_v2', {})
        }
        
        return {
            'kol_id': kol_id,
            'kol_name': data.get('name'),
            'red_id': data.get('redId'),
            'gender': data.get('gender'),
            'location': data.get('location'),
            'travel_area_list': data.get('travelAreaList'),
            'head_photo': data.get('headPhoto'),
            'fans_count': self._safe_int(data.get('fansCount')),
            'like_collect_count': self._safe_int(data.get('likeCollectCountInfo')),
            'business_note_count': self._safe_int(data.get('businessNoteCount')),
            'total_note_count': self._safe_int(data.get('totalNoteCount')),
            'picture_price': self._safe_float(data.get('picturePrice')),
            'video_price': self._safe_float(data.get('videoPrice')),
            'picture_state': self._safe_int(data.get('pictureState')),
            'video_state': self._safe_int(data.get('videoState')),
            'lower_price': self._safe_float(data.get('lowerPrice')),
            'content_tags': data.get('contentTags'),
            'feature_tags': data.get('featureTags'),
            'personal_tags': data.get('personalTags'),
            'trade_type': data.get('tradeType'),
            'click_mid_num': self._safe_int(data.get('clickMidNum')),
            'inter_mid_num': self._safe_int(data.get('interMidNum')),
            'fans_30_growth_num': self._safe_int(data.get('fans30GrowthNum')),
            'fans_30_growth_rate': self._safe_float(data.get('fans30GrowthRate')),
            'current_level': self._safe_int(data.get('currentLevel')),
            'cooper_type': self._safe_int(data.get('cooperType')),
            'user_type': self._safe_int(data.get('userType')),
            'low_active': data.get('lowActive'),
            'kol_advantage': summary_v2.get('kolAdvantage'),
            'cooperate_state': self._safe_int(data.get('cooperateState')),
            'estimate_picture_cpm': self._safe_float(data.get('estimatePictureCpm')),
            'estimate_video_cpm': self._safe_float(data.get('estimateVideoCpm')),
            'estimate_picture_engage_cost': self._safe_float(data.get('estimatePictureEngageCost')),
            'estimate_video_engage_cost': self._safe_float(data.get('estimateVideoEngageCost')),
            'response_rate': self._safe_float(summary_v1.get('responseRate')),
            'invite_num': self._safe_int(summary_v1.get('inviteNum')),
            'active_day_in_last_7': self._safe_int(summary_v1.get('activeDayInLast7')),
            'is_active': summary_v1.get('isActive'),
            'easy_connect': summary_v1.get('easyConnect'),
            'raw_data': raw_data,
            'api_fetch_status': 'fetched',
            'fetch_date': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
    
    def _extract_audience(self, kol_id: str, apis: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä»kol_fans_portraitæå–ç²‰ä¸ç”»åƒ"""
        portrait = apis.get('kol_fans_portrait', {})
        if portrait.get('code') != 0:
            return None
        
        data = portrait.get('data', {})
        if not data:
            return None
        
        return {
            'kol_id': kol_id,
            'gender_distribution': data.get('gender'),
            'age_distribution': data.get('ages'),
            'province_distribution': data.get('provinces'),
            'city_distribution': data.get('cities'),
            'device_distribution': data.get('devices'),
            'interest_distribution': data.get('interests'),
            'date_key': self._parse_date(data.get('dateKey')),
            'raw_data': portrait,
            'fetch_date': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
    
    def _extract_fans_summary(self, kol_id: str, apis: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä»kol_fans_summaryæå–ç²‰ä¸è´¨é‡"""
        summary = apis.get('kol_fans_summary', {})
        if summary.get('code') != 0:
            return None
        
        data = summary.get('data', {})
        if not data:
            return None
        
        return {
            'kol_id': kol_id,
            'fans_num': self._safe_int(data.get('fansNum')),
            'fans_increase_num': self._safe_int(data.get('fansIncreaseNum')),
            'fans_growth_rate': self._safe_float(data.get('fansGrowthRate')),
            'fans_growth_beyond_rate': self._safe_float(data.get('fansGrowthBeyondRate')),
            'active_fans_l28': self._safe_int(data.get('activeFansL28')),
            'active_fans_rate': self._safe_float(data.get('activeFansRate')),
            'active_fans_beyond_rate': self._safe_float(data.get('activeFansBeyondRate')),
            'engage_fans_l30': self._safe_int(data.get('engageFansL30')),
            'engage_fans_rate': self._safe_float(data.get('engageFansRate')),
            'engage_fans_beyond_rate': self._safe_float(data.get('engageFansBeyondRate')),
            'read_fans_in_30': self._safe_int(data.get('readFansIn30')),
            'read_fans_rate': self._safe_float(data.get('readFansRate')),
            'read_fans_beyond_rate': self._safe_float(data.get('readFansBeyondRate')),
            'pay_fans_user_rate_30d': self._safe_float(data.get('payFansUserRate30d')),
            'pay_fans_user_num_30d': self._safe_int(data.get('payFansUserNum30d')),
            'raw_data': summary,
            'fetch_date': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
    
    def _extract_fans_trend(self, kol_id: str, apis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»kol_fans_trendæå–ç²‰ä¸è¶‹åŠ¿"""
        trend = apis.get('kol_fans_trend', {})
        if trend.get('code') != 0:
            return []
        
        data = trend.get('data', {}) or {}
        trend_list = data.get('list', []) or []
        
        records = []
        for item in trend_list:
            date_key = self._parse_date(item.get('dateKey'))
            if date_key:
                records.append({
                    'kol_id': kol_id,
                    'date_key': date_key,
                    'fans_num': self._safe_int(item.get('num'))
                })
        
        return records
    
    def _extract_note_rate(self, kol_id: str, apis: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä»kol_note_rateæå–ç¬”è®°æ•°æ®ç‡"""
        note_rate = apis.get('kol_note_rate', {})
        if note_rate.get('code') != 0:
            return None
        
        data = note_rate.get('data', {})
        if not data:
            return None
        
        return {
            'kol_id': kol_id,
            'note_number': self._safe_int(data.get('noteNumber')),
            'video_note_number': self._safe_int(data.get('videoNoteNumber')),
            'imp_median': self._safe_int(data.get('impMedian')),
            'imp_median_beyond_rate': self._safe_float(data.get('impMedianBeyondRate')),
            'read_median': self._safe_int(data.get('readMedian')),
            'read_median_beyond_rate': self._safe_float(data.get('readMedianBeyondRate')),
            'interaction_median': self._safe_int(data.get('interactionMedian')),
            'interaction_rate': self._safe_float(data.get('interactionRate')),
            'interaction_beyond_rate': self._safe_float(data.get('interactionBeyondRate')),
            'like_median': self._safe_int(data.get('likeMedian')),
            'collect_median': self._safe_int(data.get('collectMedian')),
            'comment_median': self._safe_int(data.get('commentMedian')),
            'share_median': self._safe_int(data.get('shareMedian')),
            'hundred_like_percent': self._safe_float(data.get('hundredLikePercent')),
            'thousand_like_percent': self._safe_float(data.get('thousandLikePercent')),
            'page_percent_vo': data.get('pagePercentVo'),
            'long_term_common_note_vo': data.get('longTermCommonNoteVo'),
            'long_term_cooperate_note_vo': data.get('longTermCooperateNoteVo'),
            'note_type': data.get('noteType'),
            'raw_data': note_rate,
            'fetch_date': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
    
    def _extract_notes(self, kol_id: str, apis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»kol_note_listæå–ç¬”è®°åˆ—è¡¨"""
        note_list = apis.get('kol_note_list', {})
        if note_list.get('code') != 0:
            return []
        
        data = note_list.get('data', {}) or {}
        notes = data.get('list', []) or []
        
        records = []
        for note in notes:
            note_id = note.get('noteId')
            if note_id:
                records.append({
                    'kol_id': kol_id,
                    'note_id': note_id,
                    'title': note.get('title'),
                    'img_url': note.get('imgUrl'),
                    'is_video': note.get('isVideo'),
                    'is_advertise': note.get('isAdvertise'),
                    'brand_name': note.get('brandName'),
                    'read_num': self._safe_int(note.get('readNum')),
                    'like_num': self._safe_int(note.get('likeNum')),
                    'collect_num': self._safe_int(note.get('collectNum')),
                    'third_read_user_num': self._safe_int(note.get('thirdReadUserNum')),
                    'publish_date': self._parse_date(note.get('date')),
                    'raw_data': note,
                    'detail_fetched': False,
                    'updated_at': datetime.now().isoformat()
                })
        
        return records
    
    def _extract_cost_effective(self, kol_id: str, apis: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä»kol_cost_effectiveæå–æ€§ä»·æ¯”"""
        cost = apis.get('kol_cost_effective', {})
        if cost.get('code') != 0:
            return None
        
        data = cost.get('data', {})
        if not data:
            return None
        
        return {
            'kol_id': kol_id,
            'picture_read_cost': self._safe_float(data.get('pictureReadCost')),
            'picture_surpass_rate': self._safe_float(data.get('pictureSurpassRate')),
            'picture_case': self._safe_int(data.get('pictureCase')),
            'estimate_picture_cpm': self._safe_float(data.get('estimatePictureCpm')),
            'estimate_picture_cpm_compare': self._safe_float(data.get('estimatePictureCpmCompare')),
            'estimate_picture_engage_cost': self._safe_float(data.get('estimatePictureEngageCost')),
            'estimate_picture_engage_cost_compare': self._safe_float(data.get('estimatePictureEngageCostCompare')),
            'video_read_cost': self._safe_float(data.get('videoReadCost')),
            'video_surpass_rate': self._safe_float(data.get('videoSurpassRate')),
            'video_case': self._safe_int(data.get('videoCase')),
            'estimate_video_cpm': self._safe_float(data.get('estimateVideoCpm')),
            'estimate_video_cpm_compare': self._safe_float(data.get('estimateVideoCpmCompare')),
            'estimate_video_engage_cost': self._safe_float(data.get('estimateVideoEngageCost')),
            'estimate_video_engage_cost_compare': self._safe_float(data.get('estimateVideoEngageCostCompare')),
            'raw_data': cost,
            'fetch_date': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
    
    def _extract_core_data(self, kol_id: str, apis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»kol_core_dataæå–æ ¸å¿ƒæ•°æ®"""
        core = apis.get('kol_core_data', {})
        if core.get('code') != 0:
            return []
        
        data = core.get('data', {}) or {}
        daily_data = data.get('dailyData', []) or []
        
        records = []
        for item in daily_data:
            date_key = self._parse_date(item.get('dateKey'))
            if date_key:
                records.append({
                    'kol_id': kol_id,
                    'date_key': date_key,
                    'imp': self._safe_int(item.get('imp')),
                    'read': self._safe_int(item.get('read')),
                    'engage': self._safe_int(item.get('engage')),
                    'third_user_num': self._safe_int(item.get('thirdUserNum')),
                    'cpm': self._safe_float(item.get('cpm')),
                    'cpe': self._safe_float(item.get('cpe')),
                    'cpuv': self._safe_float(item.get('cpuv')),
                    'cpv': self._safe_float(item.get('cpv'))
                })
        
        return records
    
    def _import_single_kol_internal(self, kol_data: Dict[str, Any]) -> Tuple[bool, str, Dict[str, int]]:
        """å¯¼å…¥å•ä¸ªKOLçš„æ•°æ®ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        kol_id = kol_data.get('kol_id')
        kol_name = kol_data.get('kol_name', 'Unknown')
        apis = kol_data.get('apis', {})
        
        client = self._get_client()
        local_stats = {
            'base_info': 0, 'audience': 0, 'fans_summary': 0,
            'fans_trend': 0, 'note_rate': 0, 'notes': 0,
            'cost_effective': 0, 'core_data': 0
        }
        
        # 1. æ›´æ–°åŸºç¡€ä¿¡æ¯
        base_info = self._extract_base_info(kol_id, apis)
        if base_info:
            client.table('gg_pgy_kol_base_info').upsert(base_info, on_conflict='kol_id').execute()
            local_stats['base_info'] = 1
        
        # 2. æ’å…¥ç²‰ä¸ç”»åƒ
        audience = self._extract_audience(kol_id, apis)
        if audience:
            client.table('gg_pgy_kol_audience').delete().eq('kol_id', kol_id).execute()
            client.table('gg_pgy_kol_audience').insert(audience).execute()
            local_stats['audience'] = 1
        
        # 3. æ’å…¥ç²‰ä¸è´¨é‡
        fans_summary = self._extract_fans_summary(kol_id, apis)
        if fans_summary:
            client.table('gg_pgy_kol_fans_summary').delete().eq('kol_id', kol_id).execute()
            client.table('gg_pgy_kol_fans_summary').insert(fans_summary).execute()
            local_stats['fans_summary'] = 1
        
        # 4. æ’å…¥ç²‰ä¸è¶‹åŠ¿
        fans_trend = self._extract_fans_trend(kol_id, apis)
        if fans_trend:
            client.table('gg_pgy_kol_fans_trend').delete().eq('kol_id', kol_id).execute()
            batch_size = 50
            for i in range(0, len(fans_trend), batch_size):
                batch = fans_trend[i:i+batch_size]
                client.table('gg_pgy_kol_fans_trend').insert(batch).execute()
            local_stats['fans_trend'] = len(fans_trend)
        
        # 5. æ’å…¥ç¬”è®°æ•°æ®ç‡
        note_rate = self._extract_note_rate(kol_id, apis)
        if note_rate:
            client.table('gg_pgy_kol_note_rate').delete().eq('kol_id', kol_id).execute()
            client.table('gg_pgy_kol_note_rate').insert(note_rate).execute()
            local_stats['note_rate'] = 1
        
        # 6. æ’å…¥ç¬”è®°åˆ—è¡¨
        notes = self._extract_notes(kol_id, apis)
        if notes:
            client.table('gg_pgy_kol_notes').delete().eq('kol_id', kol_id).execute()
            batch_size = 50
            for i in range(0, len(notes), batch_size):
                batch = notes[i:i+batch_size]
                client.table('gg_pgy_kol_notes').insert(batch).execute()
            local_stats['notes'] = len(notes)
        
        # 7. æ’å…¥æ€§ä»·æ¯”
        cost_effective = self._extract_cost_effective(kol_id, apis)
        if cost_effective:
            client.table('gg_pgy_kol_cost_effective').delete().eq('kol_id', kol_id).execute()
            client.table('gg_pgy_kol_cost_effective').insert(cost_effective).execute()
            local_stats['cost_effective'] = 1
        
        # 8. æ’å…¥æ ¸å¿ƒæ•°æ®
        core_data = self._extract_core_data(kol_id, apis)
        if core_data:
            client.table('gg_pgy_kol_core_data').delete().eq('kol_id', kol_id).execute()
            batch_size = 50
            for i in range(0, len(core_data), batch_size):
                batch = core_data[i:i+batch_size]
                client.table('gg_pgy_kol_core_data').insert(batch).execute()
            local_stats['core_data'] = len(core_data)
        
        return True, f"æˆåŠŸå¯¼å…¥ {kol_name} ({kol_id})", local_stats
    
    def import_single_kol_with_retry(self, kol_dir: Path, index: int, total: int) -> Tuple[bool, str]:
        """å¯¼å…¥å•ä¸ªKOLï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        kol_data = self.load_kol_data(kol_dir)
        if not kol_data:
            with self._count_lock:
                self.processed_count += 1
            self.stats.increment('skipped_kols')
            logger.warning(f"[{index}/{total}] âš ï¸ è·³è¿‡æ— æ•ˆç›®å½•: {kol_dir.name}")
            return True, "è·³è¿‡"
        
        kol_id = kol_data.get('kol_id')
        kol_name = kol_data.get('kol_name', 'Unknown')
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰APIéƒ½æ˜¯skipped
        apis = kol_data.get('apis', {})
        all_skipped = all(
            api_data.get('skipped', False) or api_data.get('code') != 0
            for api_data in apis.values()
        )
        if all_skipped:
            with self._count_lock:
                self.processed_count += 1
            self.stats.increment('skipped_kols')
            logger.info(f"[{index}/{total}] â­ï¸ è·³è¿‡(æ— æœ‰æ•ˆæ•°æ®): {kol_name} ({kol_id})")
            return True, "è·³è¿‡"
        
        last_error = None
        for attempt in range(self.max_retries):
            try:
                success, msg, local_stats = self._import_single_kol_internal(kol_data)
                
                if success:
                    # æ›´æ–°å…¨å±€ç»Ÿè®¡
                    self.stats.increment('base_info_updated', local_stats['base_info'])
                    self.stats.increment('audience_inserted', local_stats['audience'])
                    self.stats.increment('fans_summary_inserted', local_stats['fans_summary'])
                    self.stats.increment('fans_trend_inserted', local_stats['fans_trend'])
                    self.stats.increment('note_rate_inserted', local_stats['note_rate'])
                    self.stats.increment('notes_inserted', local_stats['notes'])
                    self.stats.increment('cost_effective_inserted', local_stats['cost_effective'])
                    self.stats.increment('core_data_inserted', local_stats['core_data'])
                    self.stats.increment('imported_kols')
                    
                    if attempt > 0:
                        self.stats.increment('retried_kols')
                        logger.info(f"[{index}/{total}] âœ… é‡è¯•æˆåŠŸ(ç¬¬{attempt+1}æ¬¡): {msg}")
                    else:
                        logger.info(f"[{index}/{total}] âœ… {msg}")
                    
                    with self._count_lock:
                        self.processed_count += 1
                        elapsed = time.time() - self.start_time
                        avg_time = elapsed / self.processed_count
                        remaining = (total - self.processed_count) * avg_time
                        logger.info(f"    ğŸ“Š è¿›åº¦: {self.processed_count}/{total} ({100*self.processed_count/total:.1f}%) | é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")
                    
                    return True, msg
                
            except Exception as e:
                last_error = str(e)
                if attempt < self.max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    logger.warning(f"[{index}/{total}] âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥: {kol_name} ({kol_id}), é”™è¯¯: {last_error}, {wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        error_msg = f"å¯¼å…¥ {kol_name} ({kol_id}) å¤±è´¥(é‡è¯•{self.max_retries}æ¬¡): {last_error}"
        self.stats.add_error(error_msg)
        self.stats.add_failed_kol(kol_id)
        self.stats.increment('failed_kols')
        logger.error(f"[{index}/{total}] âŒ {error_msg}")
        
        with self._count_lock:
            self.processed_count += 1
        
        return False, error_msg
    
    def load_kol_data(self, kol_dir: Path) -> Optional[Dict[str, Any]]:
        """åŠ è½½å•ä¸ªKOLçš„æ•°æ®æ–‡ä»¶"""
        data_file = kol_dir / "all_data.json"
        if not data_file.exists():
            return None
        
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {data_file}: {e}")
            return None
    
    def get_all_kol_dirs(self) -> List[Path]:
        """è·å–æ‰€æœ‰KOLæ•°æ®ç›®å½•"""
        if not self.data_dir.exists():
            return []
        
        kol_dirs = []
        for item in self.data_dir.iterdir():
            if item.is_dir() and item.name.startswith('kol_'):
                kol_dirs.append(item)
        
        return sorted(kol_dirs, key=lambda x: x.name)
    
    def import_kols_concurrent(self, kol_dirs: List[Path], limit: int = None):
        """å¹¶å‘å¯¼å…¥KOLæ•°æ®"""
        if limit:
            kol_dirs = kol_dirs[:limit]
        
        total = len(kol_dirs)
        self.stats.total_kols = total
        self.start_time = time.time()
        self.processed_count = 0
        
        logger.info("=" * 70)
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘å¯¼å…¥ {total} ä¸ªKOLçš„æ•°æ®")
        logger.info(f"   å¹¶å‘æ•°: {self.concurrency}, æœ€å¤§é‡è¯•æ¬¡æ•°: {self.max_retries}")
        logger.info("=" * 70)
        
        with ThreadPoolExecutor(max_workers=self.concurrency) as executor:
            futures = {
                executor.submit(self.import_single_kol_with_retry, kol_dir, i+1, total): kol_dir
                for i, kol_dir in enumerate(kol_dirs)
            }
            
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    kol_dir = futures[future]
                    logger.error(f"âŒ æœªé¢„æœŸçš„é”™è¯¯ {kol_dir.name}: {e}")
        
        elapsed = time.time() - self.start_time
        self._print_summary(elapsed)
        
        return self.stats
    
    def _print_summary(self, elapsed: float):
        """æ‰“å°å¯¼å…¥æ±‡æ€»"""
        logger.info("")
        logger.info("=" * 70)
        logger.info("ğŸ“‹ å¯¼å…¥å®Œæˆæ±‡æ€»")
        logger.info("=" * 70)
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {elapsed/60:.2f} åˆ†é’Ÿ")
        logger.info(f"ğŸ“Š æ€»KOLæ•°: {self.stats.total_kols}")
        logger.info(f"âœ… æˆåŠŸå¯¼å…¥: {self.stats.imported_kols}")
        logger.info(f"â­ï¸  è·³è¿‡(æ— æ•°æ®): {self.stats.skipped_kols}")
        logger.info(f"ğŸ”„ é‡è¯•æˆåŠŸ: {self.stats.retried_kols}")
        logger.info(f"âŒ å¤±è´¥: {self.stats.failed_kols}")
        logger.info("-" * 50)
        logger.info(f"ğŸ“ åŸºç¡€ä¿¡æ¯æ›´æ–°: {self.stats.base_info_updated}")
        logger.info(f"ğŸ‘¥ ç²‰ä¸ç”»åƒæ’å…¥: {self.stats.audience_inserted}")
        logger.info(f"ğŸ“ˆ ç²‰ä¸è´¨é‡æ’å…¥: {self.stats.fans_summary_inserted}")
        logger.info(f"ğŸ“‰ ç²‰ä¸è¶‹åŠ¿æ’å…¥: {self.stats.fans_trend_inserted} æ¡")
        logger.info(f"ğŸ“° ç¬”è®°æ•°æ®ç‡æ’å…¥: {self.stats.note_rate_inserted}")
        logger.info(f"ğŸ“„ ç¬”è®°åˆ—è¡¨æ’å…¥: {self.stats.notes_inserted} æ¡")
        logger.info(f"ğŸ’° æ€§ä»·æ¯”æ’å…¥: {self.stats.cost_effective_inserted}")
        logger.info(f"ğŸ“Š æ ¸å¿ƒæ•°æ®æ’å…¥: {self.stats.core_data_inserted} æ¡")
        
        if self.stats.errors:
            logger.info("-" * 50)
            logger.info(f"âŒ é”™è¯¯åˆ—è¡¨ ({len(self.stats.errors)}):")
            for err in self.stats.errors[:20]:
                logger.info(f"  - {err}")
            if len(self.stats.errors) > 20:
                logger.info(f"  ... è¿˜æœ‰ {len(self.stats.errors) - 20} ä¸ªé”™è¯¯")


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    def __init__(self):
        self.client = self._load_supabase_client()
        self.data_dir = Path(__file__).parent / "output" / "api_data"
    
    def _load_supabase_client(self):
        """åŠ è½½Supabaseå®¢æˆ·ç«¯"""
        from supabase import create_client
        
        backend_dir = Path(__file__).parent.parent.parent.parent
        env_path = backend_dir / '.env'
        
        if env_path.exists():
            load_dotenv(env_path)
        
        url = os.getenv('SUPABASE_URL')
        key = os.getenv('SUPABASE_KEY')
        
        return create_client(url, key)
    
    def validate_kol(self, kol_id: str) -> Dict[str, Any]:
        """éªŒè¯å•ä¸ªKOLçš„æ•°æ®å®Œæ•´æ€§"""
        results = {
            'kol_id': kol_id,
            'valid': True,
            'checks': {}
        }
        
        kol_dir = self.data_dir / f"kol_{kol_id}"
        json_file = kol_dir / "all_data.json"
        
        if not json_file.exists():
            results['valid'] = False
            results['error'] = f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file}"
            return results
        
        with open(json_file, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        apis = json_data.get('apis', {})
        
        checks = {
            'base_info': self._check_base_info(kol_id, apis),
            'audience': self._check_audience(kol_id, apis),
            'fans_summary': self._check_fans_summary(kol_id, apis),
            'fans_trend': self._check_fans_trend(kol_id, apis),
            'note_rate': self._check_note_rate(kol_id, apis),
            'notes': self._check_notes(kol_id, apis),
            'cost_effective': self._check_cost_effective(kol_id, apis),
            'core_data': self._check_core_data(kol_id, apis)
        }
        
        results['checks'] = checks
        results['valid'] = all(c['valid'] for c in checks.values())
        
        return results
    
    def _check_base_info(self, kol_id: str, apis: Dict) -> Dict:
        db_data = self.client.table('gg_pgy_kol_base_info').select('*').eq('kol_id', kol_id).execute()
        
        if not db_data.data:
            api_code = apis.get('kol_info', {}).get('code')
            if api_code != 0:
                return {'valid': True, 'note': 'APIæ— æœ‰æ•ˆæ•°æ®'}
            return {'valid': False, 'error': 'æ•°æ®åº“ä¸­æ— è®°å½•'}
        
        db_row = db_data.data[0]
        api_data = apis.get('kol_info', {}).get('data', {})
        
        if not api_data:
            return {'valid': True, 'note': 'APIæ— æ•°æ®ï¼Œè·³è¿‡'}
        
        mismatches = []
        field_mappings = {
            'kol_name': 'name',
            'fans_count': 'fansCount',
            'video_price': 'videoPrice'
        }
        
        for db_field, api_field in field_mappings.items():
            db_val = db_row.get(db_field)
            api_val = api_data.get(api_field)
            
            if db_val is not None and api_val is not None:
                if isinstance(api_val, (int, float)):
                    if abs(float(db_val or 0) - float(api_val or 0)) > 0.01:
                        mismatches.append(f"{db_field}: DB={db_val}, API={api_val}")
                elif str(db_val) != str(api_val):
                    mismatches.append(f"{db_field}: DB={db_val}, API={api_val}")
        
        return {
            'valid': len(mismatches) == 0,
            'mismatches': mismatches if mismatches else None
        }
    
    def _check_audience(self, kol_id: str, apis: Dict) -> Dict:
        db_data = self.client.table('gg_pgy_kol_audience').select('*').eq('kol_id', kol_id).execute()
        api_code = apis.get('kol_fans_portrait', {}).get('code')
        
        if api_code != 0:
            return {'valid': True, 'note': 'APIè¿”å›é0ï¼Œè·³è¿‡'}
        
        if not db_data.data:
            return {'valid': False, 'error': 'æ•°æ®åº“ä¸­æ— è®°å½•'}
        
        return {'valid': True, 'count': len(db_data.data)}
    
    def _check_fans_summary(self, kol_id: str, apis: Dict) -> Dict:
        db_data = self.client.table('gg_pgy_kol_fans_summary').select('*').eq('kol_id', kol_id).execute()
        api_code = apis.get('kol_fans_summary', {}).get('code')
        
        if api_code != 0:
            return {'valid': True, 'note': 'APIè¿”å›é0ï¼Œè·³è¿‡'}
        
        if not db_data.data:
            return {'valid': False, 'error': 'æ•°æ®åº“ä¸­æ— è®°å½•'}
        
        return {'valid': True, 'count': len(db_data.data)}
    
    def _check_fans_trend(self, kol_id: str, apis: Dict) -> Dict:
        db_data = self.client.table('gg_pgy_kol_fans_trend').select('*').eq('kol_id', kol_id).execute()
        api_data = apis.get('kol_fans_trend', {}) or {}
        data = api_data.get('data', {}) or {}
        api_list = data.get('list', []) or []
        
        db_count = len(db_data.data)
        api_count = len(api_list)
        
        return {
            'valid': db_count == api_count,
            'db_count': db_count,
            'api_count': api_count
        }
    
    def _check_note_rate(self, kol_id: str, apis: Dict) -> Dict:
        db_data = self.client.table('gg_pgy_kol_note_rate').select('*').eq('kol_id', kol_id).execute()
        api_code = apis.get('kol_note_rate', {}).get('code')
        
        if api_code != 0:
            return {'valid': True, 'note': 'APIè¿”å›é0ï¼Œè·³è¿‡'}
        
        if not db_data.data:
            return {'valid': False, 'error': 'æ•°æ®åº“ä¸­æ— è®°å½•'}
        
        return {'valid': True, 'count': len(db_data.data)}
    
    def _check_notes(self, kol_id: str, apis: Dict) -> Dict:
        db_data = self.client.table('gg_pgy_kol_notes').select('*').eq('kol_id', kol_id).execute()
        api_data = apis.get('kol_note_list', {}) or {}
        data = api_data.get('data', {}) or {}
        api_list = data.get('list', []) or []
        
        db_count = len(db_data.data)
        api_count = len(api_list)
        
        return {
            'valid': db_count == api_count,
            'db_count': db_count,
            'api_count': api_count
        }
    
    def _check_cost_effective(self, kol_id: str, apis: Dict) -> Dict:
        db_data = self.client.table('gg_pgy_kol_cost_effective').select('*').eq('kol_id', kol_id).execute()
        api_code = apis.get('kol_cost_effective', {}).get('code')
        
        if api_code != 0:
            return {'valid': True, 'note': 'APIè¿”å›é0ï¼Œè·³è¿‡'}
        
        if not db_data.data:
            return {'valid': False, 'error': 'æ•°æ®åº“ä¸­æ— è®°å½•'}
        
        return {'valid': True, 'count': len(db_data.data)}
    
    def _check_core_data(self, kol_id: str, apis: Dict) -> Dict:
        db_data = self.client.table('gg_pgy_kol_core_data').select('*').eq('kol_id', kol_id).execute()
        core_data = apis.get('kol_core_data', {}) or {}
        data = core_data.get('data', {}) or {}
        api_list = data.get('dailyData', []) or []
        
        db_count = len(db_data.data)
        api_count = len(api_list)
        
        return {
            'valid': db_count == api_count,
            'db_count': db_count,
            'api_count': api_count
        }
    
    def validate_all(self, kol_ids: List[str] = None, limit: int = None) -> Dict[str, Any]:
        """éªŒè¯æ‰€æœ‰æˆ–æŒ‡å®šçš„KOL"""
        if kol_ids is None:
            kol_ids = []
            for item in sorted(self.data_dir.iterdir()):
                if item.is_dir() and item.name.startswith('kol_'):
                    kol_ids.append(item.name.replace('kol_', ''))
        
        if limit:
            kol_ids = kol_ids[:limit]
        
        results = {
            'total': len(kol_ids),
            'valid': 0,
            'invalid': 0,
            'details': []
        }
        
        for i, kol_id in enumerate(kol_ids):
            if (i + 1) % 50 == 0:
                logger.info(f"éªŒè¯è¿›åº¦: {i+1}/{len(kol_ids)}")
            
            result = self.validate_kol(kol_id)
            results['details'].append(result)
            
            if result['valid']:
                results['valid'] += 1
            else:
                results['invalid'] += 1
        
        return results


class DataReporter:
    """æ•°æ®æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.client = self._load_supabase_client()
    
    def _load_supabase_client(self):
        from supabase import create_client
        
        backend_dir = Path(__file__).parent.parent.parent.parent
        env_path = backend_dir / '.env'
        
        if env_path.exists():
            load_dotenv(env_path)
        
        url = os.getenv('SUPABASE_URL')
        key = os.getenv('SUPABASE_KEY')
        
        return create_client(url, key)
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„æ•°æ®æŠ¥å‘Š"""
        report = {
            'generated_at': datetime.now().isoformat(),
            'tables': {}
        }
        
        tables_config = {
            'gg_pgy_kol_base_info': [
                'kol_id', 'kol_name', 'red_id', 'gender', 'location', 'head_photo',
                'fans_count', 'like_collect_count', 'business_note_count', 'total_note_count',
                'picture_price', 'video_price', 'lower_price', 'content_tags', 'feature_tags',
                'personal_tags', 'trade_type', 'click_mid_num', 'inter_mid_num',
                'fans_30_growth_num', 'fans_30_growth_rate', 'current_level', 'cooper_type',
                'user_type', 'low_active', 'kol_advantage', 'cooperate_state',
                'response_rate', 'invite_num', 'active_day_in_last_7', 'is_active', 'easy_connect'
            ],
            'gg_pgy_kol_audience': [
                'kol_id', 'gender_distribution', 'age_distribution', 'province_distribution',
                'city_distribution', 'device_distribution', 'interest_distribution', 'date_key'
            ],
            'gg_pgy_kol_fans_summary': [
                'kol_id', 'fans_num', 'fans_increase_num', 'fans_growth_rate',
                'fans_growth_beyond_rate', 'active_fans_l28', 'active_fans_rate',
                'active_fans_beyond_rate', 'engage_fans_l30', 'engage_fans_rate',
                'engage_fans_beyond_rate', 'read_fans_in_30', 'read_fans_rate',
                'read_fans_beyond_rate', 'pay_fans_user_rate_30d', 'pay_fans_user_num_30d'
            ],
            'gg_pgy_kol_note_rate': [
                'kol_id', 'note_number', 'video_note_number', 'imp_median',
                'imp_median_beyond_rate', 'read_median', 'read_median_beyond_rate',
                'interaction_median', 'interaction_rate', 'interaction_beyond_rate',
                'like_median', 'collect_median', 'comment_median', 'share_median',
                'hundred_like_percent', 'thousand_like_percent'
            ],
            'gg_pgy_kol_cost_effective': [
                'kol_id', 'picture_read_cost', 'picture_surpass_rate', 'picture_case',
                'estimate_picture_cpm', 'estimate_picture_cpm_compare', 'estimate_picture_engage_cost',
                'estimate_picture_engage_cost_compare', 'video_read_cost', 'video_surpass_rate',
                'video_case', 'estimate_video_cpm', 'estimate_video_cpm_compare',
                'estimate_video_engage_cost', 'estimate_video_engage_cost_compare'
            ]
        }
        
        for table_name, fields in tables_config.items():
            logger.info(f"åˆ†æè¡¨: {table_name}")
            table_report = self._analyze_table(table_name, fields)
            report['tables'][table_name] = table_report
        
        # ç»Ÿè®¡å¤šæ¡è®°å½•çš„è¡¨
        multi_record_tables = {
            'gg_pgy_kol_fans_trend': 'fans_num',
            'gg_pgy_kol_notes': 'read_num',
            'gg_pgy_kol_core_data': 'imp'
        }
        
        for table_name, sample_field in multi_record_tables.items():
            logger.info(f"åˆ†æè¡¨(å¤šè®°å½•): {table_name}")
            count_result = self.client.table(table_name).select('kol_id', count='exact').execute()
            report['tables'][table_name] = {
                'total_records': count_result.count if hasattr(count_result, 'count') else len(count_result.data),
                'note': 'å¤šè®°å½•è¡¨ï¼Œæ¯ä¸ªKOLæœ‰å¤šæ¡è®°å½•'
            }
        
        return report
    
    def _analyze_table(self, table_name: str, fields: List[str]) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªè¡¨çš„å­—æ®µç©ºå€¼æƒ…å†µ"""
        # è·å–æ‰€æœ‰è®°å½•
        result = self.client.table(table_name).select(','.join(fields)).execute()
        
        if not result.data:
            return {'total_records': 0, 'fields': {}}
        
        total = len(result.data)
        field_stats = {}
        
        for field in fields:
            null_count = sum(1 for row in result.data if row.get(field) is None)
            empty_count = sum(1 for row in result.data if row.get(field) == '' or row.get(field) == [] or row.get(field) == {})
            valid_count = total - null_count - empty_count
            
            field_stats[field] = {
                'total': total,
                'valid': valid_count,
                'null': null_count,
                'empty': empty_count,
                'valid_rate': f"{100 * valid_count / total:.1f}%",
                'null_rate': f"{100 * null_count / total:.1f}%"
            }
        
        return {
            'total_records': total,
            'fields': field_stats
        }
    
    def print_report(self, report: Dict[str, Any]):
        """æ‰“å°æŠ¥å‘Š"""
        logger.info("")
        logger.info("=" * 80)
        logger.info("ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š")
        logger.info(f"   ç”Ÿæˆæ—¶é—´: {report['generated_at']}")
        logger.info("=" * 80)
        
        for table_name, table_data in report['tables'].items():
            logger.info("")
            logger.info(f"ğŸ“‹ è¡¨: {table_name}")
            logger.info(f"   è®°å½•æ•°: {table_data['total_records']}")
            
            if 'fields' in table_data:
                logger.info("   å­—æ®µç©ºå€¼ç»Ÿè®¡:")
                logger.info(f"   {'å­—æ®µå':<35} {'æœ‰æ•ˆæ•°':<10} {'ç©ºå€¼æ•°':<10} {'æœ‰æ•ˆç‡':<10} {'ç©ºå€¼ç‡':<10}")
                logger.info("   " + "-" * 75)
                
                for field_name, stats in table_data['fields'].items():
                    logger.info(f"   {field_name:<35} {stats['valid']:<10} {stats['null']:<10} {stats['valid_rate']:<10} {stats['null_rate']:<10}")
            
            if 'note' in table_data:
                logger.info(f"   å¤‡æ³¨: {table_data['note']}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¯¼å…¥APIæ•°æ®åˆ°æ•°æ®åº“')
    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶å¯¼å…¥æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--concurrency', type=int, default=10, help='å¹¶å‘æ•°ï¼ˆé»˜è®¤10ï¼‰')
    parser.add_argument('--validate', action='store_true', help='ä»…éªŒè¯æ•°æ®ï¼Œä¸å¯¼å…¥')
    parser.add_argument('--validate-all', action='store_true', help='éªŒè¯æ‰€æœ‰æ•°æ®')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š')
    parser.add_argument('--kol-id', type=str, help='æŒ‡å®šå•ä¸ªKOL IDè¿›è¡ŒéªŒè¯')
    parser.add_argument('--full', action='store_true', help='æ‰§è¡Œå®Œæ•´æµç¨‹ï¼šå¯¼å…¥+éªŒè¯+æŠ¥å‘Š')
    args = parser.parse_args()
    
    if args.report:
        reporter = DataReporter()
        report = reporter.generate_report()
        reporter.print_report(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = Path(__file__).parent / "output" / "data_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        logger.info(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
    elif args.validate or args.validate_all:
        validator = DataValidator()
        
        if args.kol_id:
            result = validator.validate_kol(args.kol_id)
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            limit = None if args.validate_all else 10
            results = validator.validate_all(limit=args.limit or limit)
            print(f"\nâœ… éªŒè¯ç»“æœ: {results['valid']}/{results['total']} é€šè¿‡")
            
            invalid_details = [d for d in results['details'] if not d['valid']]
            if invalid_details:
                print(f"\nâŒ å¤±è´¥è¯¦æƒ… ({len(invalid_details)}):")
                for detail in invalid_details[:10]:
                    print(f"\n  {detail['kol_id']}:")
                    for check_name, check_result in detail.get('checks', {}).items():
                        if not check_result.get('valid'):
                            print(f"    {check_name}: {check_result}")
    
    elif args.full:
        # å®Œæ•´æµç¨‹ï¼šå¯¼å…¥ + éªŒè¯ + æŠ¥å‘Š
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´æµç¨‹ï¼šå¯¼å…¥ â†’ éªŒè¯ â†’ æŠ¥å‘Š")
        
        # 1. å¯¼å…¥
        importer = ApiDataImporter(concurrency=args.concurrency)
        kol_dirs = importer.get_all_kol_dirs()
        importer.import_kols_concurrent(kol_dirs, limit=args.limit)
        
        # 2. éªŒè¯
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ” å¼€å§‹éªŒè¯å¯¼å…¥çš„æ•°æ®...")
        validator = DataValidator()
        results = validator.validate_all(limit=args.limit)
        logger.info(f"\nâœ… éªŒè¯ç»“æœ: {results['valid']}/{results['total']} é€šè¿‡")
        
        invalid_count = results['invalid']
        if invalid_count > 0:
            logger.warning(f"âš ï¸ æœ‰ {invalid_count} ä¸ªKOLæ•°æ®éªŒè¯å¤±è´¥")
        
        # 3. æŠ¥å‘Š
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")
        reporter = DataReporter()
        report = reporter.generate_report()
        reporter.print_report(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path(__file__).parent / "output" / "data_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        logger.info(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
    else:
        # ä»…å¯¼å…¥
        importer = ApiDataImporter(concurrency=args.concurrency)
        kol_dirs = importer.get_all_kol_dirs()
        
        if args.limit:
            logger.info(f"æµ‹è¯•æ¨¡å¼ï¼šä»…å¯¼å…¥å‰ {args.limit} ä¸ªKOL")
        
        importer.import_kols_concurrent(kol_dirs, limit=args.limit)


if __name__ == "__main__":
    main()
