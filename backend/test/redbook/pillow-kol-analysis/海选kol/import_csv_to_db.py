#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CSV æ•°æ®å¯¼å…¥è„šæœ¬

å°† MCN æä¾›çš„ KOL ç­›é€‰è¡¨å¯¼å…¥åˆ° gg_pgy_kol_base_info è¡¨
"""

import os
import re
import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv
from supabase import create_client, Client


def load_supabase_client() -> Client:
    """åŠ è½½ Supabase å®¢æˆ·ç«¯"""
    backend_dir = Path(__file__).parent.parent.parent.parent
    env_path = backend_dir / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
    
    url = os.getenv('SUPABASE_URL')
    key = os.getenv('SUPABASE_KEY')
    
    if not url or not key:
        raise ValueError("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® SUPABASE_URL å’Œ SUPABASE_KEY")
    
    return create_client(url, key)


def extract_kol_id_from_pgy_link(pgy_link: str) -> Optional[str]:
    """ä»è’²å…¬è‹±é“¾æ¥æå– KOL ID
    
    ç¤ºä¾‹é“¾æ¥:
    https://pgy.xiaohongshu.com/solar/pre-trade/blogger-detail/63c4e411000000002600430e
    https://pgy.xiaohongshu.com/solar/pre-trade/blogger-detail/63c4e411000000002600430e?track_id=xxx
    """
    if not pgy_link or pgy_link.strip() == '':
        return None
    
    # åŒ¹é… blogger-detail/ åé¢çš„ ID
    pattern = r'blogger-detail/([a-f0-9]+)'
    match = re.search(pattern, pgy_link)
    
    if match:
        return match.group(1)
    
    return None


def extract_user_id_from_xhs_link(xhs_link: str) -> Optional[str]:
    """ä»å°çº¢ä¹¦é“¾æ¥æå–ç”¨æˆ· ID
    
    ç¤ºä¾‹é“¾æ¥:
    https://www.xiaohongshu.com/user/profile/63c4e411000000002600430e
    """
    if not xhs_link or xhs_link.strip() == '':
        return None
    
    # åŒ¹é… profile/ åé¢çš„ ID
    pattern = r'profile/([a-f0-9]+)'
    match = re.search(pattern, xhs_link)
    
    if match:
        return match.group(1)
    
    return None


def parse_price(price_str: str) -> Optional[str]:
    """è§£æä»·æ ¼å­—ç¬¦ä¸²"""
    if not price_str or price_str.strip() == '':
        return None
    return price_str.strip()


def parse_fans_wan(fans_str: str) -> Optional[float]:
    """è§£æç²‰ä¸æ•°ï¼ˆä¸‡ï¼‰"""
    if not fans_str or fans_str.strip() == '':
        return None
    
    try:
        # ç§»é™¤å¯èƒ½çš„é€—å·å’Œç©ºæ ¼
        cleaned = fans_str.strip().replace(',', '')
        return float(cleaned)
    except ValueError:
        return None


def parse_csv_row(row: List[str], headers: List[str]) -> Optional[Dict[str, Any]]:
    """è§£æ CSV è¡Œæ•°æ®
    
    CSV åˆ—ç»“æ„:
    0: ç©ºåˆ—ï¼ˆè¡Œå‰ç¼€ï¼Œå¦‚"åé¦ˆï¼š"ã€"xqx"ç­‰ï¼‰
    1: è¾¾äººæ˜µç§°
    2: æ–¹å‘
    3: å°çº¢ä¹¦é“¾æ¥
    4: è’²å…¬è‹±é“¾æ¥
    5: ç²‰ä¸æ•°ï¼ˆwï¼‰
    6: èµè—è¯„ï¼ˆwï¼‰
    7: å›¾æ–‡éæŠ¥å¤‡
    8: è§†é¢‘éæŠ¥å¤‡
    9: æ˜¯å¦é€‰ä¸­
    10: å¤‡æ³¨
    11: è¾¾äººåˆ›ä½œæ–¹å‘
    12: xqxåé¦ˆ
    13: wyyåé¦ˆ
    """
    # è·³è¿‡ç©ºè¡Œæˆ–æ ‡é¢˜è¡Œ
    if len(row) < 5:
        return None
    
    # è·å–è¾¾äººæ˜µç§°ï¼ˆç¬¬2åˆ—ï¼Œç´¢å¼•1ï¼‰
    kol_name = row[1].strip() if len(row) > 1 else ''
    
    # è·³è¿‡æ²¡æœ‰æ˜µç§°çš„è¡Œ
    if not kol_name or kol_name == 'è¾¾äººæ˜µç§°':
        return None
    
    # è·å–è’²å…¬è‹±é“¾æ¥ï¼ˆç¬¬5åˆ—ï¼Œç´¢å¼•4ï¼‰
    pgy_link = row[4].strip() if len(row) > 4 else ''
    
    # æå– KOL ID
    kol_id = extract_kol_id_from_pgy_link(pgy_link)
    
    # å¦‚æœæ²¡æœ‰è’²å…¬è‹±é“¾æ¥ï¼Œå°è¯•ä»å°çº¢ä¹¦é“¾æ¥æå–
    if not kol_id:
        xhs_link = row[3].strip() if len(row) > 3 else ''
        kol_id = extract_user_id_from_xhs_link(xhs_link)
    
    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ IDï¼Œè·³è¿‡è¿™è¡Œ
    if not kol_id:
        print(f"  âš ï¸ è·³è¿‡: {kol_name} - æ— æ³•æå– KOL ID")
        return None
    
    # æ„å»ºæ•°æ®å­—å…¸
    data = {
        'kol_id': kol_id,
        'kol_name': kol_name,
        'csv_row_prefix': row[0].strip() if len(row) > 0 else None,
        'csv_direction': row[2].strip() if len(row) > 2 else None,
        'csv_xiaohongshu_link': row[3].strip() if len(row) > 3 else None,
        'csv_pgy_link': pgy_link if pgy_link else None,
        'csv_fans_wan': parse_fans_wan(row[5]) if len(row) > 5 else None,
        'csv_like_collect_wan': parse_fans_wan(row[6]) if len(row) > 6 else None,
        'csv_picture_price': parse_price(row[7]) if len(row) > 7 else None,
        'csv_video_price': parse_price(row[8]) if len(row) > 8 else None,
        'csv_is_selected': row[9].strip() if len(row) > 9 else None,
        'csv_remark': row[10].strip() if len(row) > 10 else None,
        'csv_creator_direction': row[11].strip() if len(row) > 11 else None,
        'csv_xqx_feedback': row[12].strip() if len(row) > 12 else None,
        'csv_wyy_feedback': row[13].strip() if len(row) > 13 else None,
        'api_fetch_status': 'not_fetched',
        'created_at': datetime.now().isoformat(),
        'updated_at': datetime.now().isoformat(),
    }
    
    # ç§»é™¤ None å€¼
    data = {k: v for k, v in data.items() if v is not None and v != ''}
    
    return data


def read_csv_file(csv_path: str) -> List[Dict[str, Any]]:
    """è¯»å– CSV æ–‡ä»¶å¹¶è§£æ"""
    records = []
    skipped = 0
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        headers = next(reader)  # è·³è¿‡æ ‡é¢˜è¡Œ
        
        for row_num, row in enumerate(reader, start=2):
            data = parse_csv_row(row, headers)
            if data:
                records.append(data)
            else:
                skipped += 1
    
    return records, skipped


def import_to_supabase(client: Client, records: List[Dict[str, Any]]) -> Dict[str, int]:
    """å¯¼å…¥æ•°æ®åˆ° Supabase"""
    stats = {
        'inserted': 0,
        'updated': 0,
        'failed': 0,
        'duplicates': []
    }
    
    # æ£€æŸ¥é‡å¤çš„ kol_id
    kol_ids = [r['kol_id'] for r in records]
    seen = set()
    duplicates = set()
    for kol_id in kol_ids:
        if kol_id in seen:
            duplicates.add(kol_id)
        seen.add(kol_id)
    
    if duplicates:
        print(f"\nâš ï¸ å‘ç° {len(duplicates)} ä¸ªé‡å¤çš„ KOL ID:")
        for dup in list(duplicates)[:5]:
            # æ‰¾åˆ°æ‰€æœ‰ä½¿ç”¨è¿™ä¸ª ID çš„è®°å½•
            dup_records = [r for r in records if r['kol_id'] == dup]
            names = [r.get('kol_name', 'N/A') for r in dup_records]
            print(f"   - {dup}: {names}")
        stats['duplicates'] = list(duplicates)
    
    # å»é‡ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
    unique_records = {}
    for record in records:
        kol_id = record['kol_id']
        if kol_id not in unique_records:
            unique_records[kol_id] = record
    
    print(f"\nğŸ“Š å»é‡åè®°å½•æ•°: {len(unique_records)}")
    
    # ä½¿ç”¨ upsert å¯¼å…¥
    for kol_id, record in unique_records.items():
        try:
            result = client.table('gg_pgy_kol_base_info').upsert(
                record,
                on_conflict='kol_id'
            ).execute()
            
            if result.data:
                stats['inserted'] += 1
            else:
                stats['failed'] += 1
                print(f"  âŒ å¯¼å…¥å¤±è´¥: {record.get('kol_name', kol_id)}")
        except Exception as e:
            stats['failed'] += 1
            print(f"  âŒ å¯¼å…¥å¼‚å¸¸: {record.get('kol_name', kol_id)} - {str(e)[:50]}")
    
    return stats


def verify_import(client: Client, expected_count: int) -> Dict[str, Any]:
    """éªŒè¯å¯¼å…¥ç»“æœ"""
    # æŸ¥è¯¢æ€»è®°å½•æ•°
    result = client.table('gg_pgy_kol_base_info').select('kol_id', count='exact').execute()
    actual_count = result.count if result.count else len(result.data)
    
    # æŸ¥è¯¢æœ‰ kol_name çš„è®°å½•æ•°
    result_with_name = client.table('gg_pgy_kol_base_info').select(
        'kol_id', count='exact'
    ).not_.is_('kol_name', 'null').execute()
    with_name_count = result_with_name.count if result_with_name.count else len(result_with_name.data)
    
    # æŸ¥è¯¢æœ‰è’²å…¬è‹±é“¾æ¥çš„è®°å½•æ•°
    result_with_pgy = client.table('gg_pgy_kol_base_info').select(
        'kol_id', count='exact'
    ).not_.is_('csv_pgy_link', 'null').execute()
    with_pgy_count = result_with_pgy.count if result_with_pgy.count else len(result_with_pgy.data)
    
    return {
        'expected': expected_count,
        'actual': actual_count,
        'with_name': with_name_count,
        'with_pgy_link': with_pgy_count,
        'match': actual_count >= expected_count
    }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ“¥ CSV æ•°æ®å¯¼å…¥è„šæœ¬")
    print("=" * 60)
    
    # CSV æ–‡ä»¶è·¯å¾„
    csv_path = Path(__file__).parent / "NazzleNest&æ˜Ÿè¾°æ–‡åŒ–åˆä½œè¡¨ å‰¯æœ¬ - ç­›å·è¡¨.csv"
    
    if not csv_path.exists():
        print(f"âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return
    
    print(f"\nğŸ“„ CSV æ–‡ä»¶: {csv_path.name}")
    
    # 1. è¯»å– CSV
    print("\n[1/3] è¯»å– CSV æ–‡ä»¶...")
    records, skipped = read_csv_file(str(csv_path))
    print(f"  âœ… è§£ææˆåŠŸ: {len(records)} æ¡è®°å½•")
    print(f"  â­ï¸ è·³è¿‡: {skipped} è¡Œï¼ˆç©ºè¡Œ/æ— æ•ˆè¡Œï¼‰")
    
    if not records:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆè®°å½•ï¼Œé€€å‡º")
        return
    
    # 2. è¿æ¥æ•°æ®åº“å¹¶å¯¼å…¥
    print("\n[2/3] å¯¼å…¥åˆ°æ•°æ®åº“...")
    client = load_supabase_client()
    stats = import_to_supabase(client, records)
    
    print(f"\nğŸ“Š å¯¼å…¥ç»Ÿè®¡:")
    print(f"  âœ… æˆåŠŸ: {stats['inserted']}")
    print(f"  âŒ å¤±è´¥: {stats['failed']}")
    if stats['duplicates']:
        print(f"  âš ï¸ é‡å¤ ID: {len(stats['duplicates'])}")
    
    # 3. éªŒè¯å¯¼å…¥
    print("\n[3/3] éªŒè¯å¯¼å…¥ç»“æœ...")
    verify_result = verify_import(client, len(set(r['kol_id'] for r in records)))
    
    print(f"\nğŸ“‹ éªŒè¯ç»“æœ:")
    print(f"  é¢„æœŸè®°å½•æ•°: {verify_result['expected']}")
    print(f"  å®é™…è®°å½•æ•°: {verify_result['actual']}")
    print(f"  æœ‰æ˜µç§°è®°å½•: {verify_result['with_name']}")
    print(f"  æœ‰è’²å…¬è‹±é“¾æ¥: {verify_result['with_pgy_link']}")
    
    if verify_result['match']:
        print(f"\nâœ… å¯¼å…¥éªŒè¯é€šè¿‡ï¼")
    else:
        print(f"\nâš ï¸ å¯¼å…¥æ•°é‡ä¸åŒ¹é…ï¼Œè¯·æ£€æŸ¥")
    
    print("\n" + "=" * 60)


if __name__ == "__main__":
    main()
