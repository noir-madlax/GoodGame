#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è·å–ç¬”è®°è¯¦æƒ…å¹¶å­˜å…¥æ•°æ®åº“

åŠŸèƒ½ï¼š
1. ä»æ•°æ®åº“è·å–146ä¸ªç›®æ ‡è§†é¢‘çš„note_id
2. è°ƒç”¨note_detail_solaræ¥å£è·å–è¯¦æƒ…
3. å°†è¯¦æƒ…æ›´æ–°åˆ°gg_pgy_kol_notes.raw_dataå­—æ®µ
4. æ ‡è®°detail_fetchedä¸ºtrue
5. ä¸‹è½½è§†é¢‘æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•
"""

import os
import json
import asyncio
import aiohttp
import aiofiles
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, List, Optional, Tuple
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é¡¹ç›®è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_DIR = SCRIPT_DIR.parent
DATA_DIR = PROJECT_DIR / "02_è§†é¢‘æ•°æ®"

# åŠ è½½ç¯å¢ƒå˜é‡
BACKEND_DIR = Path("/Users/rigel/project/hdl-tikhub-goodgame/backend")
load_dotenv(BACKEND_DIR / '.env')


class NoteDetailFetcher:
    """ç¬”è®°è¯¦æƒ…è·å–å™¨"""
    
    # CDNåŸŸååˆ—è¡¨ï¼ˆç”¨äºä¸‹è½½è§†é¢‘ï¼‰
    CDN_DOMAINS = [
        'v.xiaohongshu.com',
        'sns-video-bd.xhscdn.com',
        'sns-video-hw.xhscdn.com', 
        'sns-video-qc.xhscdn.com',
        'sns-video-al.xhscdn.com',
    ]
    
    def __init__(self, concurrency: int = 5, api_delay: float = 0.5):
        self.concurrency = concurrency
        self.api_delay = api_delay
        self.token = os.getenv('JUSTONEAPI_API_KEY', '')
        self.base_url = "https://api.justoneapi.com"
        self.semaphore = asyncio.Semaphore(concurrency)
        self.download_semaphore = asyncio.Semaphore(3)
        
        # åŠ è½½APIé…ç½®
        config_path = BACKEND_DIR / "test/redbook/pgy/params/config.json"
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        self.endpoints = self.config['æ¥å£åˆ—è¡¨']
        
        # ç»Ÿè®¡
        self.stats = {
            'total': 0,
            'fetched': 0,
            'failed': 0,
            'db_updated': 0,
            'downloaded': 0,
            'download_failed': 0
        }
    
    def _get_supabase_client(self):
        """è·å–Supabaseå®¢æˆ·ç«¯"""
        from supabase import create_client
        url = os.getenv('SUPABASE_URL')
        key = os.getenv('SUPABASE_KEY')
        return create_client(url, key)
    
    def get_target_notes(self) -> List[Dict]:
        """è·å–ç›®æ ‡ç¬”è®°åˆ—è¡¨ï¼ˆ32ä½å…¥é€‰KOLçš„TOP5è§†é¢‘ï¼‰"""
        client = self._get_supabase_client()
        
        # è·å–å…¥é€‰KOL
        kol_response = client.table('gg_pgy_kol_analysis_result').select(
            'kol_id, kol_name'
        ).eq('post_frequency_pass', True
        ).eq('comment_gt_20_pass', True
        ).eq('read_fans_ratio_pass', True
        ).execute()
        
        kol_map = {row['kol_id']: row['kol_name'] for row in kol_response.data}
        logger.info(f"è·å–åˆ° {len(kol_map)} ä½å…¥é€‰KOL")
        
        all_notes = []
        
        for kol_id, kol_name in kol_map.items():
            # è·å–è¯¥KOLçš„è§†é¢‘ç¬”è®°ï¼ŒæŒ‰äº’åŠ¨æ’åºå–TOP5
            # ä¼˜å…ˆè·å–detail_fetched=falseçš„
            notes_response = client.table('gg_pgy_kol_notes').select(
                'id, note_id, kol_id, title, is_video, is_advertise, '
                'read_num, like_num, collect_num, comment_num, publish_date, img_url, detail_fetched'
            ).eq('kol_id', kol_id
            ).eq('is_video', True
            ).order('like_num', desc=True
            ).limit(5
            ).execute()
            
            if not notes_response.data:
                logger.warning(f"KOL {kol_name} æ— è§†é¢‘")
                continue
            
            for note in notes_response.data:
                note['kol_name'] = kol_name
                all_notes.append(note)
        
        self.stats['total'] = len(all_notes)
        logger.info(f"å…±éœ€å¤„ç† {len(all_notes)} ä¸ªè§†é¢‘")
        return all_notes
    
    async def fetch_note_detail(self, session: aiohttp.ClientSession, 
                                 note_id: str) -> Tuple[Optional[Dict], Optional[str]]:
        """è·å–ç¬”è®°è¯¦æƒ…ï¼Œè¿”å›(è¯¦æƒ…æ•°æ®, è§†é¢‘URL)"""
        async with self.semaphore:
            endpoint = self.endpoints.get('note_detail_solar')
            url = f"{self.base_url}{endpoint}"
            params = {
                'token': self.token,
                'noteId': note_id,
                'acceptCache': 'true'
            }
            
            try:
                async with session.get(url, params=params,
                                       timeout=aiohttp.ClientTimeout(total=30)) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        if result.get('code') == 0:
                            data = result.get('data', {})
                            video_url = None
                            video_info = data.get('videoInfo', {})
                            if video_info:
                                video_url = video_info.get('videoUrl')
                            return data, video_url
                        else:
                            code = result.get('code')
                            msg = result.get('message', '')
                            logger.warning(f"âš ï¸ {note_id}: code={code} {msg}")
                    else:
                        logger.error(f"âŒ {note_id}: HTTP {resp.status}")
            except Exception as e:
                logger.error(f"âŒ {note_id}: {e}")
            
            await asyncio.sleep(self.api_delay)
            return None, None
    
    def update_note_in_db(self, client, note_id: str, detail_data: Dict) -> bool:
        """æ›´æ–°æ•°æ®åº“ä¸­çš„ç¬”è®°è¯¦æƒ…"""
        try:
            # ä»è¯¦æƒ…ä¸­æå–å…³é”®å­—æ®µ
            update_data = {
                'raw_data': detail_data,
                'detail_fetched': True,
                'updated_at': datetime.now().isoformat()
            }
            
            # å¦‚æœæœ‰æ›´å¤šæ•°æ®ï¼Œä¹Ÿæ›´æ–°
            if detail_data.get('content'):
                pass  # raw_dataä¸­å·²åŒ…å«
            if detail_data.get('shareNum'):
                update_data['share_num'] = detail_data['shareNum']
            if detail_data.get('impNum'):
                update_data['imp_num'] = detail_data['impNum']
            if detail_data.get('followCnt'):
                update_data['follow_cnt'] = detail_data['followCnt']
            
            client.table('gg_pgy_kol_notes').update(
                update_data
            ).eq('note_id', note_id).execute()
            
            return True
        except Exception as e:
            logger.error(f"æ•°æ®åº“æ›´æ–°å¤±è´¥ {note_id}: {e}")
            return False
    
    async def download_video(self, session: aiohttp.ClientSession,
                              video_url: str, note_id: str, kol_id: str) -> Optional[str]:
        """ä¸‹è½½è§†é¢‘æ–‡ä»¶ï¼Œå°è¯•å¤šä¸ªCDN"""
        kol_dir = DATA_DIR / f"kol_{kol_id}" / "videos"
        kol_dir.mkdir(parents=True, exist_ok=True)
        video_file = kol_dir / f"{note_id}.mp4"
        
        # å·²å­˜åœ¨åˆ™è·³è¿‡
        if video_file.exists() and video_file.stat().st_size > 10000:
            logger.info(f"â­ï¸ å·²å­˜åœ¨: {note_id}")
            return str(video_file)
        
        # ä»åŸURLæå–video_key
        video_key = None
        if 'xiaohongshu.com/' in video_url:
            # http://v.xiaohongshu.com/stream/79/110/258/xxx.mp4?sign=...
            path_part = video_url.split('?')[0]
            if '/stream/' in path_part:
                video_key = path_part.split('.com/')[1]
        
        # æ„å»ºå€™é€‰URLåˆ—è¡¨
        urls_to_try = [video_url]  # åŸURLä¼˜å…ˆ
        if video_key:
            for domain in self.CDN_DOMAINS:
                cdn_url = f"http://{domain}/{video_key}"
                if cdn_url != video_url.split('?')[0]:
                    urls_to_try.append(cdn_url)
        
        async with self.download_semaphore:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)',
                'Referer': 'https://www.xiaohongshu.com/'
            }
            
            for url in urls_to_try:
                try:
                    async with session.get(url, headers=headers,
                                          timeout=aiohttp.ClientTimeout(total=300)) as resp:
                        if resp.status == 200:
                            async with aiofiles.open(video_file, 'wb') as f:
                                total = 0
                                async for chunk in resp.content.iter_chunked(1024 * 1024):
                                    await f.write(chunk)
                                    total += len(chunk)
                            
                            if total > 10000:
                                logger.info(f"ğŸ“¥ ä¸‹è½½å®Œæˆ: {note_id} ({total/1024/1024:.1f}MB)")
                                return str(video_file)
                            else:
                                video_file.unlink()
                except Exception as e:
                    logger.debug(f"CDN {url[:50]} å¤±è´¥: {e}")
                    continue
            
            logger.warning(f"âš ï¸ æ‰€æœ‰CDNéƒ½æ— æ³•ä¸‹è½½: {note_id}")
            return None
    
    async def process_note(self, session: aiohttp.ClientSession, 
                            client, note: Dict) -> Dict:
        """å¤„ç†å•ä¸ªç¬”è®°ï¼šè·å–è¯¦æƒ…ã€æ›´æ–°æ•°æ®åº“ã€ä¸‹è½½è§†é¢‘"""
        note_id = note['note_id']
        kol_id = note['kol_id']
        kol_name = note.get('kol_name', 'Unknown')
        title = (note.get('title') or '')[:25]
        
        # 1. è·å–è¯¦æƒ…
        detail_data, video_url = await self.fetch_note_detail(session, note_id)
        
        if detail_data:
            self.stats['fetched'] += 1
            
            # 2. æ›´æ–°æ•°æ®åº“
            if self.update_note_in_db(client, note_id, detail_data):
                self.stats['db_updated'] += 1
            
            # 3. ä¸‹è½½è§†é¢‘
            if video_url:
                file_path = await self.download_video(session, video_url, note_id, kol_id)
                if file_path:
                    self.stats['downloaded'] += 1
                    note['file_path'] = file_path
                    note['downloaded'] = True
                else:
                    self.stats['download_failed'] += 1
            
            logger.info(f"âœ… {kol_name}: {title}...")
            note['video_url'] = video_url
            note['detail_fetched'] = True
        else:
            self.stats['failed'] += 1
            logger.warning(f"âŒ {kol_name}: {note_id}")
        
        await asyncio.sleep(self.api_delay)
        return note
    
    def save_local_metadata(self, notes: List[Dict]):
        """ä¿å­˜æœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶"""
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        
        # æŒ‰KOLåˆ†ç»„
        kol_notes = {}
        for n in notes:
            kol_id = n['kol_id']
            if kol_id not in kol_notes:
                kol_notes[kol_id] = []
            kol_notes[kol_id].append(n)
        
        for kol_id, notes_list in kol_notes.items():
            kol_dir = DATA_DIR / f"kol_{kol_id}"
            kol_dir.mkdir(parents=True, exist_ok=True)
            
            metadata = {
                'kol_id': kol_id,
                'kol_name': notes_list[0].get('kol_name', 'Unknown'),
                'video_count': len(notes_list),
                'videos': notes_list
            }
            
            with open(kol_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
        
        # æ±‡æ€»æ–‡ä»¶
        summary = {
            'generated_at': datetime.now().isoformat(),
            'stats': self.stats,
            'total_kols': len(kol_notes),
            'total_videos': len(notes),
            'videos': notes
        }
        
        with open(DATA_DIR / 'video_list.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“ æœ¬åœ°å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {DATA_DIR}")
    
    async def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ‰¹é‡è·å–ç¬”è®°è¯¦æƒ…")
        logger.info("=" * 60)
        
        # 1. è·å–ç›®æ ‡ç¬”è®°åˆ—è¡¨
        notes = self.get_target_notes()
        
        # è·å–æ•°æ®åº“å®¢æˆ·ç«¯
        client = self._get_supabase_client()
        
        # 2. åˆ›å»ºconnectoré¿å…sessioné—®é¢˜
        connector = aiohttp.TCPConnector(limit=10, force_close=True)
        
        # 3. æ‰¹é‡å¤„ç†
        async with aiohttp.ClientSession(connector=connector) as session:
            batch_size = 5  # å‡å°‘æ‰¹æ¬¡å¤§å°
            for i in range(0, len(notes), batch_size):
                batch = notes[i:i+batch_size]
                logger.info(f"\nå¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(notes)-1)//batch_size + 1}")
                
                # ä¸²è¡Œå¤„ç†æ¯ä¸ªç¬”è®°ï¼ˆæ›´ç¨³å®šï¼‰
                for j, note in enumerate(batch):
                    try:
                        result = await self.process_note(session, client, note)
                        notes[i+j] = result
                    except Exception as e:
                        logger.error(f"å¤„ç†å¤±è´¥ {note.get('note_id')}: {e}")
                        self.stats['failed'] += 1
                
                # æ¯æ‰¹æ¬¡åæš‚åœä¸€ä¸‹
                await asyncio.sleep(1)
        
        # 4. ä¿å­˜æœ¬åœ°å…ƒæ•°æ®
        self.save_local_metadata(notes)
        
        # 5. æ‰“å°ç»Ÿè®¡
        self._print_stats()
        
        return notes
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡"""
        print("\n" + "=" * 60)
        print("å¤„ç†å®Œæˆ")
        print("=" * 60)
        print(f"æ€»ç¬”è®°æ•°: {self.stats['total']}")
        print(f"è¯¦æƒ…è·å–æˆåŠŸ: {self.stats['fetched']}")
        print(f"è¯¦æƒ…è·å–å¤±è´¥: {self.stats['failed']}")
        print(f"æ•°æ®åº“æ›´æ–°æˆåŠŸ: {self.stats['db_updated']}")
        print(f"è§†é¢‘ä¸‹è½½æˆåŠŸ: {self.stats['downloaded']}")
        print(f"è§†é¢‘ä¸‹è½½å¤±è´¥: {self.stats['download_failed']}")
        print("=" * 60)


async def main():
    """ä¸»å‡½æ•°"""
    fetcher = NoteDetailFetcher(concurrency=5, api_delay=0.5)
    await fetcher.run()


if __name__ == "__main__":
    asyncio.run(main())
