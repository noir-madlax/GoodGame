#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨Gemini 2.5 Flashåˆ†æè§†é¢‘å†…å®¹

åˆ†æç»´åº¦ï¼š
1. åšä¸»å½¢è±¡ä¸å®¶å±…é£æ ¼
2. å‰ªè¾‘èƒ½åŠ›
3. å£æ’­èƒ½åŠ›

æ¨¡å‹é…ç½®ï¼š
- æ¨¡å‹: gemini-2.5-flash (Google Gemini APIç›´è¿)
- æ¸©åº¦: 0.3
- max_output_tokens: 8192
- å¹¶è¡Œå¤„ç†: 5ä¸ªè§†é¢‘åŒæ—¶åˆ†æ
"""

import os
import sys
import json
import time
import asyncio
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, Optional, List
from concurrent.futures import ThreadPoolExecutor, as_completed
import google.generativeai as genai

# åŠ è½½ç¯å¢ƒå˜é‡
BACKEND_DIR = Path("/Users/rigel/project/hdl-tikhub-goodgame/backend")
load_dotenv(BACKEND_DIR / '.env')

# é¡¹ç›®è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_DIR = SCRIPT_DIR.parent
DATA_DIR = PROJECT_DIR / "02_è§†é¢‘æ•°æ®"
RESULT_DIR = PROJECT_DIR / "03_åˆ†æç»“æœ"
PROMPT_DIR = PROJECT_DIR / "prompts"

# é…ç½®Gemini - ä¼˜å…ˆä½¿ç”¨GEMINI_API_KEY2
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY2', '') or os.getenv('GEMINI_API_KEY', '') or os.getenv('GEMINI_API_KEY_ANALYZE', '')
genai.configure(api_key=GEMINI_API_KEY)


def log(msg: str):
    """å®æ—¶æ‰“å°æ—¥å¿—"""
    print(msg, flush=True)


def get_supabase_client():
    """è·å–Supabaseå®¢æˆ·ç«¯"""
    from supabase import create_client
    url = os.getenv('SUPABASE_URL')
    key = os.getenv('SUPABASE_KEY')
    return create_client(url, key)


def enrich_video_metadata(note_id: str, basic_meta: Dict) -> Dict:
    """ä»æ•°æ®åº“è·å–å®Œæ•´çš„è§†é¢‘å…ƒæ•°æ®"""
    client = get_supabase_client()
    
    note_resp = client.table('gg_pgy_kol_notes').select(
        'note_id, kol_id, title, raw_data'
    ).eq('note_id', note_id).single().execute()
    
    if note_resp.data:
        raw_data = note_resp.data.get('raw_data') or {}
        basic_meta['content'] = raw_data.get('content', '')
    
    kol_id = basic_meta.get('kol_id')
    if kol_id:
        kol_resp = client.table('gg_pgy_kol_analysis_result').select(
            'kol_name, fans_count_current'
        ).eq('kol_id', kol_id).single().execute()
        
        if kol_resp.data:
            basic_meta['kol_name'] = kol_resp.data.get('kol_name') or basic_meta.get('kol_name', '')
            basic_meta['fans_count'] = kol_resp.data.get('fans_count_current', 0)
    
    return basic_meta


def load_system_prompt() -> str:
    """ä»æ–‡ä»¶åŠ è½½ç³»ç»Ÿprompt"""
    prompt_file = PROMPT_DIR / "video_analysis_prompt.txt"
    if prompt_file.exists():
        with open(prompt_file, 'r', encoding='utf-8') as f:
            return f.read()
    raise FileNotFoundError(f"Promptæ–‡ä»¶ä¸å­˜åœ¨: {prompt_file}")


def create_analysis_prompt(video_metadata: Dict) -> str:
    """åˆ›å»ºåˆ†ææç¤ºè¯"""
    content = video_metadata.get('content', 'æ— æ­£æ–‡')
    if len(content) > 1000:
        content = content[:1000] + "..."
    
    return f"""
## è§†é¢‘èƒŒæ™¯ä¿¡æ¯

- **åšä¸»åç§°**: {video_metadata.get('kol_name', 'æœªçŸ¥')}
- **è§†é¢‘æ ‡é¢˜**: {video_metadata.get('title', 'æ— æ ‡é¢˜')}
- **è§†é¢‘æ­£æ–‡**: 
{content}

è¯·è§‚çœ‹ä¸Šä¼ çš„è§†é¢‘ï¼Œç»“åˆä»¥ä¸ŠèƒŒæ™¯ä¿¡æ¯ï¼Œè¿›è¡Œä¸“ä¸šåˆ†æã€‚
"""


def analyze_video(video_path: str, video_metadata: Dict, video_index: int = 0, total: int = 1, max_retries: int = 3) -> Optional[Dict]:
    """åˆ†æå•ä¸ªè§†é¢‘ï¼ˆå¸¦å®æ—¶æ—¥å¿—å’Œé‡è¯•ï¼‰"""
    note_id = video_metadata.get('note_id', 'unknown')
    title = (video_metadata.get('title') or 'æ— æ ‡é¢˜')[:25]
    
    if not GEMINI_API_KEY:
        log(f"  [{video_index}/{total}] âŒ æœªé…ç½® GEMINI_API_KEY")
        return None
    
    for attempt in range(max_retries):
        try:
            system_prompt = load_system_prompt()
            
            if attempt == 0:
                log(f"  [{video_index}/{total}] ğŸ“¤ ä¸Šä¼ : {title}...")
            else:
                log(f"  [{video_index}/{total}] ğŸ”„ é‡è¯•({attempt+1}/{max_retries}): {title}...")
                time.sleep(30 * attempt)  # æŒ‡æ•°é€€é¿
            
            video_file = genai.upload_file(video_path, mime_type="video/mp4")
            
            log(f"  [{video_index}/{total}] â³ å¤„ç†ä¸­...")
            while video_file.state.name == "PROCESSING":
                time.sleep(3)
                video_file = genai.get_file(video_file.name)
            
            if video_file.state.name != "ACTIVE":
                log(f"  [{video_index}/{total}] âŒ å¤„ç†å¤±è´¥: {video_file.state.name}")
                continue
            
            model = genai.GenerativeModel(
                model_name="gemini-2.5-flash",
                generation_config={"temperature": 0.3, "max_output_tokens": 8192}
            )
            
            user_prompt = create_analysis_prompt(video_metadata)
            
            log(f"  [{video_index}/{total}] ğŸ” åˆ†æä¸­...")
            response = model.generate_content(
                [video_file, system_prompt, user_prompt],
                request_options={"timeout": 300}
            )
            
            result_text = response.text
            
            # æå–JSON
            if "```json" in result_text:
                json_start = result_text.find("```json") + 7
                json_end = result_text.find("```", json_start)
                result_text = result_text[json_start:json_end].strip()
            elif "```" in result_text:
                json_start = result_text.find("```") + 3
                json_end = result_text.find("```", json_start)
                result_text = result_text[json_start:json_end].strip()
            
            # ä¿®å¤ä¸å®Œæ•´JSON
            try:
                result = json.loads(result_text)
            except json.JSONDecodeError:
                if result_text.count('{') > result_text.count('}'):
                    missing = result_text.count('{') - result_text.count('}')
                    result_text = result_text.rstrip(',\n ') + '\n' + '}' * missing
                    result = json.loads(result_text)
                else:
                    raise
            
            # æ¸…ç†ä¸Šä¼ æ–‡ä»¶
            try:
                genai.delete_file(video_file.name)
            except:
                pass
            
            score = result.get('overall_assessment', {}).get('overall_score', 'N/A')
            log(f"  [{video_index}/{total}] âœ… å®Œæˆ: {title} | è¯„åˆ†: {score}")
            
            return result
            
        except json.JSONDecodeError as e:
            log(f"  [{video_index}/{total}] âŒ JSONé”™è¯¯: {title}")
            return None
        except Exception as e:
            err_str = str(e)
            if "429" in err_str or "quota" in err_str.lower():
                log(f"  [{video_index}/{total}] âš ï¸ é…é¢é™åˆ¶ï¼Œç­‰å¾…é‡è¯•...")
                if attempt < max_retries - 1:
                    time.sleep(60 * (attempt + 1))  # 1åˆ†é’Ÿ, 2åˆ†é’Ÿ...
                    continue
            log(f"  [{video_index}/{total}] âŒ å¼‚å¸¸: {title} - {err_str[:50]}")
            if attempt == max_retries - 1:
                return None
    
    return None


def save_result(result: Dict, video_metadata: Dict):
    """ä¿å­˜åˆ†æç»“æœ"""
    kol_id = video_metadata.get('kol_id')
    note_id = video_metadata.get('note_id')
    
    result_dir = RESULT_DIR / f"kol_{kol_id}"
    result_dir.mkdir(parents=True, exist_ok=True)
    
    result_file = result_dir / f"{note_id}_analysis.json"
    
    full_result = {
        'video_metadata': video_metadata,
        'analysis_result': result,
        'analyzed_at': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(full_result, f, ensure_ascii=False, indent=2)


def analyze_single_video_task(args):
    """å•ä¸ªè§†é¢‘åˆ†æä»»åŠ¡ï¼ˆç”¨äºå¹¶è¡Œï¼‰"""
    video_meta, video_index, total = args
    note_id = video_meta['note_id']
    
    # ä»æ•°æ®åº“è·å–content
    video_meta = enrich_video_metadata(note_id, video_meta.copy())
    
    video_path = video_meta.get('file_path')
    if not video_path or not Path(video_path).exists():
        log(f"  [{video_index}/{total}] âŒ æ–‡ä»¶ä¸å­˜åœ¨: {note_id}")
        return None
    
    result = analyze_video(video_path, video_meta, video_index, total)
    
    if result:
        save_result(result, video_meta)
    
    return (note_id, result)


def analyze_kol_videos(kol_id: str, skip_analyzed: bool = True, max_workers: int = 5):
    """å¹¶è¡Œåˆ†ææŒ‡å®šKOLçš„æ‰€æœ‰è§†é¢‘"""
    video_list_file = DATA_DIR / "video_list.json"
    with open(video_list_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    videos = [v for v in data['videos'] if v.get('kol_id') == kol_id and v.get('downloaded')]
    
    if not videos:
        log(f"âŒ æ‰¾ä¸åˆ°KOL {kol_id} çš„è§†é¢‘")
        return
    
    kol_name = videos[0].get('kol_name', 'Unknown')
    
    # æ£€æŸ¥å·²åˆ†æ
    analyzed = set()
    kol_result_dir = RESULT_DIR / f"kol_{kol_id}"
    if kol_result_dir.exists():
        analyzed = {f.stem.replace('_analysis', '') for f in kol_result_dir.glob('*_analysis.json')}
    
    # ç­›é€‰å¾…åˆ†æ
    if skip_analyzed:
        to_analyze = [v for v in videos if v['note_id'] not in analyzed]
        skipped = len(videos) - len(to_analyze)
    else:
        to_analyze = videos
        skipped = 0
    
    log(f"\n{'='*60}")
    log(f"KOL: {kol_name}")
    log(f"æ€»è§†é¢‘: {len(videos)} | å¾…åˆ†æ: {len(to_analyze)} | è·³è¿‡: {skipped}")
    log(f"å¹¶è¡Œæ•°: {min(max_workers, len(to_analyze))}")
    log(f"{'='*60}")
    
    if not to_analyze:
        log("âœ… æ‰€æœ‰è§†é¢‘å·²åˆ†æå®Œæˆ")
        return
    
    # å‡†å¤‡ä»»åŠ¡
    tasks = [(v, i+1, len(to_analyze)) for i, v in enumerate(to_analyze)]
    
    # å¹¶è¡Œæ‰§è¡Œ
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(analyze_single_video_task, task): task for task in tasks}
        for future in as_completed(futures):
            result = future.result()
            if result:
                results.append(result)
    
    success = sum(1 for r in results if r[1] is not None)
    log(f"\nâœ… {kol_name} åˆ†æå®Œæˆ: {success}/{len(to_analyze)} æˆåŠŸ")


def generate_kol_summary(kol_id: str, kol_name: str) -> Optional[str]:
    """ç”ŸæˆKOLæ±‡æ€»æŠ¥å‘Š"""
    
    kol_result_dir = RESULT_DIR / f"kol_{kol_id}"
    if not kol_result_dir.exists():
        log(f"âŒ æ‰¾ä¸åˆ°KOL {kol_name} çš„åˆ†æç»“æœ")
        return None
    
    analysis_files = list(kol_result_dir.glob("*_analysis.json"))
    if not analysis_files:
        log(f"âŒ KOL {kol_name} æ²¡æœ‰åˆ†æç»“æœ")
        return None
    
    video_analyses = []
    for f in analysis_files:
        with open(f, 'r', encoding='utf-8') as file:
            video_analyses.append(json.load(file))
    
    log(f"ğŸ“Š æ±‡æ€» {kol_name} çš„ {len(video_analyses)} ä¸ªè§†é¢‘...")
    
    # æ”¶é›†æ•°æ®
    scores_d1, scores_d2, scores_d3 = [], [], []
    all_evidence = []
    home_styles, soft_decorations, pillow_observations = [], [], []
    
    for va in video_analyses:
        meta = va.get('video_metadata', {})
        result = va.get('analysis_result', {})
        
        d1 = result.get('dimension_1_style', {})
        d2 = result.get('dimension_2_editing', {})
        d3 = result.get('dimension_3_speaking', {})
        
        if d1.get('score'): scores_d1.append(d1['score'])
        if d2.get('score'): scores_d2.append(d2['score'])
        if d3.get('score'): scores_d3.append(d3['score'])
        
        if d1.get('home_style_type'): home_styles.append(d1['home_style_type'])
        if d1.get('soft_decoration_details'): soft_decorations.append(d1['soft_decoration_details'])
        if d1.get('pillow_or_cushion_observed'): pillow_observations.append(d1['pillow_or_cushion_observed'])
        
        video_info = {
            'title': meta.get('title', ''),
            'is_ad': meta.get('is_advertise', False),
            'video_summary': result.get('video_summary', ''),
            'd1_score': d1.get('score', 0),
            'd2_score': d2.get('score', 0),
            'd3_score': d3.get('score', 0),
            # å®Œæ•´ä¿ç•™è¯„åˆ†ç†ç”±ï¼Œä¸æˆªæ–­
            'd1_reasoning': d1.get('score_reasoning', 'æ— '),
            'd2_reasoning': d2.get('score_reasoning', 'æ— '),
            'd3_reasoning': d3.get('score_reasoning', 'æ— '),
            'd3_key_quotes': d3.get('key_quotes', []),
            'overall': result.get('overall_assessment', {})
        }
        all_evidence.append(video_info)
    
    # è®¡ç®—å¹³å‡åˆ†
    avg_d1 = sum(scores_d1) / len(scores_d1) if scores_d1 else 0
    avg_d2 = sum(scores_d2) / len(scores_d2) if scores_d2 else 0
    avg_d3 = sum(scores_d3) / len(scores_d3) if scores_d3 else 0
    avg_overall = (avg_d1 + avg_d2 + avg_d3) / 3
    
    fans_count = video_analyses[0].get('video_metadata', {}).get('fans_count', 0)
    
    def get_rating(score):
        if score >= 4.5: return "ä¼˜ç§€ â­â­â­"
        elif score >= 4: return "è‰¯å¥½ â­â­"
        elif score >= 3: return "ä¸€èˆ¬ â­"
        else: return "è¾ƒå¼±"
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# {kol_name} å†…å®¹èƒ½åŠ›ç»¼åˆè¯„ä¼°æŠ¥å‘Š

> **è¯„ä¼°ç›®çš„**ï¼šåˆ¤æ–­è¯¥åšä¸»æ˜¯å¦é€‚åˆè¿›è¡Œ**æŠ±æ•å“ç‰Œ**çš„æ¨å¹¿åˆä½œ

---

## ä¸€ã€åšä¸»åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | ä¿¡æ¯ |
|------|------|
| åšä¸»åç§° | {kol_name} |
| ç²‰ä¸æ•°é‡ | {fans_count or 0:,} |
| åˆ†æè§†é¢‘æ•° | {len(video_analyses)} ä¸ª |
| è¯„ä¼°æ—¶é—´ | {time.strftime('%Y-%m-%d %H:%M:%S')} |

---

## äºŒã€ä¸‰ç»´åº¦ç»¼åˆè¯„åˆ†

| è¯„ä¼°ç»´åº¦ | å¹³å‡å¾—åˆ† | è¯„çº§ | è¯´æ˜ |
|----------|----------|------|------|
| ç»´åº¦1ï¼šå½¢è±¡ä¸å®¶å±…é£æ ¼ | {avg_d1:.1f} åˆ† | {get_rating(avg_d1)} | å¯¹æŠ±æ•æ¨å¹¿æœ€é‡è¦çš„ç»´åº¦ |
| ç»´åº¦2ï¼šå‰ªè¾‘èƒ½åŠ› | {avg_d2:.1f} åˆ† | {get_rating(avg_d2)} | è§†é¢‘åˆ¶ä½œä¸“ä¸šåº¦ |
| ç»´åº¦3ï¼šå£æ’­èƒ½åŠ› | {avg_d3:.1f} åˆ† | {get_rating(avg_d3)} | äº§å“å–ç‚¹ä¼ è¾¾èƒ½åŠ› |
| **ç»¼åˆè¯„åˆ†** | **{avg_overall:.1f} åˆ†** | **{get_rating(avg_overall)}** | - |

---

## ä¸‰ã€å„è§†é¢‘åˆ†ææ‘˜è¦

"""
    
    for i, ev in enumerate(all_evidence, 1):
        ad_tag = " [å¹¿å‘Š]" if ev['is_ad'] else ""
        report += f"""### è§†é¢‘{i}: {ev['title']}{ad_tag}

**å†…å®¹æ¦‚è¦**: {ev['video_summary']}

**è¯„åˆ†è¯¦æƒ…**:

| ç»´åº¦ | å¾—åˆ† | è¯„åˆ†ç†ç”± |
|------|------|----------|
| å½¢è±¡é£æ ¼ | {ev['d1_score']}åˆ† | {ev['d1_reasoning']} |
| å‰ªè¾‘èƒ½åŠ› | {ev['d2_score']}åˆ† | {ev['d2_reasoning']} |
| å£æ’­èƒ½åŠ› | {ev['d3_score']}åˆ† | {ev['d3_reasoning']} |

---

"""
    
    # å®¶å±…é£æ ¼åˆ†æ
    report += """## å››ã€æŠ±æ•æ¨å¹¿é€‚åˆåº¦åˆ†æ

### 4.1 å®¶å±…é£æ ¼ä¸è½¯è£…å…ƒç´ 

"""
    if home_styles:
        report += f"**å®¶å±…é£æ ¼**: {', '.join(set(home_styles))}\n\n"
    
    if soft_decorations:
        report += "**è½¯è£…å…ƒç´ **:\n"
        for i, dec in enumerate(soft_decorations, 1):
            report += f"- è§†é¢‘{i}: {dec}\n"
        report += "\n"
    
    has_pillow = any(p and 'æ— ' not in p and 'æ²¡æœ‰' not in p for p in pillow_observations if p)
    if has_pillow:
        report += "**æŠ±æ•å‡ºç°**: âœ… æœ‰\n"
    else:
        report += "**æŠ±æ•å‡ºç°**: âŒ æœªæ˜æ˜¾å‡ºç°\n"
    
    # å£æ’­åˆ†æ
    report += f"""
### 4.2 å£æ’­èƒ½åŠ›

"""
    if avg_d3 > 1.5:
        report += f"åšä¸»å…·æœ‰å£æ’­èƒ½åŠ›ï¼ˆ{avg_d3:.1f}åˆ†ï¼‰ï¼Œèƒ½å¤Ÿä¼ è¾¾äº§å“å–ç‚¹ã€‚\n"
    else:
        report += "åšä¸»è§†é¢‘ä¸»è¦ä¾é ç”»é¢ä¼ è¾¾ï¼Œå£æ’­è¾ƒå°‘ã€‚å¦‚éœ€å£æ’­ä»‹ç»ï¼Œéœ€é¢å¤–æ²Ÿé€šã€‚\n"
    
    # å¥‘åˆåº¦
    report += "\n### 4.3 å“ç‰Œå¥‘åˆåº¦\n\n"
    if avg_d1 >= 4:
        report += "âœ… **é«˜åº¦å¥‘åˆ**: å®¶å±…é£æ ¼æ¸©é¦¨æœ‰å“å‘³ï¼Œé€‚åˆæŠ±æ•æ¨å¹¿ã€‚\n"
    elif avg_d1 >= 3:
        report += "âš ï¸ **åŸºæœ¬å¥‘åˆ**: å®¶å±…ç¯å¢ƒå°šå¯ï¼Œå¯è€ƒè™‘åˆä½œã€‚\n"
    else:
        report += "âŒ **å¥‘åˆåº¦ä½**: å¯èƒ½ä¸å¤ªé€‚åˆå®¶å±…è½¯è£…æ¨å¹¿ã€‚\n"
    
    # ä¼˜åŠ£åŠ¿
    all_strengths, all_weaknesses = [], []
    for ev in all_evidence:
        overall = ev.get('overall', {})
        all_strengths.extend(overall.get('strengths', []))
        all_weaknesses.extend(overall.get('weaknesses', []))
    
    report += "\n---\n\n## äº”ã€ç»¼åˆè¯„ä¼°\n\n### 5.1 ä¸»è¦ä¼˜åŠ¿\n\n"
    for s in list(set(all_strengths))[:5]:
        report += f"- {s}\n"
    
    report += "\n### 5.2 ä¸è¶³ä¹‹å¤„\n\n"
    for w in list(set(all_weaknesses))[:5]:
        report += f"- {w}\n"
    
    # æœ€ç»ˆç»“è®º
    if avg_overall >= 4:
        recommendation, pillow_fit = "â­â­â­ å¼ºçƒˆæ¨è", "éå¸¸é€‚åˆ"
    elif avg_overall >= 3.5:
        recommendation, pillow_fit = "â­â­ æ¨è", "é€‚åˆ"
    elif avg_overall >= 3:
        recommendation, pillow_fit = "â­ è°¨æ…", "ä¸€èˆ¬"
    else:
        recommendation, pillow_fit = "æš‚ä¸æ¨è", "ä¸å¤ªé€‚åˆ"
    
    report += f"""
---

## å…­ã€æœ€ç»ˆç»“è®º

| é¡¹ç›® | ç»“è®º |
|------|------|
| **æ¨èç­‰çº§** | {recommendation} |
| **æŠ±æ•æ¨å¹¿é€‚åˆåº¦** | {pillow_fit} |
| **å½¢è±¡é£æ ¼** | {avg_d1:.1f}åˆ† |
| **å‰ªè¾‘èƒ½åŠ›** | {avg_d2:.1f}åˆ† |
| **å£æ’­èƒ½åŠ›** | {avg_d3:.1f}åˆ† |
| **ç»¼åˆè¯„åˆ†** | {avg_overall:.1f}åˆ† |

---

*æŠ¥å‘Šç”Ÿæˆ: {time.strftime('%Y-%m-%d %H:%M:%S')} | æ¨¡å‹: Gemini 2.5 Flash | è§†é¢‘æ•°: {len(video_analyses)}*
"""
    
    # ä¿å­˜
    report_file = kol_result_dir / f"{kol_name}_ç»¼åˆè¯„ä¼°æŠ¥å‘Š.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    summary_json = {
        'kol_id': kol_id,
        'kol_name': kol_name,
        'fans_count': fans_count,
        'video_count': len(video_analyses),
        'scores': {
            'd1_avg': round(avg_d1, 1),
            'd2_avg': round(avg_d2, 1),
            'd3_avg': round(avg_d3, 1),
            'overall': round(avg_overall, 1)
        },
        'recommendation': recommendation,
        'pillow_fit': pillow_fit,
        'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    json_file = kol_result_dir / f"{kol_name}_ç»¼åˆè¯„ä¼°.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(summary_json, f, ensure_ascii=False, indent=2)
    
    log(f"âœ… æŠ¥å‘Š: {report_file.name}")
    return report


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='Geminiè§†é¢‘åˆ†æ')
    parser.add_argument('--kol-id', type=str, help='åˆ†ææŒ‡å®šKOLçš„æ‰€æœ‰è§†é¢‘')
    parser.add_argument('--kol-ids', type=str, help='æ‰¹é‡åˆ†æå¤šä¸ªKOLï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--summary', type=str, help='ç”ŸæˆæŒ‡å®šKOLçš„æ±‡æ€»æŠ¥å‘Š')
    parser.add_argument('--workers', type=int, default=5, help='å¹¶è¡Œæ•°ï¼ˆé»˜è®¤5ï¼‰')
    args = parser.parse_args()
    
    if args.kol_id:
        analyze_kol_videos(args.kol_id, max_workers=args.workers)
        # åˆ†æå®Œè‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š
        video_list_file = DATA_DIR / "video_list.json"
        with open(video_list_file, 'r') as f:
            data = json.load(f)
        for v in data['videos']:
            if v.get('kol_id') == args.kol_id:
                generate_kol_summary(args.kol_id, v.get('kol_name', 'Unknown'))
                break
    
    elif args.kol_ids:
        kol_ids = [k.strip() for k in args.kol_ids.split(',')]
        for kol_id in kol_ids:
            analyze_kol_videos(kol_id, max_workers=args.workers)
            # ç”ŸæˆæŠ¥å‘Š
            video_list_file = DATA_DIR / "video_list.json"
            with open(video_list_file, 'r') as f:
                data = json.load(f)
            for v in data['videos']:
                if v.get('kol_id') == kol_id:
                    generate_kol_summary(kol_id, v.get('kol_name', 'Unknown'))
                    break
            log("\n" + "="*60 + "\n")
    
    elif args.summary:
        video_list_file = DATA_DIR / "video_list.json"
        with open(video_list_file, 'r') as f:
            data = json.load(f)
        for v in data['videos']:
            if v.get('kol_id') == args.summary:
                generate_kol_summary(args.summary, v.get('kol_name', 'Unknown'))
                break
    
    else:
        print("ç”¨æ³•:")
        print("  --kol-id <ID>      åˆ†ææŒ‡å®šKOLçš„æ‰€æœ‰è§†é¢‘")
        print("  --kol-ids <IDs>    æ‰¹é‡åˆ†æå¤šä¸ªKOLï¼ˆé€—å·åˆ†éš”ï¼‰")
        print("  --summary <ID>     ç”ŸæˆæŒ‡å®šKOLçš„æ±‡æ€»æŠ¥å‘Š")
        print("  --workers <N>      å¹¶è¡Œæ•°ï¼ˆé»˜è®¤5ï¼‰")


if __name__ == "__main__":
    main()
