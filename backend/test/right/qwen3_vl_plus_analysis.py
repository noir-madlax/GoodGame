#!/usr/bin/env python3
"""
é˜¿é‡Œäº‘ Qwen3 VL Plus è§†é¢‘åˆ†æè„šæœ¬
ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°çš„ Qwen3-VL-Plus æ¨¡å‹ç›´æ¥åˆ†æè§†é¢‘å†…å®¹
"""
import os
import sys
import json
import time
from pathlib import Path
from typing import Dict, Any, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

# ç®€åŒ–çš„æ—¥å¿—ç±»
class SimpleLogger:
    """ç®€åŒ–çš„æ—¥å¿—ç±»"""
    def info(self, msg):
        print(f"[INFO] {msg}")
    
    def error(self, msg, exc_info=False):
        print(f"[ERROR] {msg}")
        if exc_info:
            import traceback
            traceback.print_exc()

log = SimpleLogger()

# å°è¯•å¯¼å…¥é˜¿é‡Œäº‘SDK
try:
    from dashscope import MultiModalConversation
    import dashscope
except ImportError:
    log.error("éœ€è¦å®‰è£…é˜¿é‡Œäº‘SDK: pip install dashscope")
    MultiModalConversation = None
    dashscope = None


class ModelMetrics:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.start_time = None
        self.end_time = None
        self.total_time = 0
        self.input_tokens = 0
        self.output_tokens = 0
        self.total_tokens = 0
        self.success = False
        self.error_message = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "model_name": self.model_name,
            "timing": {
                "total_time_seconds": round(self.total_time, 2)
            },
            "tokens": {
                "input_tokens": self.input_tokens,
                "output_tokens": self.output_tokens,
                "total_tokens": self.total_tokens
            },
            "status": {
                "success": self.success,
                "error_message": self.error_message
            }
        }


def load_api_key() -> str:
    """ä» .env æ–‡ä»¶åŠ è½½é˜¿é‡Œäº‘ API Key"""
    # å…ˆå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
    api_key = os.getenv("ALICLOUD_API_KEY", "")
    if api_key:
        return api_key
    
    # å°è¯•ä»å¤šä¸ªä½ç½®æŸ¥æ‰¾ .env æ–‡ä»¶
    env_paths = [
        Path(__file__).resolve().parent / ".env",
        Path(__file__).resolve().parents[1] / ".env",
        Path(__file__).resolve().parents[2] / ".env",
    ]
    
    for env_path in env_paths:
        if env_path.exists():
            log.info(f"ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
            with open(env_path) as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("ALICLOUD_API_KEY="):
                        api_key = line.split("=", 1)[1].strip().strip('"').strip("'")
                        if api_key:
                            return api_key
    
    raise RuntimeError("æœªæ‰¾åˆ° ALICLOUD_API_KEY ç¯å¢ƒå˜é‡")


def prepare_video_url(video_path: Path) -> str:
    """å‡†å¤‡è§†é¢‘URL - ä½¿ç”¨æœ¬åœ°æ–‡ä»¶è·¯å¾„
    
    æ³¨æ„ï¼šDashScopeæ”¯æŒæœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œæ ¼å¼ä¸º file:///path/to/video.mp4
    """
    try:
        log.info(f"å‡†å¤‡è§†é¢‘è·¯å¾„: {video_path.name}")
        
        # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶è·¯å¾„
        video_url = f"file://{video_path.resolve()}"
        log.info(f"âœ“ è§†é¢‘è·¯å¾„: {video_url}")
        return video_url
            
    except Exception as e:
        log.error(f"å‡†å¤‡è§†é¢‘è·¯å¾„å¤±è´¥: {e}", exc_info=True)
        raise


def analyze_video_with_qwen3_vl_plus(
    video_path: Path,
    api_key: str,
    system_prompt: str,
    user_prompt: str
) -> Tuple[Dict[str, Any], ModelMetrics]:
    """ä½¿ç”¨ Qwen3-VL-Plus åˆ†æè§†é¢‘
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        api_key: é˜¿é‡Œäº‘APIå¯†é’¥
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        user_prompt: ç”¨æˆ·æç¤ºè¯
        
    Returns:
        (åˆ†æç»“æœ, æ€§èƒ½æŒ‡æ ‡)
    """
    if MultiModalConversation is None or dashscope is None:
        raise RuntimeError("éœ€è¦å®‰è£… dashscope: pip install dashscope")
    
    metrics = ModelMetrics("Qwen3-VL-Plus (Aliyun)")
    metrics.start_time = time.time()
    
    try:
        # è®¾ç½®API Key
        dashscope.api_key = api_key
        
        # å‡†å¤‡è§†é¢‘URL
        log.info("æ­¥éª¤ 1: å‡†å¤‡è§†é¢‘æ–‡ä»¶è·¯å¾„...")
        video_url = prepare_video_url(video_path)
        
        # æ„å»ºæ¶ˆæ¯
        log.info("æ­¥éª¤ 2: è°ƒç”¨ Qwen-VL-Plus è¿›è¡Œåˆ†æ...")
        
        messages = [
            {
                'role': 'system',
                'content': [{'text': system_prompt}]
            },
            {
                'role': 'user',
                'content': [
                    {'video': video_url},
                    {'text': user_prompt}
                ]
            }
        ]
        
        # è°ƒç”¨API
        response = MultiModalConversation.call(
            model='qwen-vl-plus',  # æˆ– qwen-vl-max
            messages=messages,
            result_format='message',
            stream=False
        )
        
        metrics.end_time = time.time()
        metrics.total_time = metrics.end_time - metrics.start_time
        
        # æ£€æŸ¥å“åº”
        if response.status_code == 200:
            log.info("âœ“ åˆ†æå®Œæˆ")
            
            # æå–ç»“æœ
            output = response.output
            choices = output.get('choices', [])
            if choices:
                message = choices[0].get('message', {})
                content = message.get('content', [])
                
                # æå–æ–‡æœ¬å†…å®¹
                text_content = ""
                for item in content:
                    if isinstance(item, dict) and 'text' in item:
                        text_content += item['text']
                
                # æå–tokenä½¿ç”¨æƒ…å†µ
                usage = output.get('usage', {})
                metrics.input_tokens = usage.get('input_tokens', 0)
                metrics.output_tokens = usage.get('output_tokens', 0)
                metrics.total_tokens = metrics.input_tokens + metrics.output_tokens
                
                # è§£æJSON
                try:
                    result = json.loads(text_content)
                except json.JSONDecodeError:
                    # å°è¯•æå–JSONç‰‡æ®µ
                    s = text_content.strip()
                    l = s.find("{")
                    r = s.rfind("}")
                    if 0 <= l < r:
                        result = json.loads(s[l : r + 1])
                    else:
                        raise RuntimeError(f"æ— æ³•è§£æå“åº”ä¸ºJSON: {text_content[:200]}")
                
                metrics.success = True
                
                log.info(f"è€—æ—¶: {metrics.total_time:.1f}ç§’")
                log.info(f"Tokens: {metrics.input_tokens} è¾“å…¥, {metrics.output_tokens} è¾“å‡º")
                
                return result, metrics
            else:
                raise RuntimeError("APIè¿”å›çš„choicesä¸ºç©º")
        else:
            raise RuntimeError(f"APIè°ƒç”¨å¤±è´¥: {response.code} - {response.message}")
            
    except Exception as e:
        metrics.end_time = time.time()
        metrics.total_time = metrics.end_time - metrics.start_time
        metrics.success = False
        metrics.error_message = str(e)
        log.error(f"åˆ†æå¤±è´¥: {e}", exc_info=True)
        raise


def build_prompts(video1_summary: Dict, video2_summary: Dict) -> Tuple[str, str]:
    """æ„å»ºåˆ†ææç¤ºè¯"""
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æä¸“å®¶ï¼Œä¸“é—¨ä»äº‹è§†é¢‘ä¾µæƒåˆ†æå’Œç›¸ä¼¼åº¦æ£€æµ‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ä»”ç»†è§‚çœ‹å¹¶åˆ†æä¸¤ä¸ªè§†é¢‘çš„å†…å®¹
2. ä»å¤šä¸ªç»´åº¦æ¯”å¯¹ä¸¤ä¸ªè§†é¢‘çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§
3. æä¾›å®¢è§‚ã€è¯¦ç»†çš„åˆ†ææ•°æ®å’Œè¯æ®
4. **ä¸åšä¾µæƒç»“è®ºåˆ¤æ–­**ï¼Œåªæä¾›æ•°æ®åˆ†æåŸºç¡€

åˆ†æç»´åº¦åŒ…æ‹¬ä½†ä¸é™äºï¼š
- è§†è§‰å†…å®¹ï¼šåœºæ™¯ã€ç”»é¢æ„å›¾ã€è‰²å½©é£æ ¼ã€é•œå¤´è¯­è¨€
- éŸ³é¢‘å†…å®¹ï¼šèƒŒæ™¯éŸ³ä¹ã€é…éŸ³ã€éŸ³æ•ˆ
- æ–‡å­—å†…å®¹ï¼šå­—å¹•ã€æ ‡é¢˜ã€æ–‡æ¡ˆ
- å™äº‹ç»“æ„ï¼šæ•…äº‹çº¿ã€æƒ…èŠ‚å‘å±•ã€èŠ‚å¥
- åˆ›ä½œå…ƒç´ ï¼šç‰¹æ•ˆã€è½¬åœºã€å‰ªè¾‘æ‰‹æ³•
- æŠ€æœ¯å‚æ•°ï¼šåˆ†è¾¨ç‡ã€æ—¶é•¿ã€æ ¼å¼

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼Œç»“æ„å¦‚ä¸‹ï¼š
{
  "similarity_analysis": {
    "overall_similarity_score": <0-100çš„ç›¸ä¼¼åº¦è¯„åˆ†>,
    "visual_similarity": {
      "score": <0-100>,
      "description": "è§†è§‰ç›¸ä¼¼åº¦æè¿°",
      "evidence": ["è¯æ®1", "è¯æ®2", ...]
    },
    "audio_similarity": {
      "score": <0-100>,
      "description": "éŸ³é¢‘ç›¸ä¼¼åº¦æè¿°",
      "evidence": ["è¯æ®1", "è¯æ®2", ...]
    },
    "text_similarity": {
      "score": <0-100>,
      "description": "æ–‡å­—ç›¸ä¼¼åº¦æè¿°",
      "evidence": ["è¯æ®1", "è¯æ®2", ...]
    },
    "narrative_similarity": {
      "score": <0-100>,
      "description": "å™äº‹ç»“æ„ç›¸ä¼¼åº¦æè¿°",
      "evidence": ["è¯æ®1", "è¯æ®2", ...]
    }
  },
  "difference_analysis": {
    "visual_differences": ["å·®å¼‚1", "å·®å¼‚2", ...],
    "audio_differences": ["å·®å¼‚1", "å·®å¼‚2", ...],
    "text_differences": ["å·®å¼‚1", "å·®å¼‚2", ...],
    "narrative_differences": ["å·®å¼‚1", "å·®å¼‚2", ...],
    "technical_differences": ["å·®å¼‚1", "å·®å¼‚2", ...]
  },
  "transformation_analysis": {
    "modifications": ["ä¿®æ”¹1", "ä¿®æ”¹2", ...],
    "additions": ["æ–°å¢å†…å®¹1", "æ–°å¢å†…å®¹2", ...],
    "deletions": ["åˆ é™¤å†…å®¹1", "åˆ é™¤å†…å®¹2", ...],
    "rearrangements": ["é‡æ–°æ’åˆ—1", "é‡æ–°æ’åˆ—2", ...]
  },
  "content_overlap": {
    "shared_scenes": ["å…±åŒåœºæ™¯æè¿°1", "å…±åŒåœºæ™¯æè¿°2", ...],
    "shared_dialogues": ["å…±åŒå¯¹è¯1", "å…±åŒå¯¹è¯2", ...],
    "shared_visual_elements": ["å…±åŒè§†è§‰å…ƒç´ 1", "å…±åŒè§†è§‰å…ƒç´ 2", ...]
  },
  "metadata_comparison": {
    "duration_comparison": "æ—¶é•¿å¯¹æ¯”æè¿°",
    "quality_comparison": "ç”»è´¨å¯¹æ¯”æè¿°",
    "format_comparison": "æ ¼å¼å¯¹æ¯”æè¿°"
  },
  "summary": {
    "key_findings": ["å…³é”®å‘ç°1", "å…³é”®å‘ç°2", ...],
    "data_quality_notes": "æ•°æ®è´¨é‡è¯´æ˜"
  }
}

æ³¨æ„ï¼š
1. æ‰€æœ‰åˆ†æ•°ä½¿ç”¨ 0-100 çš„èŒƒå›´
2. è¯æ®å’Œæè¿°è¦å…·ä½“ã€å®¢è§‚
3. ä¸è¦ä½¿ç”¨"ä¾µæƒ"ã€"æŠ„è¢­"ç­‰ç»“è®ºæ€§è¯æ±‡
4. ä¸“æ³¨äºå¯è§‚æµ‹çš„äº‹å®å’Œæ•°æ®"""
    
    user_prompt = f"""è¯·åˆ†æä»¥ä¸‹ä¸¤ä¸ªè§†é¢‘çš„ç›¸ä¼¼åº¦å’Œå·®å¼‚ã€‚

**è§†é¢‘1åŸºæœ¬ä¿¡æ¯ï¼š**
- ID: {video1_summary.get('video_id', 'N/A')}
- æ ‡é¢˜: {video1_summary.get('title', 'N/A')}
- ä½œè€…: {video1_summary.get('author', 'N/A')}
- æ—¶é•¿: {video1_summary.get('duration', 0) / 1000:.1f}ç§’

**è§†é¢‘2åŸºæœ¬ä¿¡æ¯ï¼š**
- ID: {video2_summary.get('video_id', 'N/A')}
- æ ‡é¢˜: {video2_summary.get('title', 'N/A')}
- ä½œè€…: {video2_summary.get('author', 'N/A')}
- æ—¶é•¿: {video2_summary.get('duration', 0) / 1000:.1f}ç§’

è¯·æŒ‰ç…§ç³»ç»ŸæŒ‡ä»¤ä¸­å®šä¹‰çš„ JSON æ ¼å¼ï¼Œæä¾›å®Œæ•´çš„åˆ†æç»“æœã€‚"""
    
    return system_prompt, user_prompt


def validate_result(result: Dict[str, Any]) -> Dict[str, Any]:
    """éªŒè¯åˆ†æç»“æœ"""
    validation = {
        "is_valid": True,
        "missing_fields": [],
        "completeness_score": 0,
        "notes": []
    }
    
    required_fields = [
        "similarity_analysis",
        "difference_analysis",
        "transformation_analysis",
        "content_overlap",
        "metadata_comparison",
        "summary"
    ]
    
    for field in required_fields:
        if field not in result:
            validation["missing_fields"].append(field)
            validation["is_valid"] = False
    
    total_fields = len(required_fields)
    present_fields = total_fields - len(validation["missing_fields"])
    validation["completeness_score"] = int((present_fields / total_fields) * 100)
    
    if validation["is_valid"]:
        validation["notes"].append("ç»“æœç»“æ„å®Œæ•´ï¼Œç¬¦åˆé¢„æœŸ")
    else:
        validation["notes"].append(f"ç¼ºå°‘å­—æ®µ: {', '.join(validation['missing_fields'])}")
    
    return validation


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("é˜¿é‡Œäº‘ Qwen3-VL-Plus è§†é¢‘æ¯”å¯¹åˆ†æ")
    print("=" * 80 + "\n")
    
    # è§†é¢‘æ–‡ä»¶è·¯å¾„
    video1_path = Path(__file__).resolve().parent / "output" / "7521959446235548985" / "v1.mp4"
    video2_path = Path(__file__).resolve().parent / "output" / "7523787273016839434" / "7523787273016839434.mp4"
    
    # æ‘˜è¦æ–‡ä»¶è·¯å¾„
    summary1_path = video1_path.parent / "summary.json"
    summary2_path = video2_path.parent / "summary.json"
    
    # è¾“å‡ºç›®å½•
    output_dir = Path(__file__).resolve().parent / "output"
    
    # éªŒè¯æ–‡ä»¶
    for path in [video1_path, video2_path, summary1_path, summary2_path]:
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    
    print(f"âœ“ è§†é¢‘1: {video1_path.name}")
    print(f"âœ“ è§†é¢‘2: {video2_path.name}\n")
    
    # åŠ è½½æ‘˜è¦
    with open(summary1_path) as f:
        video1_summary = json.load(f)
    with open(summary2_path) as f:
        video2_summary = json.load(f)
    
    # åŠ è½½ API Key
    api_key = load_api_key()
    print(f"âœ“ API Key: {api_key[:20]}...\n")
    
    # æ„å»ºæç¤ºè¯
    system_prompt, user_prompt = build_prompts(video1_summary, video2_summary)
    
    try:
        # æ³¨æ„ï¼šQwen3-VL-Plus ä¸€æ¬¡åªèƒ½å¤„ç†ä¸€ä¸ªè§†é¢‘
        # æˆ‘ä»¬éœ€è¦åˆ†åˆ«åˆ†æä¸¤ä¸ªè§†é¢‘ï¼Œç„¶åæ¯”å¯¹ç»“æœ
        print("âš ï¸  æ³¨æ„: Qwen3-VL-Plus éœ€è¦åˆ†åˆ«åˆ†ææ¯ä¸ªè§†é¢‘\n")
        
        print("=" * 80)
        print("åˆ†æè§†é¢‘1...")
        print("=" * 80)
        result1, metrics1 = analyze_video_with_qwen3_vl_plus(
            video1_path, api_key,
            "ä½ æ˜¯è§†é¢‘å†…å®¹åˆ†æä¸“å®¶ã€‚è¯·è¯¦ç»†æè¿°è¿™ä¸ªè§†é¢‘çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€äººç‰©ã€å¯¹è¯ã€æƒ…èŠ‚ç­‰ã€‚",
            f"è¯·è¯¦ç»†åˆ†æè¿™ä¸ªè§†é¢‘ï¼ˆ{video1_summary.get('title', 'N/A')}ï¼‰çš„å†…å®¹ã€‚"
        )
        
        print("\n" + "=" * 80)
        print("åˆ†æè§†é¢‘2...")
        print("=" * 80)
        result2, metrics2 = analyze_video_with_qwen3_vl_plus(
            video2_path, api_key,
            "ä½ æ˜¯è§†é¢‘å†…å®¹åˆ†æä¸“å®¶ã€‚è¯·è¯¦ç»†æè¿°è¿™ä¸ªè§†é¢‘çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€äººç‰©ã€å¯¹è¯ã€æƒ…èŠ‚ç­‰ã€‚",
            f"è¯·è¯¦ç»†åˆ†æè¿™ä¸ªè§†é¢‘ï¼ˆ{video2_summary.get('title', 'N/A')}ï¼‰çš„å†…å®¹ã€‚"
        )
        
        # ä¿å­˜å•ç‹¬çš„åˆ†æç»“æœ
        output_file1 = output_dir / "qwen3_vl_plus_video1_analysis.json"
        with open(output_file1, "w", encoding="utf-8") as f:
            json.dump(result1, f, ensure_ascii=False, indent=2)
        
        output_file2 = output_dir / "qwen3_vl_plus_video2_analysis.json"
        with open(output_file2, "w", encoding="utf-8") as f:
            json.dump(result2, f, ensure_ascii=False, indent=2)
        
        print("\n" + "=" * 80)
        print("åˆ†æå®Œæˆ")
        print("=" * 80)
        print(f"\nâœ“ è§†é¢‘1åˆ†æç»“æœ: {output_file1.name}")
        print(f"âœ“ è§†é¢‘2åˆ†æç»“æœ: {output_file2.name}")
        
        print(f"\nğŸ“Š è§†é¢‘1åˆ†æè€—æ—¶: {metrics1.total_time:.1f}ç§’")
        print(f"ğŸ“Š è§†é¢‘2åˆ†æè€—æ—¶: {metrics2.total_time:.1f}ç§’")
        print(f"ğŸ“Š æ€»è€—æ—¶: {metrics1.total_time + metrics2.total_time:.1f}ç§’")
        
        if metrics1.input_tokens > 0:
            print(f"\nğŸ¯ è§†é¢‘1 Tokens: {metrics1.input_tokens} è¾“å…¥, {metrics1.output_tokens} è¾“å‡º")
        if metrics2.input_tokens > 0:
            print(f"ğŸ¯ è§†é¢‘2 Tokens: {metrics2.input_tokens} è¾“å…¥, {metrics2.output_tokens} è¾“å‡º")
        
        print("\n" + "=" * 80)
        print("âœ… ä»»åŠ¡å®Œæˆï¼")
        print("=" * 80 + "\n")
        
    except Exception as e:
        log.error(f"åˆ†æå¤±è´¥: {e}", exc_info=True)
        print("\n" + "=" * 80)
        print("âŒ åˆ†æå¤±è´¥")
        print("=" * 80 + "\n")
        raise


if __name__ == "__main__":
    main()

